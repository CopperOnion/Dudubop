{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "fake_news_frame = pd.read_csv('fake_and_real_news/Fake.csv')\n",
    "true_news_frame = pd.read_csv('fake_and_real_news/True.csv')\n",
    "\n",
    "# add authenticity label\n",
    "\n",
    "\n",
    "fake_column = [\"Fake\"] * len(fake_news_frame)\n",
    "#print(len(fake_column))\n",
    "\n",
    "fake_news_frame.insert(4, 'authenticity', fake_column)\n",
    "#fake_news_frame.to_csv(path_or_buf='fake_and_real_news/NewFake.csv')\n",
    "\n",
    "\n",
    "true_column = [\"True\"] * len(true_news_frame)\n",
    "#print(len(true_column))\n",
    "\n",
    "true_news_frame.insert(4, 'authenticity', true_column)\n",
    "#true_news_frame.to_csv(path_or_buf='fake_and_real_news/NewTrue.csv')\n",
    "\n",
    "\n",
    "# combine datasets\n",
    "\n",
    "frames = [fake_news_frame, true_news_frame]\n",
    "combined_news_frame = pd.concat(frames)\n",
    "\n",
    "combined_news_frame.to_csv(path_or_buf='fake_and_real_news/Combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Drunk Bragging Trump Staffer Started Russian Collusion Investigation\n",
      "Text: House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.\n",
      "Subject: News\n",
      "Date: December 31, 2017\n",
      "Authenticity: Fake\n"
     ]
    }
   ],
   "source": [
    "news_frame = pd.read_csv('fake_and_real_news/Combined.csv')\n",
    "\n",
    "n = 1\n",
    "title = news_frame.iloc[n, 1]\n",
    "text = news_frame.iloc[n, 2]\n",
    "subject = news_frame.iloc[n, 3]\n",
    "date = news_frame.iloc[n, 4]\n",
    "authenticity = news_frame.iloc[n, 5]\n",
    "\n",
    "print('Title: {}'.format(title))\n",
    "print('Text: {}'.format(text))\n",
    "print('Subject: {}'.format(subject))\n",
    "print('Date: {}'.format(date))\n",
    "print('Authenticity: {}'.format(authenticity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\" News dataset. \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"Args:\n",
    "            csv_file (string): Path to the news csv file.\n",
    "            root_dir (string): Path to the root directory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.news_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.news_frame)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        news = self.news_frame.iloc[idx, 1:]\n",
    "        \n",
    "        # we are going to have to make some way to parse text as words, and to feed it to the NN\n",
    "        \n",
    "        return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset('fake_and_real_news/Combined.csv', 'fake_and_real_news/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: populate text dataset with news article texts\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "text_dataset = []\n",
    "\n",
    "for idx in range(0, len(news_dataset)):\n",
    "    text_dataset.append(news_dataset[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/paul/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/paul/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/paul/.local/lib/python3.6/site-packages (from nltk) (4.48.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/paul/.local/lib/python3.6/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/paul/.local/lib/python3.6/site-packages (from nltk) (0.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing: encode one news article using nltk encoder\n",
    "import nltk\n",
    "\n",
    "from torchnlp.encoders.text import TreebankEncoder\n",
    "\n",
    "\n",
    "encoder = TreebankEncoder(text_dataset)\n",
    "encoder.encode(\"Test input.\")\n",
    "print(len(encoder.vocab))\n",
    "\n",
    "encoder.encode(text_dataset[10923])\n",
    "\n",
    "#lineno = 0\n",
    "\n",
    "#for text in text_dataset.values():\n",
    "#    encoder.encode(text)\n",
    "#    if lineno % 5000 == 0:\n",
    "#        print(lineno)\n",
    "#    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(text_dataset[10923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 lineno\n",
      "398 len\n",
      "As Donald Trump s campaign continues to sink deeper into its self-sabotaging downward spiral, it s becoming clear that even Trump s campaign surrogates and former staffers are having trouble trying to stay positive about the outcome of this election. In just the past few days, we ve seen them completely deny that Trump s campaign made some massive changes in desperation, ignore polls that The Donald is losing and now thanks to former Trump campaign manager Corey Lewandowski, they re holding onto false hope that Trump can still somehow win this.Earlier today on CNN, Lewandowski   who is forbidden by contract to say anything negative about Trump   tried to convince everyone that Trump was still on track to win this election because his biggest opponent, Democratic nominee Hillary Clinton, was losing voters to Green Party candidate Jill Stein and Libertarian Party candidate Gary Johnson. Lewandowski said: This is not a two-person race. Gary Johnson and Jill Stein are in this race. I would say Jill Stein should be in the debates. She s a person who is going to take away votes from Hillary Clinton. You just had her here on the show  At the end of the day, Donald Trump is going to win this election, he s going to win in states because Gary Johnson and Jill Stein are going to take votes away from Hillary Clinton. While Lewandowski clung to his delusions, CNN panelist Bill Kristol was sitting right beside him, shaking his head while smiling and muffling his laughter. When Kristol could finally get enough control over himself to speak, he dropped a major truth bomb that Lewandowski and Trump weren t going to like: That s the last recourse of a campaign that s going to lose. It s hoping third-and-fourth-party candidates are going to magically change the equation. At the end of the day, 38% of the American people think Donald Trump should be president, it s never gotten much above that number. You can watch Kristol giggle at Lewandoski s silly hopes below:.@CLewandowski_: Donald Trump is going to win bc Gary Johnson + Jill Stein will take votes away fr Hillary Clinton https://t.co/jvwPm42LGm  New Day (@NewDay) August 18, 2016Featured image is a screenshot\n",
      "10000 lineno\n",
      "196 len\n",
      "Unbelievable! Women s March Activist went off on a crazy rant claiming that somehow President Trump s tax returns are linked to missing black girls in the U.S.:Transcript: to lock arms with one another. We demand that we see Trump s taxes while we also demand that all of our rights are protected. We demand that we see Trump s taxes while we also demand that women s rights be treated as human rights. We must speak with one voice and one accord and we must draw a direct line between Trump s taxes and missing girls, missing black girls all across this country.We must say that while we resist one, we resist all or else our fight is inauthentic. We must say Trump s taxes and equal education. We must say Trump s taxes no more police brutality. We must say that Trump s taxes is where we start today but we will not end there. That all our rights must be protected   and who the hell do you think you are to hide anything and treat us as though we are less than human?Via: GP\n",
      "15000 lineno\n",
      "669 len\n",
      "By looking at the two Democrat presidential candidates left standing, most Americans would think their platform is primarily about the destruction of capitalism and gun control. The real truth however, is that the Democrat party knows without their unyielding support for the culture of death, they would cease to exist. The sickening truth is, they either support the killing of the most vulnerable or it s curtains for them. What does that say about how far our society has fallen when leftist TV hosts celebrate China s brutal  One Child  policy on Twitter?Quintanilla may not have been endorsing the one-child policy, but he didn t offer any overt or even implied criticism of it either. In fact, saying  it worked  sounds a lot like he is suggesting it was a success.The problem, of course, is that the one-child policy, which China converted to a two-child policy this week, was a brutal, harrowing invasion of the human rights of millions of families under the guise of national policy. Since 1979, an estimated 13 million women a year underwent abortions, many of them forced to do so by government officials. Another 200 million women were sterilized under the same policy.https://twitter.com/carlquintanilla/status/659734982915657728These practices mostly went unseen and unnoticed in the era before social media and digital cameras. In 2012, the world got to see the result of this policy on one young woman when her family members posted an image of her lying next to her dead baby after she d been forced to have an abortion.The young woman, Feng Jianmei, already had one child; so after she became pregnant again, family planning officials called her house to persuade her to have an abortion. When that failed, they showed up at her home and spent hours pressuring her for consent. Jianmei slipped out of the house but was followed by a group of 15 officials to her aunt s house. After briefly escaping and hiding under the bed of a relative, Jianmei was found by the government officials and was reportedly carried out by four men.Here is the horrible story about forced abortions in China and their victims:Meanwhile, family planning officials were negotiating with Jianmei s husband. At first they demanded $15,000, but then dropped the birth planning fee to around $5,000. In any case, it was more than he could raise. Finally, Jianmei was forced to give consent by having her thumbprint placed on a form and was injected with a drug that killed her baby. Her family posted this image, which went viral, of Jianmei with her dead daughter. For criticizing the government, government officials led a march denouncing her family as  traitors,  and her husband was beaten.And that nightmare is not the only kind of suffering caused by the policy. Those who dared to violate the policy had to keep their secret children out of sight of government officials. As the Washington Post reported last month, the consequences of getting caught were severe: It was terrifying if you had an over-quota child,  my father says now.  If the government knew, you would be in trouble. People would come to your house, remove all your grains and do anything they could to you. And sometimes, they d destroy your house. My mother recalls:  Even for a new house, they d get on the roof, rip it apart and bulldoze the entire house. We had to keep moving and hiding. It was really painful for us. We knew it wasn t a long-term solution. The one-child policy was a decades long campaign of forced abortion, extortion, sterilization, and terror that traumatized hundreds of millions of people. Saying  it worked  seems like an odd way to sum up such a policy. Via: Breitbart News\n",
      "20000 lineno\n",
      "298 len\n",
      "The way Trump is packing in thousands of supporters at every rally in every state he visits, it s pretty clear his popularity way overshadows that of #CrookedHillary. She s barely getting enough people to fill a phone booth at her  rallies!   This event, at the Baptist National Convention in Kansas City turned out to be downright embarrassing for Hillary and her campaign. In a venue meant to hold 5,000 people, Hillary only drew about 1,000. Perhaps there just isn t a huge amount of interest with Baptist church members in Kansas City when it comes to spending time listening to a sociological liar Workers cutting the room down by a third with lower than expected turnout for #clintonkc speech. pic.twitter.com/L60YFfBpO1  Frank Morris (@FrankNewsman) September 8, 2016Check out the number of empty seats!Convention hall turned out lights in un-occupied part of hall for @HillaryClinton speech, now partitioning it off pic.twitter.com/O5Az8EfiRD  Brian Abel (@BrianAbelTV) September 8, 2016Here are the requested \"during speech\" pics of KC convention hall pic.twitter.com/ZRGOm77xnO  Brian Abel (@BrianAbelTV) September 8, 2016And finally, only 1,000 people filled a venue meant for 5,000. Who needs polls that are rigged by leftist media or polling firms with an agenda. From now on, we re going to continue to use the number of supporters who actually take time out of their busy day to see the candidates as a true indicator of who s leading in the race Front of press constituted 10 rows. 14 seats per section, 6 sections. Less than 1,000 of the >5k setup https://t.co/C6BcyCIl9F  Brian Abel (@BrianAbelTV) September 8, 2016h/t Gateway Pundit\n",
      "25000 lineno\n",
      "430 len\n",
      "WASHINGTON (Reuters) - U.S. President Donald Trump on Wednesday criticized Facebook Inc (FB.O) as “anti-Trump” and questioned its role during the 2016 presidential campaign, amid probes into alleged Russian interference in the election and possible collusion by Trump’s associates. His salvo came as the social media giant prepares to hand over 3,000 political ads to congressional investigators that it has said were likely purchased by Russian entities during and after last year’s presidential contest. Trump appeared to embrace the focus on the social media network in his comments on Wednesday, which also took aim at more traditional medial outlets, long targeted by the president as “fake news.” “Facebook was always anti-Trump. The networks were always anti-Trump,” Trump said on Twitter, levelling the same charge against the New York Times and the Washington Post. “Collusion?” Representatives for Facebook and the newspapers did not immediately respond to a request for comment on Trump’s tweet. U.S. Representative Adam Schiff, the top Democrat on the House Intelligence Committee, which is among those investigating Russia’s role, said he expected to have the ads by next week and that they should be made public. “You really need to see them ... to recognise how cynical an effort this was by the Kremlin, how they sought to just accentuate those divisions ... and drive American against American,” Schiff told MSNBC, adding that Facebook and Twitter Inc (TWTR.N) executives should testify publicly about the issue. “I have concerns about how long it took Facebook to realise the Russians were advertising on their network,” Schiff told MSNBC, adding that he has spoken several times with the company’s chief executive, Mark Zuckerberg.  Facebook and other technology companies are coming under increased scrutiny amid the Russia investigations. The probes, being conducted by several congressional committees along with the Department of Justice, have clouded Trump’s tenure since taking office in January and threatened his agenda, which has yet to secure a major legislative victory.  Moscow has denied any collusion. Trump himself has previously praised Facebook and credited it with helping him win the November election. His campaign has said it spent some $70 million on Facebook ads, and it also ran  a live Facebook show.  His latest comments did not appear to affect shares of the company, which were up 1.4 percent at $166.50 a share in late morning trading after analysts raised their price target for the stock. \n",
      "30000 lineno\n",
      "482 len\n",
      "WASHINGTON (Reuters) - President-elect Donald Trump voiced new doubts on Wednesday that Russian hackers attempted to influence the U.S. election on his behalf, saying WikiLeaks had denied Moscow was behind documents it made public during the campaign. Trump, writing on Twitter, continued to raise questions about the findings by U.S. intelligence agencies that Russia was behind a series of leaks that embarrassed Democratic candidate Hillary Clinton’s campaign before the Nov. 8 vote. The tweets prompted White House spokesman Josh Earnest to ask, “Who are you going to believe?” Documents stolen from the Democratic National Committee and John Podesta, Clinton’s campaign manager, were leaked to the media in advance of the election. One email showed the Clinton campaign received a question in advance of a town hall forum. Trump resumed sending notes on Twitter about the hacking issue on Wednesday, saying, “(WikiLeaks founder) Julian Assange said ‘a 14 year old could have hacked Podesta’ - why was DNC so careless? Also said the Russians did not give him the info!” Trump also quoted Assange as telling Fox News that U.S. media coverage of the matter was “very dishonest.” Asked about the tweets, Earnest said the president-elect’s public comments have pitted the Russians and Assange against 17 U.S. government intelligence agencies, outside cyber experts and lawmakers from both parties. “There’s a pretty stark line that’s been drawn, and the president-elect will have to determine who he’s going to believe,” he said at a daily news briefing. Vice President-elect Mike Pence defended Trump as simply voicing a “very sincere and healthy American skepticism about intelligence conclusions” he has been hearing. “Given some of the intelligence failures of recent years, the president-elect has made it clear to the American people that he’s skeptical about conclusions from the bureaucracy,” Pence told reporters at the U.S. Capitol.  But the top ranking elected Republican, House of Representatives Speaker Paul Ryan, had harsh words for Assange, whose group released the hacked emails. “I think the guy’s a sycophant for Russia, he leaks, he steals data and compromises national security,” Ryan told radio host Hugh Hewitt.  President Barack Obama last month ordered an investigation into malicious cyber activity and foreign intervention in the 2016 presidential election before he leaves office on Jan. 20. Earnest said the intelligence community will meet the deadline with ample time to spare. Separately, five Democratic U.S. senators introduced legislation urging the creation of an independent, nonpartisan commission to investigate any Russian interference in the election. Several lawmakers, including a few of Trump’s fellow Republicans, have backed calls for an investigation. Trump and Pence were scheduled to receive a briefing from intelligence officials on the hacking issue on Friday. \n",
      "35000 lineno\n",
      "420 len\n",
      "GENEVA (Reuters) - The U.N refugee agency on Friday accused Australia of abandoning hundreds of refugees and asylum seekers on Papua New Guinea s Manus Island and said it must take responsibility for the mess it created with its  offshore processing  system. About 800 refugees are still in a precarious situation on Manus Island, having been forcibly removed from a holding camp last month when Australia decided to close it, UNHCR spokeswoman Cecile Pouilly told a regular U.N. briefing in Geneva.   We are talking here about people who have suffered tremendously, extreme trauma, and are now feeling so insecure in the places where they are staying. There are many victims of torture, people who have been deeply traumatised, having no idea what is going to happen next to them,  she said.  In light of the continued perilous situation on Papua New Guinea s Manus Island for refugees and asylum seekers abandoned by Australia, UNHCR has called again this week on the Australian government to live up to its responsibility and urgently find humane and appropriate solutions.       Conditions in the camp, and another on the tiny Pacific island of Nauru, have been widely criticised by the United Nations and human rights groups. The two camps have been cornerstones of Australia s contentious immigration policy under which it refuses to allow asylum-seekers arriving by boat to reach its shores. The policy, aimed at deterring people from making a perilous sea voyage to Australia, has bipartisan political support. The closure of the Manus Island camp, criticised by the United Nations as  shocking , caused chaos, with the men refusing to leave the compound for fear of being attacked by Manus island residents. [nL3N1NT437] Pouilly said that in the past four weeks, there had been at least five security incidents, including an attempt by three people armed with machetes and an axe to force their way into a site where 150 refugees and asylum seekers have been accommodated since the Australian facility closed. Pouilly said that although Papua New Guinea now had to deal with the situation, the buck should stop with Australia.  What we clearly are saying is that it s Australia s responsibility in the first place,  she said.  Australia is the country that created the situation by putting in place this offshore processing facility. So what we are asking is for Australia to find solutions for these people.      \n",
      "40000 lineno\n",
      "535 len\n",
      "STRASBOURG (Reuters) - The European Parliament proposed on Wednesday to reduce EU funds to Turkey that are linked to its stalled bid to join the bloc, a call EU leaders are expected to back given a deteriorating relations with Ankara. Of the 217 million euros set to go to Turkey for reforms, infrastructure and agriculture in 2018, EU lawmakers agreed to cut up to 80 million euros. Of that, 50 million euros should be cut at first, with a further 30-million-euro reduction if Turkey does not improve its human rights record.  Turkey is not respecting freedom of speech, freedom of expression, human rights and is drifting further away from European democratic standards,  said centre-right lawmaker Siegfried Muresan, who led the budget discussions.   We cannot pretend we don t see that,  he told Reuters, emphasizing that the cuts would affect only the money earmarked for political reforms, not for infrastructure and farming. EU leaders must still sign off on the cuts but are expected to do so after an agreement at a summit last week to reduce the so-called pre-accession aid that is meant to help EU candidate countries prepare for membership. German Chancellor Angela Merkel, who pressed for action on Turkey during her re-election campaign, described Turkish behavior on human rights as  unacceptable  in Brussels last Thursday. The European Parliament s decision came on the first day of the trial in Istanbul of 11 human rights activists in Turkey, including a German and a Swedish national. At the EU summit, Merkel said that the rule of law in Turkey was  moving in the wrong direction , in a reference to the large-scale purge that President Tayyip Erdogan has carried out following a failed coup attempt in July 2016. While the EU condemned the coup attempt, the scope of Erdogan s response, his detention of U.S. and European citizens including dual nationals, and his jibes at Germany  for what he has called  Nazi-like  behavior have soured EU-Turkey ties. Erdogan says the purges across society are necessary to maintain stability in a NATO country bordering Iraq and Syria. Launched in 2005 after decades of seeking the formal start of an EU membership bid, Ankara s membership negotiations were always sensitive for France and Germany because of Turkey s status as a large, mainly Muslim country. They are not officially frozen, despite calls from Austria to formally scrap Turkey s EU membership program. That is in part because the EU relies on Ankara to take in Syrian refugees in return for billions of euros of aid. But a majority of EU countries, led by Germany and the Netherlands, say it no longer makes sense to fund political reforms in Turkey when formal EU membership talks have not taken place since last year. Aside from money that the EU gives Turkey as part of its  2016 migration deal, Ankara was set to receive 4.4 billion euros from the EU between 2014 and 2020. Some EU governments want that money to go to non-governmental groups in Turkey, not to Ankara. \n",
      "435.4645641231235\n"
     ]
    }
   ],
   "source": [
    "lineno = 0\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for text in text_dataset:\n",
    "    lengths.append(len(encoder.encode(text)))\n",
    "    lineno += 1\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno, \"lineno\")\n",
    "        print(len(encoder.encode(text)), \"len\")\n",
    "        print(text)\n",
    "\n",
    "# MAX_TEXT_LENGTH = max(lengths)\n",
    "\n",
    "average_len = sum(lengths)/len(lengths)\n",
    "print(average_len)\n",
    "\n",
    "TEXT_LENGTH = round(average_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  39,  696,   57,  997,   57,  998,  999,   17, 1000,  155,   57, 1001,\n",
      "        1002,   26,  463,  449, 1003, 1004,   23,  673,  151, 1005, 1006,  153,\n",
      "        1007,   26,   72, 1008, 1009,   17, 1010, 1011, 1012, 1013,  268,   33,\n",
      "         843,   57, 1014, 1015, 1008,   13, 1016, 1017,   57, 1018, 1019,   23,\n",
      "         408,  168,   13, 1020, 1021,  354,   13, 1022,   17, 1023, 1024,   23,\n",
      "         317, 1025,  348, 1007,   26,   72, 1008, 1009,   17, 1026,   23,  542,\n",
      "         102,  195, 1027,  195,  385, 1028, 1015, 1019,  286,  168,  448, 1029,\n",
      "         574,  191, 1030,   57, 1031, 1032,  168,   30, 1033,   23,  136,  311,\n",
      "          94,  463,   33, 1017,   94,   71, 1034,  153,  490, 1035, 1036, 1037,\n",
      "        1038,  215,  217, 1039,   26, 1040,   17, 1041,  183,  168,  398, 1042,\n",
      "        1043,   33, 1044,   94,  215,   91,   26,  463,   23, 1045, 1015, 1046,\n",
      "        1018,  562,  153,   33,  998,   25,  398, 1047, 1048,  317,  844,  159,\n",
      "         262, 1049, 1018, 1019,   23,  136,   55,  602,  673,   11,  190,  153,\n",
      "         439,   71,   72, 1050,   97,   57, 1051, 1052,   26, 1053,  809,   33,\n",
      "         110,   57, 1054,   57, 1055,  574, 1047, 1056, 1057,   57,  191,  286,\n",
      "         216,   13, 1058, 1059, 1060,   23,   19, 1061,   61,   72, 1062,   92,\n",
      "        1063, 1064, 1065, 1066,  160,  153,  102,  963,  663,   71,  599,   72,\n",
      "          33, 1067, 1068, 1069, 1070,   23, 1071,  195,  191, 1072,   23, 1012,\n",
      "          87, 1073, 1074,  286, 1075, 1076,   57, 1014, 1015, 1019,  557, 1077,\n",
      "        1036,  191,   33, 1078, 1079,   57,  102,  963,  663, 1080,   72,   23,\n",
      "         102,  195, 1081, 1082,   39,  998,  184,  219,  102, 1083,   26, 1084,\n",
      "         168,  613,   75,   13,  118,   23,  118,  546,  136,  191,   94, 1085,\n",
      "        1086, 1087,  159,  494,   13,  769, 1088,  844,   33, 1089, 1090, 1091,\n",
      "          23,  230,   26,  151,   33, 1092, 1093,   26, 1094, 1095,  107,  959,\n",
      "          23, 1096,   23,  195, 1028,  673,  174, 1097, 1098, 1099, 1077, 1100,\n",
      "         168, 1101,  134,  102,  151, 1102,  550, 1014, 1015, 1019,   25,  199,\n",
      "        1103, 1104, 1105, 1009,   17, 1026,   23,   17,  956, 1106,   26, 1107,\n",
      "        1108,   75, 1069, 1109, 1110, 1111,  428, 1112,  271])\n",
      "The number of cases of cops brutalizing and killing people of color seems to see no end. Now, we have another case that needs to be shared far and wide. An Alabama woman by the name of Angela Williams shared a graphic photo of her son, lying in a hospital bed with a beaten and fractured face, on Facebook. It needs to be shared far and wide, because this is unacceptable.It is unclear why Williams  son was in police custody or what sort of altercation resulted in his arrest, but when you see the photo you will realize that these details matter not. Cops are not supposed to beat and brutalize those in their custody. In the post you are about to see, Ms. Williams expresses her hope that the cops had their body cameras on while they were beating her son, but I think we all know that there will be some kind of convenient  malfunction  to explain away the lack of existence of dash or body camera footage of what was clearly a brutal beating. Hell, it could even be described as attempted murder. Something tells me that this young man will never be the same. Without further ado, here is what Troy, Alabama s finest decided was appropriate treatment of Angela Williams  son:No matter what the perceived crime of this young man might be, this is completely unacceptable. The cops who did this need to rot in jail for a long, long time   but what you wanna bet they get a paid vacation while the force  investigates  itself, only to have the officers returned to duty posthaste?This, folks, is why we say BLACK LIVES MATTER. No way in hell would this have happened if Angela Williams  son had been white. Please share far and wide, and stay tuned to Addicting Info for further updates.Featured image via David McNew/Stringer/Getty Images\n",
      "333\n",
      "333\n",
      "333\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: pad texts to have matching length\n",
    "\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import pad_tensor\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "padded_texts = []\n",
    "\n",
    "print(encoder.encode(text_dataset[5])[0:TEXT_LENGTH])\n",
    "print(text_dataset[5])\n",
    "print(lengths[5])\n",
    "print(len(encoder.encode(text_dataset[5])[0:TEXT_LENGTH]))\n",
    "print(len(encoder.encode(text_dataset[5])))\n",
    "# print(encoder.encode(text_dataset[5])[TEXT_LENGTH])\n",
    "\n",
    "for text in text_dataset:\n",
    "    if len(encoder.encode(text)) < TEXT_LENGTH:\n",
    "        padded_texts.append(pad_tensor(encoder.encode(text).long(), TEXT_LENGTH))\n",
    "#         print(len(pad_tensor(encoder.encode(text).long(), TEXT_LENGTH)))\n",
    "    if len(encoder.encode(text)) > TEXT_LENGTH:\n",
    "        padded_texts.append(encoder.encode(text)[0:TEXT_LENGTH])\n",
    "    if len(encoder.encode(text)) == TEXT_LENGTH:\n",
    "        padded_texts.append(encoder.encode(text))\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: convert longs to floats (for NN)\n",
    "\n",
    "# lineno = 0\n",
    "\n",
    "# for text in padded_texts:\n",
    "#     for long in text:\n",
    "#         long = float(long)\n",
    "#     if lineno % 5000 == 0:\n",
    "#         print(lineno)\n",
    "#     lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: split news article texts into strings, store in dictionary\n",
    "\n",
    "# dictionary = []\n",
    "# lengths = []\n",
    "\n",
    "# split_texts = []\n",
    "# lineno = 0\n",
    "\n",
    "# for text in text_dataset.values():\n",
    "#     text_words = text.split()\n",
    "#     split_texts.append(text_words)\n",
    "#     #print(len(text), lineno)\n",
    "#     lengths.append(len(text))\n",
    "#     lineno += 1\n",
    "#     for word in text_words:\n",
    "#         if word not in dictionary:\n",
    "#             dictionary.append(word)\n",
    "\n",
    "            \n",
    "# MAX_TEXT_LENGTH = max(lengths)\n",
    "# print(MAX_TEXT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: pad data to provide uniform input to NN\n",
    "\n",
    "# dictionary = {'fake': 0, 'true': 1}\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# random train data to test NN\n",
    "\n",
    "# split_texts = []\n",
    "\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "\n",
    "# MAX_TEXT_LENGTH = 7\n",
    "\n",
    "###\n",
    "\n",
    "# for text in split_texts:\n",
    "#     while len(text) < MAX_TEXT_LENGTH:\n",
    "#         text.append('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: convert split texts (lists of strings) into lists of ints, using dictionary\n",
    "\n",
    "# converted_split_texts = []\n",
    "\n",
    "# for text in split_texts:\n",
    "#     converted_text = []\n",
    "#     for word in text:\n",
    "#         converted_text.append(dictionary[word])\n",
    "#     converted_split_texts.append(converted_text)\n",
    "    \n",
    "# print(converted_split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: collect vector of true/fake booleans to train dataset\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "true_fake_dataset = []\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "for idx in np.nditer(rows):\n",
    "    if news_dataset[int(idx)][4] == 'Fake':\n",
    "        true_fake_dataset.append(torch.Tensor([0]))\n",
    "    if news_dataset[int(idx)][4] == 'True':\n",
    "        true_fake_dataset.append(torch.Tensor([1]))\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1\n",
    "        \n",
    "###\n",
    "\n",
    "# random train data to test NN\n",
    "\n",
    "# true_fake_dataset = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n",
      "44898\n",
      "44898\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: add true/fake label to padded_texts\n",
    "\n",
    "print(len(padded_texts))\n",
    "print(len(true_fake_dataset))\n",
    "print(len(text_dataset))\n",
    "\n",
    "trainset = []\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "for idx, text in enumerate(padded_texts):\n",
    "    if lineno % 5000 == 0:\n",
    "#         print(padded_texts[idx][0])\n",
    "#         print(len(padded_texts[idx]))\n",
    "        print(lineno)\n",
    "    lineno += 1\n",
    "    trainset.append((text, true_fake_dataset[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[30000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,\n",
       "           19,  20,  21,  22,  23,  24,  25,  26,  27,  13,  28,  29,  26,  30,\n",
       "           31,  23,  32,  17,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "           43,  25,   7,  44,  45,  26,  46,  17,  24,   8,   9,  46,  47,  48,\n",
       "           49,  50,  51,  52,  53,  17,  54,  23,  55,  56,  26,  10,  11,  57,\n",
       "           58,  59,  23,  60,  23,  31,  23,  32,  23,  17,  61,  33,  34,  35,\n",
       "           62,  63,  64,  23,  13,  14,  17,  65,  15,  16,  23,  66,  67,  68,\n",
       "           69,  70,  71,  72,  13,  73,  74,  75,  76,  77,  48,  49,  50,  51,\n",
       "           52,  53,  17,  54,  23,  55,  56,  26,  10,  11,  57,  58,  59,  23,\n",
       "           60,  23,  31,  23,  32,  23,  17,  61,  33,  34,  35,  62,  63,  64,\n",
       "           23,  13,  14,  17,  65,  15,  78,  70,  71,  72,  13,  73,  74,  75,\n",
       "           76,  77,   5,  79,   6,  80,  81,  82,  83,  84,  85,  23,  86,  87,\n",
       "           88,  89,  90,  91,  92,  93,  92,  94,  95,  96,  97,  57,  98,  99,\n",
       "           13,  15,  16,  87, 100, 101, 102, 103,  23, 104,  23, 105, 106, 107,\n",
       "          108,   6,  77, 109, 110,  57, 111, 112,   9,  61, 113, 114,  26, 115,\n",
       "          116,  33, 117, 118, 119,  26,  10,  33, 120, 121,  13, 122, 123,  74,\n",
       "           77, 124, 125, 126,  80,  81, 127,  83,  84,  85,  23, 128,  44, 129,\n",
       "           94, 130,  80,  81, 131,  83,  84,  85,  23, 132, 133, 134, 135,  70,\n",
       "           13,  73,  74,  75,  76,  23, 136,  55, 137, 138, 139, 140, 141,  57,\n",
       "          142, 143, 144,  80,  81, 145,  83,  84,  85,  23, 146,  94, 147, 148,\n",
       "          149, 107, 150,  94, 151,  26, 152, 153, 154, 155, 153, 156,  94,  94,\n",
       "          151,  26, 157, 107, 158,  46,  33, 159,  11, 156, 160, 107, 161, 162,\n",
       "           80,  81, 163,  83,  84,  85,  23, 164, 165,  33, 166, 167, 168,  13,\n",
       "           15, 169,  10, 107, 107, 170,  80,  81, 171,  83,  84,  85,  23, 172,\n",
       "          173,   9,   7, 174, 122, 123,  74, 107, 175, 176,  80,  81, 177,  83,\n",
       "           84,  85,  23, 178,  87,   6,  87,  15,  16,  87, 179,  88, 180, 181,\n",
       "           15,  16,  26,  11,  23, 182,  26,  58, 154,  31,  17, 183, 184, 151,\n",
       "          185, 160,  17, 186, 187, 188, 159,   7, 189,   9, 190, 191,  26, 192,\n",
       "          193,  77,   5,  79,   6,  80,  81,  82,  83,  84,  85,  23, 194, 195,\n",
       "          196, 123,  75, 197, 198,  87, 199, 200, 102,  75, 201, 202, 203, 204,\n",
       "           26,  30,  31,  17,  32,  75,  15,  16,  87,  23, 205,  23, 206,  23,\n",
       "           17]), tensor([0.])),\n",
       " (tensor([272, 273, 274, 275, 276, 277, 195, 278,  26, 151,  13, 279, 280, 198,\n",
       "           87, 199, 281,  33, 282,  23, 101, 154,  57, 283,  23, 153,  33, 284,\n",
       "          285, 286, 191, 287,  33, 288, 289, 187,  24,  87, 199, 290,  29,  20,\n",
       "           33, 291,  57, 292,  17,  33, 293, 168, 294,  26, 295, 197,  48,  19,\n",
       "          296,  23,  33, 297, 195, 217, 191, 298,  33, 289,  23, 299,  26, 300,\n",
       "          301, 268,  33,  15, 302, 303,   6, 304, 305, 306, 307, 286, 308, 168,\n",
       "           13, 309, 310, 311,  24, 312, 313,  57, 314, 315, 316, 317, 318, 319,\n",
       "          320,  57, 153,  23, 307, 321,   9,   7,  13, 322, 323,  75,   6,  23,\n",
       "           92,  30, 324, 202, 325, 198,  25,  13, 326, 327, 328,  23, 136, 329,\n",
       "          187, 330,  92, 331,  13, 332, 333, 168,  13, 309, 334, 335, 336, 189,\n",
       "            9, 337,  26, 338,  13,  15, 302, 339, 340,   6,  17,  66, 341, 342,\n",
       "          343,  57, 344, 345, 346, 222,  33, 347, 348, 286, 349, 222, 153,  33,\n",
       "           40, 350, 351, 352, 353, 354, 355, 356,  75,   6,  23, 136, 357,   6,\n",
       "          358, 354, 114, 331, 359,  13, 360, 361, 362, 363,  23, 307, 312,  26,\n",
       "          364, 365, 366, 367, 153, 314, 368, 262, 369, 370, 371, 372, 317, 373,\n",
       "          374, 375, 318, 376, 377, 378, 326, 379, 307, 380, 153, 381,  20,  33,\n",
       "          382, 383, 384, 354,  33, 364,  23, 366, 367,  23, 195, 385,  23,  33,\n",
       "          386, 387, 388, 345, 346, 389,  23, 311, 390, 391, 392, 393, 394, 395,\n",
       "           23, 364, 368, 396,  33, 397,  91, 379, 307,  26, 398, 120, 399,  23,\n",
       "          299,  26, 400, 401,  17,  40, 120,  17, 402, 368, 354, 403, 313,  57,\n",
       "           33, 404, 405, 307, 406, 407,  26, 408,  26,  33, 409,  17, 195, 266,\n",
       "           13, 410, 411, 354, 412, 413, 414, 415,  87, 416, 417,   9,  13, 418,\n",
       "          348,  87,  13, 188, 419,  41, 420, 421, 268, 422, 423, 271, 229,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0]), tensor([0.])),\n",
       " (tensor([424, 425,  23,  19, 286, 312, 153,  40, 426, 427, 428, 429,  23, 184,\n",
       "          286, 331, 430,  75, 431, 432, 433, 168,   5,   6,  87, 324,  23, 202,\n",
       "          434, 435, 436,  57,  30, 437, 438,  23, 439, 286,  13, 440, 441, 317,\n",
       "           13, 442, 340, 429,  17, 443, 444, 445, 446,  23, 184,  24, 389,  25,\n",
       "          447, 268,  33, 448,  75, 449, 450, 451,  23, 452, 153, 453,  30, 454,\n",
       "          262, 455, 429, 456,  33, 448,  26, 457, 446, 458,  24, 459,  23,  17,\n",
       "          266,  23,  13, 460, 461, 202, 199, 462, 268,  33, 293,  26, 463,  33,\n",
       "          464, 195, 465,  19,  36,  37,  61, 466, 467,  57,  33, 460, 461, 215,\n",
       "          317,  33, 468,  55, 469, 470, 268, 471, 472, 473,  26, 474,  17, 475,\n",
       "          160, 354, 398, 476, 477, 478, 479,  26, 480, 160,  23,  33,  40, 481,\n",
       "           69,  55,  71, 482,  26, 483, 484, 168,  33, 485, 354,  13, 486, 487,\n",
       "           17, 488, 489, 490, 491, 492, 493, 159, 494,  47,  55, 151, 199, 495,\n",
       "          268, 496, 155, 497, 484, 498, 499,  55, 469, 470, 268, 471, 472, 473,\n",
       "           26, 474,  17, 475, 160, 354, 398, 476, 477, 478, 479,  26, 480, 500,\n",
       "           55,  71, 482,  26, 483, 484, 168,  33, 485, 354,  13, 486, 487,  17,\n",
       "          488, 489, 490, 491, 492, 493, 159, 494,  47,  55, 151, 199, 495, 268,\n",
       "          496, 155, 497, 484, 498, 499, 501, 428, 502, 429,  23, 503,  80,  81,\n",
       "          504,  83,  84, 505,  23, 227, 506,   9, 457, 507, 477,  77, 150, 508,\n",
       "          509, 510, 511, 352, 476, 477,  26, 474, 160,  23,  33, 512, 195, 513,\n",
       "          514,  20, 515, 516, 484, 168,  33, 517, 518, 519, 520, 521, 522, 523,\n",
       "          524, 525, 526,  13, 527, 101, 508, 509, 510, 528, 529, 496, 497,  26,\n",
       "           27, 484,  13, 530,  57, 398, 531, 532, 498, 533, 534, 428, 502, 429,\n",
       "           23, 503,  80,  81, 504,  83,  84, 505,  23, 535, 536, 537, 114, 538,\n",
       "          195, 539, 540, 541,  17, 153, 460, 461, 417,   9,  36,  23,  17,   7,\n",
       "          542,  33, 543, 217,  26, 544, 545,  20,  33, 546, 547,   9, 548, 159,\n",
       "          112,   9,  77, 549, 550,  94, 482,  26, 551, 552, 458, 553, 217,  26,\n",
       "          554, 429,  23, 435, 460, 461, 555, 556, 557, 558, 559,  80,  81, 560,\n",
       "           83,  84, 505,  23, 561,   7, 562,  33, 563,  57,  33, 564, 565, 566,\n",
       "            9, 567, 568, 569,  80,  81, 570,  83,  84, 505,  23, 571,  23, 572,\n",
       "          573,  23, 574, 575, 576, 107, 577, 578, 579,  80,  81, 580,  83,  84,\n",
       "          505,  23, 581, 582, 583,  80,  81, 584,  83,  84, 505,  23, 585, 537,\n",
       "          539]), tensor([0.])),\n",
       " (tensor([424, 681, 682,  23,   5,   6, 683, 153,  24, 134,  72, 684,  26, 220,\n",
       "           33, 685, 682,  23, 136,  24, 195, 686,  75,  33, 687, 682, 168,  13,\n",
       "          688,  39,  40,  41,  42,  43, 689,  40,  66, 690, 691,  75, 692, 693,\n",
       "           17, 266,   6, 195, 317, 694,  26, 695,  33, 696,  57, 693, 697,  30,\n",
       "          698, 699,  58, 700,  57,   6,  87, 701,  20,   6, 702, 703,  57, 693,\n",
       "          182, 704, 705, 706, 102, 707,  23,  24, 137, 708, 691,  87, 709, 710,\n",
       "          268, 711, 712, 713, 714, 556, 557, 715, 716, 717, 718,  80,  81, 719,\n",
       "           83,  84, 720,  23, 721, 722, 511, 191,  13, 723, 724, 725, 726, 317,\n",
       "            6,  87, 727, 728, 729,  23, 136, 730,  91, 102, 324, 195, 731, 732,\n",
       "           39, 733, 734,  13, 735,  26, 691,  17, 693, 557, 736, 691,  23, 673,\n",
       "          215, 737,  26, 738,  33, 739,  17, 217, 317,  33, 693, 740, 741,  23,\n",
       "           33, 733, 321,   9, 742, 743, 727,  57,   5,   6,  23, 184, 202, 744,\n",
       "          745, 746, 168,  13, 747,  20,  33, 693, 748,  23, 195, 749,  26, 750,\n",
       "          352,  33, 685, 751, 168,  33, 752,  57, 434, 753, 754, 755, 557, 556,\n",
       "          557, 756, 757, 284, 758,  80,  81, 759,  83,  84, 760,  23, 761, 762,\n",
       "           57, 763, 764,  26,  72, 317,  11, 556, 557, 765, 766,  23, 597,  33,\n",
       "          767, 768, 195, 769,  75, 268,  33, 770, 107, 771, 284, 758,  80,  81,\n",
       "          759,  83,  84, 760,  23, 772,  87, 138,  11, 634, 556, 557, 773,  48,\n",
       "          774, 151, 775, 168, 102, 776,  23, 102, 195, 729, 763,  17,  19,  87,\n",
       "          217, 777,  19, 134, 778, 779, 780,  23, 136, 184, 781, 284, 758,  80,\n",
       "           81, 759,  83,  84, 760,  23, 782,  33, 733, 286, 537,  29,  23,  33,\n",
       "          735,  26, 691, 286, 783, 557,  39, 693, 755, 751, 202, 199, 784, 180,\n",
       "           33,   6,  17, 785, 786, 787, 138, 788,  33, 789, 790, 791, 792, 793,\n",
       "          794, 217, 777, 311, 490, 204, 134, 779, 780,  23, 795,  33, 796, 797,\n",
       "           80,  17, 798, 799,  83, 800, 801,  13, 802, 751, 803, 284, 758,  80,\n",
       "           81, 759,  83,  84, 720,  23, 761, 804, 587,  20, 805, 770, 574,  33,\n",
       "            6, 806, 195, 807, 119,  26,   6,  87, 693, 739,  26, 135, 102, 808,\n",
       "          513, 809, 810, 258, 155, 811, 812, 151, 449, 813, 378, 326,  55,  95,\n",
       "          814,  26, 463,  33, 435, 815, 153, 816, 283, 817, 284, 758,  80,  81,\n",
       "          759,  83,  84, 720,  23, 721,  39, 763, 286, 818, 819, 820, 821,  91,\n",
       "          102, 195, 153, 159, 215, 822,  33, 790,  80, 823,  83, 824, 597, 825,\n",
       "          153]), tensor([0.])),\n",
       " (tensor([849, 850, 851,  30, 852, 681, 853, 751,  26, 854,   5,   6, 855,  61,\n",
       "          856,  30, 857,  39, 849, 858,  30, 751,   7, 746, 458, 859,  57,  33,\n",
       "          860, 861, 862,   6,  87, 863,  26, 864, 865,  92,  33, 866,  57, 867,\n",
       "           39, 868, 869, 317, 870,  75,  33, 871, 872,  57, 345, 873, 874, 875,\n",
       "          876,  17, 877, 878, 879, 671, 463, 880, 168,  33, 881,  57,  33, 882,\n",
       "          883, 184, 482,  26, 884, 542,  57, 885, 886, 340, 887,  17, 888,  23,\n",
       "          850, 889, 424, 102, 890, 682,  23, 891, 283, 892,  33, 893,  75, 894,\n",
       "           75, 865,  17,  75,  11,  33, 895, 896, 897, 283, 898, 153,  33,  71,\n",
       "           26, 899, 900, 901, 902, 340,  33, 903,  17, 153,  13, 904, 905, 173,\n",
       "          906,  72, 907,  39, 849,  89, 317,  26, 908,  75, 909,  57, 910, 184,\n",
       "          151, 199, 911, 180, 398, 912,  23,  17, 153, 195, 434, 808,   6, 913,\n",
       "           26, 914, 915, 850, 851, 880,  75, 597, 439, 286, 449, 916, 168,  33,\n",
       "          917,  92, 434, 918, 919,  23,  92,  33, 920,  57, 921, 215, 922, 168,\n",
       "           49, 355,  17, 434, 923, 924,  57, 925, 913,  26, 926, 927,  23, 928,\n",
       "           17, 929, 930,  23, 681, 931, 283,  26, 932, 317,  33, 933,  57,  33,\n",
       "          934,  17,  26, 864, 114, 168,  33, 935,  57, 936, 881,  23, 937, 183,\n",
       "           75, 938,  23, 101, 880,  23, 439, 195, 449, 916, 168,  33, 917,  23,\n",
       "           24, 889, 880, 939, 940,  33, 941,  57, 217, 331, 942,  17, 378, 943,\n",
       "           19, 195, 217,  26, 151,  13, 916,  26, 944,  44,  87, 945,  23,  24,\n",
       "          946, 362,  49, 947, 217,  72, 948,  92, 159, 262, 168,  33, 912,  57,\n",
       "          949,  39, 849, 380, 153, 950,  17, 951, 262, 952, 184, 953,  26, 954,\n",
       "           13, 955, 916,  26, 956, 168, 949, 787,  25,  26,  18, 398, 155,  23,\n",
       "          398, 957,  23,  17, 398, 958,  23, 850, 889, 959, 286, 449, 960, 574,\n",
       "          961, 962,  75,  13, 963, 964,  91,  26, 151,  13, 965, 706, 966,  23,\n",
       "          159, 262, 967,  57, 562,  17, 968, 542,  57,  33, 969,  91,  26,  72,\n",
       "          970, 971, 972, 398, 973, 262, 974,  90, 268,  33, 975,  17, 976, 153,\n",
       "          977, 183, 184, 151,  26,  18, 398, 957, 978, 979, 154, 980, 981, 215,\n",
       "          982, 168,  33, 981,  57, 951,  17, 950,  23, 850, 380, 983, 671, 463,\n",
       "           33, 984,  57, 985, 986, 911,  26, 351,  29, 168,  49, 531, 280, 671,\n",
       "          463,  33, 984,  57, 987,  57, 988, 184,  46, 217, 989,  26, 513, 809,\n",
       "           23, 136, 990, 180, 398, 958,  23,  18, 991, 398, 992, 993, 994,  26,\n",
       "          995]), tensor([0.])),\n",
       " (tensor([  39,  696,   57,  997,   57,  998,  999,   17, 1000,  155,   57, 1001,\n",
       "          1002,   26,  463,  449, 1003, 1004,   23,  673,  151, 1005, 1006,  153,\n",
       "          1007,   26,   72, 1008, 1009,   17, 1010, 1011, 1012, 1013,  268,   33,\n",
       "           843,   57, 1014, 1015, 1008,   13, 1016, 1017,   57, 1018, 1019,   23,\n",
       "           408,  168,   13, 1020, 1021,  354,   13, 1022,   17, 1023, 1024,   23,\n",
       "           317, 1025,  348, 1007,   26,   72, 1008, 1009,   17, 1026,   23,  542,\n",
       "           102,  195, 1027,  195,  385, 1028, 1015, 1019,  286,  168,  448, 1029,\n",
       "           574,  191, 1030,   57, 1031, 1032,  168,   30, 1033,   23,  136,  311,\n",
       "            94,  463,   33, 1017,   94,   71, 1034,  153,  490, 1035, 1036, 1037,\n",
       "          1038,  215,  217, 1039,   26, 1040,   17, 1041,  183,  168,  398, 1042,\n",
       "          1043,   33, 1044,   94,  215,   91,   26,  463,   23, 1045, 1015, 1046,\n",
       "          1018,  562,  153,   33,  998,   25,  398, 1047, 1048,  317,  844,  159,\n",
       "           262, 1049, 1018, 1019,   23,  136,   55,  602,  673,   11,  190,  153,\n",
       "           439,   71,   72, 1050,   97,   57, 1051, 1052,   26, 1053,  809,   33,\n",
       "           110,   57, 1054,   57, 1055,  574, 1047, 1056, 1057,   57,  191,  286,\n",
       "           216,   13, 1058, 1059, 1060,   23,   19, 1061,   61,   72, 1062,   92,\n",
       "          1063, 1064, 1065, 1066,  160,  153,  102,  963,  663,   71,  599,   72,\n",
       "            33, 1067, 1068, 1069, 1070,   23, 1071,  195,  191, 1072,   23, 1012,\n",
       "            87, 1073, 1074,  286, 1075, 1076,   57, 1014, 1015, 1019,  557, 1077,\n",
       "          1036,  191,   33, 1078, 1079,   57,  102,  963,  663, 1080,   72,   23,\n",
       "           102,  195, 1081, 1082,   39,  998,  184,  219,  102, 1083,   26, 1084,\n",
       "           168,  613,   75,   13,  118,   23,  118,  546,  136,  191,   94, 1085,\n",
       "          1086, 1087,  159,  494,   13,  769, 1088,  844,   33, 1089, 1090, 1091,\n",
       "            23,  230,   26,  151,   33, 1092, 1093,   26, 1094, 1095,  107,  959,\n",
       "            23, 1096,   23,  195, 1028,  673,  174, 1097, 1098, 1099, 1077, 1100,\n",
       "           168, 1101,  134,  102,  151, 1102,  550, 1014, 1015, 1019,   25,  199,\n",
       "          1103, 1104, 1105, 1009,   17, 1026,   23,   17,  956, 1106,   26, 1107,\n",
       "          1108,   75, 1069, 1109, 1110, 1111,  428, 1112,  271,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([   5,    6,  744,   13, 1113, 1114,   57,   30,  682,   20,   30,  693,\n",
       "          1115,   23, 1116,   33, 1117,  682,   24,   87,  742,  187,  795, 1118,\n",
       "            33, 1119,   57, 1120,  348, 1121,  151,  199,   13,  279, 1122,  542,\n",
       "             7,  458,  153,   23,    6, 1123,   29,   20,  293, 1124, 1125,  269,\n",
       "          1126,  317, 1127,  685,   13,  386, 1128, 1126, 1129,   26, 1130,  168,\n",
       "            13,  233, 1131,   39,  386, 1132, 1126,   87, 1133,  168, 1134,   57,\n",
       "          1135, 1136,  102, 1137,   23,   92,  940,   92, 1138, 1139,  180, 1140,\n",
       "          1141,   33,  288, 1142,   23, 1143,   23,    6,  495, 1126,  354,   13,\n",
       "           551, 1144,  173,  293, 1124, 1125,  269, 1126,   23,   33,  663,  168,\n",
       "           554,   23, 1145,  354, 1146, 1147, 1148,   23,   57,   33, 1149,  318,\n",
       "          1150,  289,   80,  182, 1018, 1151, 1152, 1153,  392,   83,   72, 1154,\n",
       "          1155, 1156,   75, 1157,   87,  304,  268, 1150, 1158, 1159,  289,  107,\n",
       "             6, 1160,  173,  293, 1124, 1125,  269, 1126,   23,   33,  663,  168,\n",
       "           554,   23, 1145,  354, 1146, 1147, 1148,   23,   57,   33, 1149,  318,\n",
       "          1150,  289,   80,  182, 1018, 1151, 1152, 1153,  392,   83,   72, 1154,\n",
       "          1155, 1156,   75, 1157,   87,  304,  268, 1150, 1158, 1159,  289,  107,\n",
       "             5,   79,    6,   80,   81,   82,   83,   84, 1161,   23,  227,  506,\n",
       "             9,  457, 1162, 1124, 1125,  269, 1126,  195, 1163,   33, 1164,   26,\n",
       "          1130,  354,  967, 1165, 1166,  746,   26,  513,  107,   77,   77,   77,\n",
       "             5,   79,    6,   80,   81,   82,   83,   84, 1161,   23, 1167,   23,\n",
       "           293, 1168, 1147, 1169, 1170,   23,  299,   26,   81, 1171,    5,   79,\n",
       "             6,   80,   81,   82,   83,   84, 1161,   23, 1172,   11,   57,   33,\n",
       "          1173,   20,    6,   87, 1174,   23,   24,   87, 1175,   30,  397,  180,\n",
       "          1176, 1177, 1126,  744, 1178,   57,   30, 1179,  168,   33,  914, 1180,\n",
       "          1181,   17,  266,   24,   87,  331,  495,  268,   33, 1182, 1183,    6,\n",
       "           202,  199, 1184,  222,  317,   30, 1185,   57,   30, 1157, 1186, 1155,\n",
       "          1156,   75, 1018, 1187,  775,  168, 1188,  711,  153,    6,   87,   88,\n",
       "            91,  269, 1126,  195,   13, 1189, 1190,   57,   33, 1191, 1192,   33,\n",
       "          1193,  153, 1126,  259, 1150, 1194,   92,   13, 1195, 1196,  547,    9,\n",
       "           135,  326, 1197,  311,  673,  829,   20,   33, 1198,  109,  711,   88,\n",
       "           286, 1199, 1200, 1201,  388,    6, 1202,  490, 1203,  542,   24,  939,\n",
       "            30,   60,   71, 1204,  484,  855, 1205,   26, 1206,  348,   87,  640,\n",
       "            13, 1207,   23, 1208,  268, 1209, 1210, 1211,  271,  229,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([1043,   33, 1212,   57,  972, 1005, 1213,  553,  153, 1214,    5,    6,\n",
       "            87, 1215,   26,  310, 1216,  180, 1217,   33,  860, 1218,   23,   33,\n",
       "            15,  302, 1219, 1220,   13,  386,  317, 1221, 1222, 1223,   33,   98,\n",
       "            87, 1224,   20,  217, 1175,   30, 1100,   17,  378, 1009,  684,  153,\n",
       "          1224, 1225,   26,   33, 1226,   23,  684,  168, 1227,   23,    6, 1228,\n",
       "           260,   33, 1229, 1230,   23, 1231,   91,   33, 1232,   57,   33, 1233,\n",
       "          1234,   23,  597,   24,  672,  134,   72, 1235,   17, 1236,  168,  916,\n",
       "           268, 1237,   22,   23,   24, 1238,   23, 1239,   25, 1240,  199, 1241,\n",
       "            26,  952,   20, 1242,   13, 1243,  153,   30,   59,  262,  465,   26,\n",
       "           174,   24, 1244,  101,   13,  333,  458, 1245,   30, 1246, 1247,  286,\n",
       "          1248,  153,    6,  393, 1249,  180,   13, 1250,  153,   13,  320, 1251,\n",
       "            23,  775, 1252, 1253, 1254, 1255,   23,   25, 1256,  114,    7,  222,\n",
       "            33,  339,  354,   30, 1257,   39,  800, 1258,  378,  154, 1239,   25,\n",
       "           199, 1241,  102,   74,   23,   17, 1259, 1260,  180, 1261,   80,   13,\n",
       "          1262,  217,  317,   33, 1233, 1234,   83,   23, 1263,  180, 1264,   80,\n",
       "           138,  217, 1259,   83,   23,   17, 1265,  180, 1266,   80, 1267,   13,\n",
       "          1268,  972,  107,   83,   23,   17,    6, 1269,   30, 1270,   20, 1271,\n",
       "            26, 1272,  168,   33, 1273,  184, 1274,   26,   33, 1219,  317, 1275,\n",
       "            57, 1276,   23,   17,  184,  262, 1277, 1145,  354, 1278, 1279,  774,\n",
       "            75,   33, 1226,   23,    6,  537,   29, 1280, 1262,   75, 1281, 1282,\n",
       "            92,   24, 1283,  557, 1261,  286,   13, 1284, 1285,   23,   33,  155,\n",
       "            57, 1266,  134,  599,  513,  684,   26,  398, 1286,  258,  159, 1287,\n",
       "            33, 1288,   57,   76,   23,   17,  952,  180, 1264,   11,  151, 1289,\n",
       "          1290,   33, 1291,  316,  742,  268,   33,  541,   23,   33, 1292,  272,\n",
       "            57,  748, 1293,  153, 1294, 1242, 1295,  286, 1296, 1154,    6,   87,\n",
       "          1297, 1298,   17,   30, 1251, 1254, 1255,   87, 1299, 1252, 1300,   23,\n",
       "            19,  134,   72,  449, 1301,  550,   13, 1302,   57, 1303, 1304, 1305,\n",
       "          1306,  352,  153,   33, 1250,  168, 1307,   25,   33, 1308, 1309, 1310,\n",
       "            92, 1311,   75,   33,   98,   26, 1312,   30, 1313, 1314,  348,  286,\n",
       "          1255,   23,  458,   11,   23,  184,  286, 1315,   75,   33,  120, 1316,\n",
       "          1317,  153,    6,  858,   20,   30, 1318, 1297,  195,   13, 1319,   26,\n",
       "            76,   23,   17,   24,  547,    9, 1320, 1321,  153,  102, 1262, 1322,\n",
       "          1323,  897,   87,  562,  153, 1324, 1325,  180,  414,  415,  215,  317,\n",
       "           398, 1100,   92,  673, 1326, 1110, 1111,  591, 1327,  271,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([1328,  155,  151, 1329,   33, 1330, 1141,   33, 1331,  153,    5,    6,\n",
       "           195, 1332, 1333,   26,  223,  434, 1334,   39, 1335,  195,   23, 1336,\n",
       "          1337, 1338,  514,  281,   33,  155,   87, 1339,   23,  542,  159,  173,\n",
       "          1340,  829,  101, 1336,  168,   33, 1341, 1342,  959,  286, 1343,  268,\n",
       "          1344,  428, 1345,    7,   13,  964,   57,  346,  260,    5,    6,   87,\n",
       "          1346,   23,  168,   13, 1347,  168,   39, 1348,  537, 1144,   26, 1349,\n",
       "           434, 1350, 1043, 1331,   23,  550,   94,  728,  829, 1351,   20,  191,\n",
       "           195, 1352,  514,  266,   33, 1353, 1354,   57, 1355, 1356, 1242,   92,\n",
       "            33, 1357, 1358,   17,   33, 1359, 1360,   57, 1361,   92,  940,   33,\n",
       "           403, 1362,   57,   33,  291,   57,  292,  168,  294,   26,  513,  458,\n",
       "             6,   87,   40, 1195, 1363,   23,  363,  391,  375,  318, 1150,   23,\n",
       "            17,   94,  151,   33, 1364,   57,  434, 1365,  671,  215, 1324,  497,\n",
       "           940,  317,   49, 1366, 1367,   23,   44, 1368,   57, 1369,   23,   33,\n",
       "           272,   57, 1370,   23, 1240,  202,   13, 1371, 1372,   57, 1140,  184,\n",
       "           215, 1373,   13, 1374,  289,   26,   33, 1375,  314, 1376,  289,   23,\n",
       "           354,   33, 1377, 1378,   57, 1379,   17, 1354,   33,  813,  153,    6,\n",
       "          1061,  151, 1380,  742, 1321,  263,  354,   33, 1381,  168,  294,   26,\n",
       "          1382,   33,  363, 1383,  168,   30, 1384,   57,  153,  195,    7,   75,\n",
       "          1385,   23, 1386, 1004,   23,  673,  151,    6, 1245,  860,  861, 1387,\n",
       "          1388, 1389,  527,   17, 1390,  980, 1391,  168,   33,  860,  861,  184,\n",
       "           254, 1180,    6,   87,  553,   26,  257, 1392, 1393,  311,   19, 1394,\n",
       "            26, 1395,   57,  865,   92,   33,  866,   57,   33, 1396, 1397, 1398,\n",
       "            23,   44, 1399,   23,  184,  195, 1400, 1401, 1402,   23,  202,   25,\n",
       "           119,   57,    6,   87, 1403, 1404,  557, 1405, 1406, 1125, 1407, 1408,\n",
       "          1409,   39, 1410, 1411, 1412,   26,    6,   87, 1413, 1414,   23, 1127,\n",
       "            23,   17,  689,   33,  553,  557,    6, 1415, 1416,   26, 1417, 1180,\n",
       "          1418,  153, 1419, 1420,  514,  168, 1421,   26, 1422, 1423, 1424,  317,\n",
       "           865,  195, 1425, 1426, 1427,   81,   82, 1428, 1429, 1430,   17, 1431,\n",
       "           180, 1432, 1433, 1400, 1434,  168, 1435,   23, 1436, 1437, 1407, 1408,\n",
       "          1438,   80,   81, 1439,   83,   84, 1440,   23, 1441, 1438,  195, 1442,\n",
       "            23,   57,  740,    6,  195, 1443,    7,  101,  434, 1444,   23,   17,\n",
       "           187,  154,  155,  168,   33, 1445,  215, 1446,  311,   19, 1394,   26,\n",
       "           102, 1447, 1448,   23,  168,  597,   33, 1449, 1416,   26, 1450,   17,\n",
       "            33,   34, 1451,   57,   33, 1452, 1091,  195,   33,  120, 1183, 1453,\n",
       "            12,   23,  673,  190,   33, 1454, 1369,   71,  217,   72,   33, 1455,\n",
       "           317,    6,  153,  159,  215, 1039,   26, 1456,  348,   87,  546,   26,\n",
       "           494,   29,   17]), tensor([0.])),\n",
       " (tensor([1471,  311,   94, 1080,  151,  672,  673,   95,  494,   13,  633,  180,\n",
       "          1472,  155, 1473,    5,    6,   87, 1474,   17, 1475,   30, 1476, 1477,\n",
       "          1478,   23,   13, 1479, 1480, 1481,  434, 1477,  153,   87,  196,  136,\n",
       "           155,  200,   61, 1324,   57,  183, 1482, 1483,   76, 1484, 1485,  195,\n",
       "           351,   26, 1486,  102, 1477,   23,  537, 1487,  812,   23,   66,    6,\n",
       "            23,  317,  681,  853,   17,   23,  940,   23,  673, 1488,  352,   13,\n",
       "           936,  168,   49, 1489,  638,   26, 1490, 1491,   23,   33, 1492,  195,\n",
       "           196,  136,  155, 1493,   11,  634,    6,   75,   11,   33, 1494,   24,\n",
       "          1495,    9,  779, 1496,   39, 1477, 1497,   13, 1498,  354,   13,  936,\n",
       "          1499, 1500,    6,   75, 1501,  684, 1502,  681,   23,  597,  599,   89,\n",
       "           809,   80,  439,  215,   61, 1503,   57,   66,  691, 1128, 1502,  681,\n",
       "          1504,   83,  229, 1505,  663, 1506,  114,   75, 1507,   30, 1508, 1192,\n",
       "            76, 1484,  768,  153, 1509,   12, 1510,  215, 1500,    6,   75,  331,\n",
       "          1242,   13,   73,   17, 1511, 1512,  820, 1513,   87,  778,  742,  191,\n",
       "            24,   87, 1496,  198,   87,  236,   11, 1514,   57, 1515, 1516, 1517,\n",
       "          1518,   23,   33,  166, 1519, 1394,   26, 1520,  311, 1472, 1521,  722,\n",
       "            87,  191,   19,  195, 1522, 1519, 1523,  587,  184, 1524,    9, 1083,\n",
       "           102,   97,   57, 1525, 1526, 1527,  102, 1477, 1528, 1529,  557,   39,\n",
       "          1100,   33, 1530,  215, 1531,   26, 1532, 1533,  195, 1534, 1535,   17,\n",
       "          1536,   39,  663,  202,  742,  196,   23,   17,   30, 1537,   71, 1538,\n",
       "            33,   34, 1539,   12,  184,  215, 1500, 1540, 1541,   23,   19,   71,\n",
       "          1542,  434, 1543, 1544,   57,  941,  222,  159,  137, 1545,  398, 1546,\n",
       "            17,  463,  159, 1547,  199, 1548,  268,   13, 1549,  663,  354,   13,\n",
       "           279, 1550,   30, 1551, 1083,   75,  102,   97,   57, 1552,  195,   23,\n",
       "            20,  820,   23, 1553,   57,   30, 1120,  959, 1477,  195, 1554, 1110,\n",
       "          1111, 1555, 1556,  271,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Net class\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(TEXT_LENGTH, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=435, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "8\n",
      "torch.Size([64, 435])\n"
     ]
    }
   ],
   "source": [
    "# initialize net and print parameters\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize net with backprop; 3 epochs\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# EPOCHS = 3\n",
    "\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for i, padded_text in enumerate(padded_texts, start=0):\n",
    "#         # data is a batch of featuresets and labels\n",
    "#         #print(true_fake_dataset[i])\n",
    "#         X = padded_text\n",
    "#         #print(X)\n",
    "#         y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "#         if i % 500 == 0:\n",
    "#             print(i)\n",
    "#         #print(y)\n",
    "#         net.zero_grad()\n",
    "#         X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float, NN must read in float\n",
    "# #         print(X_float)\n",
    "#         output = net(X_float.view(-1, MAX_TEXT_LENGTH))\n",
    "#         loss = F.nll_loss(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and print accuracy\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, padded_text in enumerate(padded_texts, start=0):\n",
    "#         X = padded_text\n",
    "#         y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "#         X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float\n",
    "#         output = net(X_float.view(-1, MAX_TEXT_LENGTH))\n",
    "# #         print(torch.argmax(output))\n",
    "#         for idx, i in enumerate(output):\n",
    "#             if torch.argmax(i) == y[idx]:\n",
    "#                 correct += 1\n",
    "#             total += 1\n",
    "\n",
    "# print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to this source for glove embedding code + initial NN code\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "# import glove relations\n",
    "\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'6B.50.dat', mode='w')\n",
    "\n",
    "\n",
    "with open(f'glove.6B.50d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "#         print(vectors[idx])\n",
    "    \n",
    "# print(vectors[20000000:20000050])\n",
    "    \n",
    "# save outputs to disk    \n",
    "\n",
    "vectors = bcolz.carray(vectors[1:].reshape((400001, 50)), rootdir=f'6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create glove dictionary\n",
    "\n",
    "vectors = bcolz.open(f'6B.50.dat')[:]\n",
    "words = pickle.load(open(f'6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.072617, -0.51393 ,  0.4728  , -0.52202 , -0.35534 ,  0.34629 ,\n",
       "        0.23211 ,  0.23096 ,  0.26694 ,  0.41028 ,  0.28031 ,  0.14107 ,\n",
       "       -0.30212 , -0.21095 , -0.10875 , -0.33659 , -0.46313 , -0.40999 ,\n",
       "        0.32764 ,  0.47401 , -0.43449 ,  0.19959 , -0.55808 , -0.34077 ,\n",
       "        0.078477,  0.62823 ,  0.17161 , -0.34454 , -0.2066  ,  0.1323  ,\n",
       "       -1.8076  , -0.38851 ,  0.37654 , -0.50422 , -0.012446,  0.046182,\n",
       "        0.70028 , -0.010573, -0.83629 , -0.24698 ,  0.6888  , -0.17986 ,\n",
       "       -0.066569, -0.48044 , -0.55946 , -0.27594 ,  0.056072, -0.18907 ,\n",
       "       -0.59021 ,  0.55559 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate weights matrix for entire vocab of encoder\n",
    "\n",
    "matrix_len = len(encoder.vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(encoder.vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296562, 50)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = torch.Tensor(weights_matrix).size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.Tensor(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, batch_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.lin = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        embedding = self.embedding(inp)\n",
    "#         print(embedding, \"Embedding output\")\n",
    "#         print(embedding.size())\n",
    "        output, h_n = self.gru(embedding, self.init_hidden(self.batch_size))\n",
    "#         print(output, \"GRU output (output)\")\n",
    "#         print(output.size())\n",
    "#         print(x.output.view(seq_len, batch, num_directions, hidden_size), \"GRU output, (output) unpacked\")\n",
    "#         print(x.h_n, \"GRU output (h_n)\")\n",
    "#         print(x.h_n.view(num_layers, num_directions, batch, hidden_size), \"GRU output (h_n) unpacked\")\n",
    "        x = F.relu(self.lin(output[:,-1,:]))\n",
    "#         print(x, \"Relu output\")\n",
    "#         print(x.size())\n",
    "        sm = F.softmax(x, dim=1)\n",
    "#         print(sm, \"Softmax output\")\n",
    "#         print(sm.size())\n",
    "#         l_sm = F.log_softmax(x, dim=1)\n",
    "#         print(l_sm, \"Log_softmax output\")\n",
    "#         print(l_sm.size())\n",
    "        return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "toynet = ToyNN(weights_matrix, 20, 3, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyNN(\n",
      "  (embedding): Embedding(296562, 50)\n",
      "  (gru): GRU(50, 20, num_layers=3, batch_first=True)\n",
      "  (lin): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n",
      "15\n",
      "torch.Size([296562, 50])\n"
     ]
    }
   ],
   "source": [
    "print(toynet)\n",
    "\n",
    "params = list(toynet.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 435, 50])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(296562, 50)\n",
    "\n",
    "output = embedding(torch.stack(padded_texts[0:10]))\n",
    "# plus_batch = torch.Tensor([3, output])\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = [torch.Tensor([1, 2, 3, 4]), torch.Tensor([5, 6, 7, 8])]\n",
    "tens = torch.stack(tens)\n",
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_padded_texts = torch.stack(padded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44898, 435])\n",
      "44898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(stacked_padded_texts.size())\n",
    "print(len(padded_texts))\n",
    "len(padded_texts[10923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = toynet(stacked_padded_texts[0:10], torch.zeros([20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44890"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "# Guide used to code training loop and splitting of dataset, as well as general understanding of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-621f5a53205a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# truncate dataset to have batch sizes of 10 (TEMPORARY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m44897\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m44896\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m44895\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "# truncate dataset to have batch sizes of 10 (TEMPORARY)\n",
    "\n",
    "trainset.pop(44897)\n",
    "trainset.pop(44896)\n",
    "trainset.pop(44895)\n",
    "trainset.pop(44894)\n",
    "trainset.pop(44893)\n",
    "trainset.pop(44892)\n",
    "trainset.pop(44891)\n",
    "trainset.pop(44890)\n",
    "\n",
    "\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44890"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into batches and shuffle them\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_loader = DataLoader(dataset=trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# trainset should be \"dataset\", FIX LATER\n",
    "# Splits trainset into 200 training articles and 44690 validation articles\n",
    "train_dataset, val_dataset = random_split(trainset, [40000, 44890-40000])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=10)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(iter(train_loader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y, lineno):\n",
    "        # initialize accuracy counter (averaging these accuracies is accurate, as all batches are size 10)\n",
    "        correct = 0\n",
    "        total = len(x)\n",
    "        \n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x, torch.zeros([20]))\n",
    "        \n",
    "        if lineno % 100 == 0:\n",
    "            print(yhat, \"yhat\")\n",
    "        # Computes loss\n",
    "        # Create 1D tensor from tensor of single-element lists (FIX IN FUTURE, THIS CAN BE OPTIMIZED EARLIER)\n",
    "#         init_tensor = next(iter(train_loader))[1]\n",
    "        init_tensor = y\n",
    "#         print(init_tensor, \"init_tensor\")\n",
    "        cat_list = []\n",
    "        for val in enumerate(init_tensor):\n",
    "#             print(val, \"val\")\n",
    "            cat_list.append(val[1][0])\n",
    "#             print(yhat, \"predicted vector\")\n",
    "#             print(yhat[val[0]], \"predicted val\")\n",
    "            # increment accuracy counter\n",
    "            if val[1][0] == torch.argmax(yhat[val[0]]):\n",
    "#                 print(val[1][0], \"target val\")\n",
    "#                 print(yhat[val[0]], \"predicted val\")\n",
    "                correct += 1\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = correct/total\n",
    "#         print(accuracy, \"accuracy\")\n",
    "                \n",
    "        y_target = torch.LongTensor(cat_list)\n",
    "#         print(y_target)\n",
    "        \n",
    "        loss = loss_fn(yhat, y_target)\n",
    "        \n",
    "        \n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss and accuracy\n",
    "        return loss.item(), accuracy\n",
    "#         return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Creates optimizer and train step\n",
    "lr = 1e-5\n",
    "n_epochs = 3\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(toynet.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7218, 0.2782],\n",
      "        [0.6535, 0.3465],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.6488, 0.3512],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.7466, 0.2534],\n",
      "        [0.6763, 0.3237]], grad_fn=<SoftmaxBackward>) yhat\n",
      "0\n",
      "tensor([[  1344,    374,    375,  ...,      0,      0,      0],\n",
      "        [  1920,    311,    219,  ...,  18521,     20, 191717],\n",
      "        [ 47166, 160856,   5047,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,   2405,   9431,  ...,      0,      0,      0],\n",
      "        [232939,     80,   9040,  ...,   1609,    237,   3529]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7493, 0.2507],\n",
      "        [0.7493, 0.2507],\n",
      "        [0.7493, 0.2507],\n",
      "        [0.6882, 0.3118],\n",
      "        [0.6591, 0.3409],\n",
      "        [0.7493, 0.2507],\n",
      "        [0.7004, 0.2996],\n",
      "        [0.6779, 0.3221],\n",
      "        [0.6752, 0.3248],\n",
      "        [0.7493, 0.2507]], grad_fn=<SoftmaxBackward>) yhat\n",
      "100\n",
      "tensor([[180734,     23,  20169,  ...,      0,      0,      0],\n",
      "        [257231,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  2108,     87,     33,  ...,     23, 188845,     26],\n",
      "        [  7000,     94,     23,  ...,   1894,   1110,   1111],\n",
      "        [  1043,     33,   1448,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7517, 0.2483],\n",
      "        [0.6936, 0.3064],\n",
      "        [0.6856, 0.3144],\n",
      "        [0.6924, 0.3076],\n",
      "        [0.6708, 0.3292],\n",
      "        [0.6996, 0.3004],\n",
      "        [0.6957, 0.3043],\n",
      "        [0.7050, 0.2950],\n",
      "        [0.7517, 0.2483],\n",
      "        [0.7517, 0.2483]], grad_fn=<SoftmaxBackward>) yhat\n",
      "200\n",
      "tensor([[ 6450,    19,    26,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,   268,    33,   860],\n",
      "        [ 3818,   662,    12,  ...,  1480,   168,    76],\n",
      "        ...,\n",
      "        [ 4813,     9,   891,  ...,  1324, 11294,  2394],\n",
      "        [    5,     6,   195,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7530, 0.2470],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.6919, 0.3081],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.6237, 0.3763],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.7530, 0.2470],\n",
      "        [0.6930, 0.3070]], grad_fn=<SoftmaxBackward>) yhat\n",
      "300\n",
      "tensor([[281536,     80,   9040,  ...,      0,      0,      0],\n",
      "        [236697,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3019,    279,     75,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [245918,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [140748,     23, 140018,  ...,      7,    191,    102]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7544, 0.2456],\n",
      "        [0.6759, 0.3241],\n",
      "        [0.7544, 0.2456],\n",
      "        [0.7544, 0.2456],\n",
      "        [0.6603, 0.3397],\n",
      "        [0.7476, 0.2524],\n",
      "        [0.7158, 0.2842],\n",
      "        [0.7544, 0.2456],\n",
      "        [0.7544, 0.2456],\n",
      "        [0.7544, 0.2456]], grad_fn=<SoftmaxBackward>) yhat\n",
      "400\n",
      "tensor([[229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3155,   1324,    497,  ...,    123,  10686,    168],\n",
      "        [  2336,     19,    674,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [268251,     23,    391,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7562, 0.2438],\n",
      "        [0.7562, 0.2438],\n",
      "        [0.6839, 0.3161],\n",
      "        [0.7562, 0.2438],\n",
      "        [0.6561, 0.3439],\n",
      "        [0.6847, 0.3153],\n",
      "        [0.7001, 0.2999],\n",
      "        [0.7562, 0.2438],\n",
      "        [0.6833, 0.3167],\n",
      "        [0.7025, 0.2975]], grad_fn=<SoftmaxBackward>) yhat\n",
      "500\n",
      "tensor([[251749,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4991, 139735, 153298,  ...,   2382,  51678,    348],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  65865,     17,     33],\n",
      "        [  6953,     23,     33,  ...,     39,    724,     87]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.6716, 0.3284],\n",
      "        [0.7581, 0.2419],\n",
      "        [0.6812, 0.3188],\n",
      "        [0.7142, 0.2858],\n",
      "        [0.7581, 0.2419],\n",
      "        [0.6993, 0.3007],\n",
      "        [0.6945, 0.3055],\n",
      "        [0.7581, 0.2419],\n",
      "        [0.7032, 0.2968],\n",
      "        [0.7581, 0.2419]], grad_fn=<SoftmaxBackward>) yhat\n",
      "600\n",
      "tensor([[226803,     80,   9040,  ...,   3675,  10010,     17],\n",
      "        [ 11477,  62205,     80,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    684,     20,   1672],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ..., 221684,     24,    380],\n",
      "        [  6879,    352,    155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7045, 0.2955],\n",
      "        [0.7068, 0.2932],\n",
      "        [0.7095, 0.2905],\n",
      "        [0.6854, 0.3146],\n",
      "        [0.7592, 0.2408],\n",
      "        [0.7592, 0.2408],\n",
      "        [0.7098, 0.2902],\n",
      "        [0.7136, 0.2864],\n",
      "        [0.7592, 0.2408],\n",
      "        [0.7097, 0.2903]], grad_fn=<SoftmaxBackward>) yhat\n",
      "700\n",
      "tensor([[239157,     80,   9040,  ...,   1324,  20646,     23],\n",
      "        [ 11456,     80,   9040,  ...,   3214,   3540,    873],\n",
      "        [229431,     80,   9040,  ...,    380,     23,     26],\n",
      "        ...,\n",
      "        [    39,    985,   1445,  ...,     26,     72,   1373],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   424,    870,     23,  ...,    119,   5865,     26]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7150, 0.2850],\n",
      "        [0.7113, 0.2887],\n",
      "        [0.7610, 0.2390],\n",
      "        [0.7111, 0.2889],\n",
      "        [0.7610, 0.2390],\n",
      "        [0.6974, 0.3026],\n",
      "        [0.7610, 0.2390],\n",
      "        [0.7610, 0.2390],\n",
      "        [0.7069, 0.2931],\n",
      "        [0.7610, 0.2390]], grad_fn=<SoftmaxBackward>) yhat\n",
      "800\n",
      "tensor([[     5,      6,     87,  ...,    173,   1709,  15605],\n",
      "        [    39,    785,   3523,  ...,    180,   1465,   1466],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1643,     66,      6,  ...,      0,      0,      0],\n",
      "        [     6,    195,   1194,  ...,     17,    219,   1050],\n",
      "        [    80, 158104,   2711,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.6632, 0.3368],\n",
      "        [0.7628, 0.2372],\n",
      "        [0.6951, 0.3049],\n",
      "        [0.6974, 0.3026],\n",
      "        [0.6977, 0.3023],\n",
      "        [0.7628, 0.2372],\n",
      "        [0.6976, 0.3024],\n",
      "        [0.7628, 0.2372],\n",
      "        [0.7130, 0.2870],\n",
      "        [0.6704, 0.3296]], grad_fn=<SoftmaxBackward>) yhat\n",
      "900\n",
      "tensor([[   959,     40,   2357,  ...,  53361,   2220,    463],\n",
      "        [269228,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4367,   5063,    195,  ...,  52431,     83,   4376],\n",
      "        ...,\n",
      "        [268289,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   591,   8657,     87,  ...,     23,    195,   4854],\n",
      "        [   424,   1746,     23,  ...,    557,  49541,   2213]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7663, 0.2337],\n",
      "        [0.6978, 0.3022],\n",
      "        [0.7663, 0.2337],\n",
      "        [0.7663, 0.2337],\n",
      "        [0.7663, 0.2337],\n",
      "        [0.7086, 0.2914],\n",
      "        [0.7663, 0.2337],\n",
      "        [0.6607, 0.3393],\n",
      "        [0.6638, 0.3362],\n",
      "        [0.7663, 0.2337]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1000\n",
      "tensor([[ 35615,  36144,  86707,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  22406,  10751,     17],\n",
      "        [  4041,   2834,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ...,  34418,     23,   9866],\n",
      "        [244117, 244117,     23,  ...,    991,     33,  10841],\n",
      "        [226803,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.6830, 0.3170],\n",
      "        [0.7676, 0.2324],\n",
      "        [0.7029, 0.2971],\n",
      "        [0.6710, 0.3290],\n",
      "        [0.7676, 0.2324],\n",
      "        [0.7208, 0.2792],\n",
      "        [0.6958, 0.3042],\n",
      "        [0.6643, 0.3357],\n",
      "        [0.6609, 0.3391],\n",
      "        [0.7485, 0.2515]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1100\n",
      "tensor([[ 35615,  36144,  86707,  ...,    293, 207525, 207422],\n",
      "        [   158,      8,      9,  ...,      0,      0,      0],\n",
      "        [ 14238,     87,    945,  ...,   5901,     23,   2672],\n",
      "        ...,\n",
      "        [  3032,    546,     13,  ...,    557,   1505,  31885],\n",
      "        [  2336,     33,   8769,  ...,     23,     30,  92610],\n",
      "        [   959,     74,    286,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7688, 0.2312],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.7018, 0.2982],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.7112, 0.2888],\n",
      "        [0.6786, 0.3214],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.7688, 0.2312]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1011,   1375,    289,  ...,      0,      0,      0],\n",
      "        [ 39680,   7544,    195,  ...,    554,  55586,  27599],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 35615,  36144,  86707,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7698, 0.2302],\n",
      "        [0.7698, 0.2302],\n",
      "        [0.6756, 0.3244],\n",
      "        [0.7158, 0.2842],\n",
      "        [0.7069, 0.2931],\n",
      "        [0.7698, 0.2302],\n",
      "        [0.6816, 0.3184],\n",
      "        [0.7159, 0.2841],\n",
      "        [0.7698, 0.2302],\n",
      "        [0.6986, 0.3014]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1300\n",
      "tensor([[268266,     23,   1266,  ...,      0,      0,      0],\n",
      "        [196347,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 21789,  24496, 169696,  ...,     57,   2848,     39],\n",
      "        ...,\n",
      "        [  7113,   7347,   2380,  ...,  15040,   9423,     23],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   979,    326,   2852,  ...,    752,    971,     24]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7718, 0.2282],\n",
      "        [0.6659, 0.3341],\n",
      "        [0.7718, 0.2282],\n",
      "        [0.6934, 0.3066],\n",
      "        [0.7718, 0.2282],\n",
      "        [0.7057, 0.2943],\n",
      "        [0.7718, 0.2282],\n",
      "        [0.6886, 0.3114],\n",
      "        [0.7265, 0.2735],\n",
      "        [0.7718, 0.2282]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   959,   1222,     23,  ...,  11427,  11437,      6],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   828,      5,      6,  ...,   1226,    215,   1240],\n",
      "        [174987,     80,   9040,  ...,    737,   2698,   1159],\n",
      "        [ 63392,   1292,    272,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.6970, 0.3030],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.6447, 0.3553],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268],\n",
      "        [0.7732, 0.2268]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1500\n",
      "tensor([[ 80831, 220285,     56,  ...,     25,   7745,     33],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [    39,   1666,    195,  ...,      0,      0,      0],\n",
      "        [184045,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7751, 0.2249],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.7156, 0.2844],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.7751, 0.2249],\n",
      "        [0.6882, 0.3118],\n",
      "        [0.7751, 0.2249]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1600\n",
      "tensor([[11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        [   39,  1666,  6988,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5619, 31158,    80,  ...,     0,     0,     0],\n",
      "        [   39,  1292,   272,  ...,    23,   184,  4756],\n",
      "        [  306, 23305,   526,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.6927, 0.3073],\n",
      "        [0.6985, 0.3015],\n",
      "        [0.6738, 0.3262],\n",
      "        [0.7774, 0.2226],\n",
      "        [0.7774, 0.2226],\n",
      "        [0.7008, 0.2992],\n",
      "        [0.7154, 0.2846],\n",
      "        [0.7774, 0.2226],\n",
      "        [0.6996, 0.3004],\n",
      "        [0.7774, 0.2226]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1700\n",
      "tensor([[283716,     80,   9040,  ...,   1216,  19279,    182],\n",
      "        [ 35615,  36144,  86707,  ...,   2472,    317,   2855],\n",
      "        [223509,  31158,     80,  ...,   2912,    537,     30],\n",
      "        ...,\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   1580,  ...,    393,    737,     75],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7267, 0.2733],\n",
      "        [0.7157, 0.2843],\n",
      "        [0.7809, 0.2191],\n",
      "        [0.7809, 0.2191],\n",
      "        [0.7244, 0.2756],\n",
      "        [0.7288, 0.2712],\n",
      "        [0.7809, 0.2191],\n",
      "        [0.7071, 0.2929],\n",
      "        [0.7809, 0.2191],\n",
      "        [0.7198, 0.2802]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1800\n",
      "tensor([[  1643,     33,   4898,  ...,  43057,   1075,     75],\n",
      "        [228532,     80,   9040,  ...,     33,  40968,    151],\n",
      "        [  2916,   8069,    173,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [281751,     23,  99282,  ...,  20664,     17,   3400],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    380,     24,    134]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7824, 0.2176],\n",
      "        [0.7824, 0.2176],\n",
      "        [0.7824, 0.2176],\n",
      "        [0.7824, 0.2176],\n",
      "        [0.7168, 0.2832],\n",
      "        [0.7357, 0.2643],\n",
      "        [0.7140, 0.2860],\n",
      "        [0.7824, 0.2176],\n",
      "        [0.7101, 0.2899],\n",
      "        [0.7082, 0.2918]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1900\n",
      "tensor([[  2627,      6,    779,  ...,      0,      0,      0],\n",
      "        [271627,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,  28118,  84157,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    168,  17415,    354],\n",
      "        [    39,   3804,  45995,  ...,    291,    202,  54317]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7817, 0.2183],\n",
      "        [0.7091, 0.2909],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.7314, 0.2686],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.7821, 0.2179],\n",
      "        [0.6991, 0.3009]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2000\n",
      "tensor([[     5,      6,    202,  ...,      0,      0,      0],\n",
      "        [   691,  22860, 145703,  ...,     39,   5728,     87],\n",
      "        [224795,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    39,   8315,    340,  ...,      0,      0,      0],\n",
      "        [224807,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    48,    550,     12,  ...,   2213,  21504,   5125]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7849, 0.2151],\n",
      "        [0.7849, 0.2151],\n",
      "        [0.7849, 0.2151],\n",
      "        [0.7384, 0.2616],\n",
      "        [0.7395, 0.2605],\n",
      "        [0.7849, 0.2151],\n",
      "        [0.7849, 0.2151],\n",
      "        [0.7086, 0.2914],\n",
      "        [0.7369, 0.2631],\n",
      "        [0.7849, 0.2151]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2100\n",
      "tensor([[  1920,  35060,   3146,  ...,      0,      0,      0],\n",
      "        [155017,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6931,    184,    134,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229682,     23, 146543,  ...,     57,   2004,  78263],\n",
      "        [  2336,     33,    408,  ...,     13,   1284,   4579],\n",
      "        [  1344,   4118,    215,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7862, 0.2138],\n",
      "        [0.7862, 0.2138],\n",
      "        [0.7862, 0.2138],\n",
      "        [0.7862, 0.2138],\n",
      "        [0.7367, 0.2633],\n",
      "        [0.7862, 0.2138],\n",
      "        [0.7292, 0.2708],\n",
      "        [0.7434, 0.2566],\n",
      "        [0.7862, 0.2138],\n",
      "        [0.7862, 0.2138]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [274054,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6450,     19,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  3386,   1140,     25,  ...,    855,    539, 101895],\n",
      "        [253492,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 26069,     45,   4465,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.7863, 0.2137]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2300\n",
      "tensor([[  6901,    682,    168,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [151345,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        [    39,   4578,   3264,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7366, 0.2634],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.7870, 0.2130]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5753,    673,   1285,  ...,      0,      0,      0],\n",
      "        [226523,     80,   9040,  ...,     23,    299,     26],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7242, 0.2758],\n",
      "        [0.7884, 0.2116],\n",
      "        [0.7299, 0.2701],\n",
      "        [0.7884, 0.2116],\n",
      "        [0.6973, 0.3027],\n",
      "        [0.7248, 0.2752],\n",
      "        [0.7369, 0.2631],\n",
      "        [0.7884, 0.2116],\n",
      "        [0.7879, 0.2121],\n",
      "        [0.7147, 0.2853]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2500\n",
      "tensor([[ 11456,     80,   9040,  ...,     33,  24230,   1754],\n",
      "        [   318,     87,    278,  ...,      0,      0,      0],\n",
      "        [    39,  97080,     20,  ...,    497,    191,    195],\n",
      "        ...,\n",
      "        [156986,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   4053,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    434,   5200,   7419]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7486, 0.2514],\n",
      "        [0.7911, 0.2089],\n",
      "        [0.7911, 0.2089],\n",
      "        [0.7911, 0.2089],\n",
      "        [0.7261, 0.2739],\n",
      "        [0.7911, 0.2089],\n",
      "        [0.7216, 0.2784],\n",
      "        [0.7911, 0.2089],\n",
      "        [0.6922, 0.3078],\n",
      "        [0.7911, 0.2089]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2600\n",
      "tensor([[ 82374,  71026,     17,  ...,   1337,     13,   4418],\n",
      "        [  1004,    153,     19,  ...,      0,      0,      0],\n",
      "        [ 14962,     33,    374,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1176,     63,   2413,  ...,      0,      0,      0],\n",
      "        [ 27819,  12766,     23,  ...,   1337,     33,   2052],\n",
      "        [    39, 194425,   1155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7967, 0.2033],\n",
      "        [0.7211, 0.2789],\n",
      "        [0.7967, 0.2033],\n",
      "        [0.7389, 0.2611],\n",
      "        [0.7222, 0.2778],\n",
      "        [0.7967, 0.2033],\n",
      "        [0.6955, 0.3045],\n",
      "        [0.7179, 0.2821],\n",
      "        [0.7967, 0.2033],\n",
      "        [0.7437, 0.2563]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2700\n",
      "tensor([[184034,     23,   9845,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,   1497, 165486, 207960],\n",
      "        [228532,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [239851,     80,   9040,  ...,   1281,  12563,     26],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,    123,  39608,  ...,    271,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7250, 0.2750],\n",
      "        [0.7964, 0.2036],\n",
      "        [0.7964, 0.2036],\n",
      "        [0.7537, 0.2463],\n",
      "        [0.7322, 0.2678],\n",
      "        [0.7410, 0.2590],\n",
      "        [0.7964, 0.2036],\n",
      "        [0.7301, 0.2699],\n",
      "        [0.7463, 0.2537],\n",
      "        [0.7163, 0.2837]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2800\n",
      "tensor([[238655,     80,   9040,  ...,   5045,     23,     33],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [     5,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   959,    195,   4418,  ...,    321,      9,  10966],\n",
      "        [ 11456,     80,   9040,  ..., 221684,    415,    380],\n",
      "        [    39,  11142,  56734,  ...,     33, 154417,   2366]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7975, 0.2025],\n",
      "        [0.7108, 0.2892]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2900\n",
      "tensor([[270542,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  2101,     26,   1213,  ...,    460,     57,  35040],\n",
      "        ...,\n",
      "        [237044,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,   3590,  24018,    317]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7453, 0.2547],\n",
      "        [0.7988, 0.2012],\n",
      "        [0.7988, 0.2012],\n",
      "        [0.7441, 0.2559],\n",
      "        [0.7874, 0.2126],\n",
      "        [0.7988, 0.2012],\n",
      "        [0.7988, 0.2012],\n",
      "        [0.7988, 0.2012],\n",
      "        [0.7564, 0.2436],\n",
      "        [0.7988, 0.2012]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3000\n",
      "tensor([[  348,   195, 49873,  ...,  2285,   184,  7332],\n",
      "        [   80,  9040,    83,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   39, 48223,   215,  ...,     0,     0,     0],\n",
      "        [ 4308,    87,   919,  ...,    23,   136,   550],\n",
      "        [ 2108,    87,    81,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7231, 0.2769],\n",
      "        [0.8008, 0.1992],\n",
      "        [0.7303, 0.2697],\n",
      "        [0.7959, 0.2041],\n",
      "        [0.7480, 0.2520],\n",
      "        [0.7245, 0.2755],\n",
      "        [0.7306, 0.2694],\n",
      "        [0.8008, 0.1992],\n",
      "        [0.7516, 0.2484],\n",
      "        [0.7477, 0.2523]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3100\n",
      "tensor([[ 14795,     87,     33,  ...,    190,   1028,     24],\n",
      "        [    39,      5,      6,  ...,      0,      0,      0],\n",
      "        [156247,     80,   9040,  ...,    286,     33,    820],\n",
      "        ...,\n",
      "        [  2424,  13494,  15193,  ...,      0,      0,      0],\n",
      "        [    48,    550,    102,  ...,   2443,     33, 179310],\n",
      "        [156986,     80,   9040,  ...,    182,    745,    180]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7107, 0.2893],\n",
      "        [0.7261, 0.2739],\n",
      "        [0.7581, 0.2419],\n",
      "        [0.8019, 0.1981],\n",
      "        [0.8019, 0.1981],\n",
      "        [0.8019, 0.1981],\n",
      "        [0.7314, 0.2686],\n",
      "        [0.7404, 0.2596],\n",
      "        [0.8019, 0.1981],\n",
      "        [0.7404, 0.2596]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3200\n",
      "tensor([[   55,   189,     9,  ...,    55,   321,     9],\n",
      "        [  828,   587,    25,  ..., 39620,  1180,  1642],\n",
      "        [11456,    80,  9040,  ...,   494,    26,    33],\n",
      "        ...,\n",
      "        [20278, 42710,   153,  ...,  3351,    57,    33],\n",
      "        [ 3155,   154,    57,  ...,     0,     0,     0],\n",
      "        [12353,    13, 22134,  ...,    26, 40852,   352]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8046, 0.1954],\n",
      "        [0.7514, 0.2486],\n",
      "        [0.8046, 0.1954],\n",
      "        [0.7404, 0.2596],\n",
      "        [0.8046, 0.1954],\n",
      "        [0.8046, 0.1954],\n",
      "        [0.8046, 0.1954],\n",
      "        [0.7418, 0.2582],\n",
      "        [0.7486, 0.2514],\n",
      "        [0.7349, 0.2651]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3300\n",
      "tensor([[  2849,   1931,   2887,  ...,      0,      0,      0],\n",
      "        [  1011,   6781,    737,  ...,   4540,   2117,     75],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ..., 227336,  16898,     17],\n",
      "        [    55,    286,    228,  ...,  80382,   9969,   2332],\n",
      "        [  6953,     23,   2180,  ...,     23,  91635,  16592]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7210, 0.2790],\n",
      "        [0.8051, 0.1949],\n",
      "        [0.8051, 0.1949],\n",
      "        [0.7421, 0.2579],\n",
      "        [0.8050, 0.1950],\n",
      "        [0.7279, 0.2721],\n",
      "        [0.8051, 0.1949],\n",
      "        [0.8051, 0.1949],\n",
      "        [0.8051, 0.1949],\n",
      "        [0.7423, 0.2577]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3400\n",
      "tensor([[ 11456,     80,   9040,  ...,   4917, 232330,   3635],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [  2954,   3496,   6107,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   348,     87,   3095,  ...,      0,      0,      0],\n",
      "        [  2849,     17,   1140,  ...,      0,      0,      0],\n",
      "        [   348,   1102,   2971,  ...,     23,    518,   3540]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7496, 0.2504],\n",
      "        [0.7483, 0.2517],\n",
      "        [0.7386, 0.2614],\n",
      "        [0.7602, 0.2398],\n",
      "        [0.8081, 0.1919],\n",
      "        [0.8081, 0.1919],\n",
      "        [0.8081, 0.1919],\n",
      "        [0.8081, 0.1919],\n",
      "        [0.7107, 0.2893],\n",
      "        [0.7625, 0.2375]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3500\n",
      "tensor([[226803,     80,   9040,  ...,   1465,    134,  24526],\n",
      "        [148423,     23, 146543,  ...,    151,    199, 258704],\n",
      "        [287560,     80,   9040,  ...,    380,    153,    434],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [226143,     80,   9040,  ...,    168,   1785,   3191],\n",
      "        [  1140,   2619,   1324,  ...,   7892,   6766,   2324]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7309, 0.2691],\n",
      "        [0.7405, 0.2595],\n",
      "        [0.8095, 0.1905],\n",
      "        [0.8095, 0.1905],\n",
      "        [0.7814, 0.2186],\n",
      "        [0.8055, 0.1945],\n",
      "        [0.8082, 0.1918],\n",
      "        [0.8095, 0.1905],\n",
      "        [0.8095, 0.1905],\n",
      "        [0.8095, 0.1905]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3600\n",
      "tensor([[196264,     80,   9040,  ..., 110862,     40,   6262],\n",
      "        [ 18735,     77,  71050,  ...,    469,   3551,     26],\n",
      "        [   556,    557,  59223,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,  12454,   3771,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8111, 0.1889],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.7178, 0.2822],\n",
      "        [0.7169, 0.2831],\n",
      "        [0.7510, 0.2490],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.8111, 0.1889]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3700\n",
      "tensor([[    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [186960,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    66,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1746,   1222,     23,  ...,      0,      0,      0],\n",
      "        [   959,      7,   3064,  ...,      0,      0,      0],\n",
      "        [267953,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7239, 0.2761],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.8113, 0.1887],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.7834, 0.2166],\n",
      "        [0.7475, 0.2525],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.8116, 0.1884]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3800\n",
      "tensor([[187396,     80,   9040,  ...,   5241,    195,  90777],\n",
      "        [   565,    184,  59549,  ...,      0,      0,      0],\n",
      "        [ 57654,  14867,     23,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  7116,  11820,    195,  ...,    708,     33,   8198],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   348,     87,    449,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8124, 0.1876],\n",
      "        [0.8124, 0.1876],\n",
      "        [0.7180, 0.2820],\n",
      "        [0.7610, 0.2390],\n",
      "        [0.7542, 0.2458],\n",
      "        [0.8124, 0.1876],\n",
      "        [0.7431, 0.2569],\n",
      "        [0.7419, 0.2581],\n",
      "        [0.7542, 0.2458],\n",
      "        [0.7471, 0.2529]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3900\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 43634,   8524, 147656,  ...,      0,      0,      0],\n",
      "        [    80, 222369,     13,  ...,   6618,  27482,    198],\n",
      "        ...,\n",
      "        [  4813,      9,   3030,  ...,   8945,    971,  13995],\n",
      "        [  1043,   1006,     94,  ...,   3773,    268,     33],\n",
      "        [ 16500,     17,   9610,  ...,    550,     94,   1287]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "0.523624999999998 train_accuracy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) yval\n",
      "0 lineno1\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0]) y_val_updated\n",
      "0 lineno2\n",
      "0.5186094069529649 valid_accuracy\n",
      "489 len(val_losses)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7852, 0.2148],\n",
      "        [0.7135, 0.2865],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.7216, 0.2784],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.8130, 0.1870],\n",
      "        [0.7446, 0.2554]], grad_fn=<SoftmaxBackward>) yhat\n",
      "0\n",
      "tensor([[  1344,    374,    375,  ...,      0,      0,      0],\n",
      "        [  1920,    311,    219,  ...,  18521,     20, 191717],\n",
      "        [ 47166, 160856,   5047,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,   2405,   9431,  ...,      0,      0,      0],\n",
      "        [232939,     80,   9040,  ...,   1609,    237,   3529]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8160, 0.1840],\n",
      "        [0.8161, 0.1839],\n",
      "        [0.8161, 0.1839],\n",
      "        [0.7695, 0.2305],\n",
      "        [0.7317, 0.2683],\n",
      "        [0.8161, 0.1839],\n",
      "        [0.7775, 0.2225],\n",
      "        [0.7507, 0.2493],\n",
      "        [0.7496, 0.2504],\n",
      "        [0.8161, 0.1839]], grad_fn=<SoftmaxBackward>) yhat\n",
      "100\n",
      "tensor([[180734,     23,  20169,  ...,      0,      0,      0],\n",
      "        [257231,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  2108,     87,     33,  ...,     23, 188845,     26],\n",
      "        [  7000,     94,     23,  ...,   1894,   1110,   1111],\n",
      "        [  1043,     33,   1448,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8179, 0.1821],\n",
      "        [0.7630, 0.2370],\n",
      "        [0.7586, 0.2414],\n",
      "        [0.7607, 0.2393],\n",
      "        [0.7344, 0.2656],\n",
      "        [0.7718, 0.2282],\n",
      "        [0.7632, 0.2368],\n",
      "        [0.7746, 0.2254],\n",
      "        [0.8179, 0.1821],\n",
      "        [0.8179, 0.1821]], grad_fn=<SoftmaxBackward>) yhat\n",
      "200\n",
      "tensor([[ 6450,    19,    26,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,   268,    33,   860],\n",
      "        [ 3818,   662,    12,  ...,  1480,   168,    76],\n",
      "        ...,\n",
      "        [ 4813,     9,   891,  ...,  1324, 11294,  2394],\n",
      "        [    5,     6,   195,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8189, 0.1811],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.7673, 0.2327],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.6824, 0.3176],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.7783, 0.2217]], grad_fn=<SoftmaxBackward>) yhat\n",
      "300\n",
      "tensor([[281536,     80,   9040,  ...,      0,      0,      0],\n",
      "        [236697,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3019,    279,     75,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [245918,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [140748,     23, 140018,  ...,      7,    191,    102]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8202, 0.1798],\n",
      "        [0.7471, 0.2529],\n",
      "        [0.8202, 0.1798],\n",
      "        [0.8202, 0.1798],\n",
      "        [0.7307, 0.2693],\n",
      "        [0.8116, 0.1884],\n",
      "        [0.7880, 0.2120],\n",
      "        [0.8202, 0.1798],\n",
      "        [0.8202, 0.1798],\n",
      "        [0.8202, 0.1798]], grad_fn=<SoftmaxBackward>) yhat\n",
      "400\n",
      "tensor([[229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3155,   1324,    497,  ...,    123,  10686,    168],\n",
      "        [  2336,     19,    674,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [268251,     23,    391,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8219, 0.1781],\n",
      "        [0.8219, 0.1781],\n",
      "        [0.7520, 0.2480],\n",
      "        [0.8219, 0.1781],\n",
      "        [0.7218, 0.2782],\n",
      "        [0.7526, 0.2474],\n",
      "        [0.7778, 0.2222],\n",
      "        [0.8219, 0.1781],\n",
      "        [0.7487, 0.2513],\n",
      "        [0.7759, 0.2241]], grad_fn=<SoftmaxBackward>) yhat\n",
      "500\n",
      "tensor([[251749,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4991, 139735, 153298,  ...,   2382,  51678,    348],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  65865,     17,     33],\n",
      "        [  6953,     23,     33,  ...,     39,    724,     87]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7348, 0.2652],\n",
      "        [0.8235, 0.1765],\n",
      "        [0.7496, 0.2504],\n",
      "        [0.7896, 0.2104],\n",
      "        [0.8235, 0.1765],\n",
      "        [0.7711, 0.2289],\n",
      "        [0.7667, 0.2333],\n",
      "        [0.8235, 0.1765],\n",
      "        [0.7765, 0.2235],\n",
      "        [0.8235, 0.1765]], grad_fn=<SoftmaxBackward>) yhat\n",
      "600\n",
      "tensor([[226803,     80,   9040,  ...,   3675,  10010,     17],\n",
      "        [ 11477,  62205,     80,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    684,     20,   1672],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ..., 221684,     24,    380],\n",
      "        [  6879,    352,    155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7734, 0.2266],\n",
      "        [0.7761, 0.2239],\n",
      "        [0.7838, 0.2162],\n",
      "        [0.7531, 0.2469],\n",
      "        [0.8243, 0.1757],\n",
      "        [0.8244, 0.1756],\n",
      "        [0.7814, 0.2186],\n",
      "        [0.7894, 0.2106],\n",
      "        [0.8244, 0.1756],\n",
      "        [0.7862, 0.2138]], grad_fn=<SoftmaxBackward>) yhat\n",
      "700\n",
      "tensor([[239157,     80,   9040,  ...,   1324,  20646,     23],\n",
      "        [ 11456,     80,   9040,  ...,   3214,   3540,    873],\n",
      "        [229431,     80,   9040,  ...,    380,     23,     26],\n",
      "        ...,\n",
      "        [    39,    985,   1445,  ...,     26,     72,   1373],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   424,    870,     23,  ...,    119,   5865,     26]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7889, 0.2111],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.8258, 0.1742],\n",
      "        [0.7818, 0.2182],\n",
      "        [0.8258, 0.1742],\n",
      "        [0.7731, 0.2269],\n",
      "        [0.8258, 0.1742],\n",
      "        [0.8258, 0.1742],\n",
      "        [0.7834, 0.2166],\n",
      "        [0.8258, 0.1742]], grad_fn=<SoftmaxBackward>) yhat\n",
      "800\n",
      "tensor([[     5,      6,     87,  ...,    173,   1709,  15605],\n",
      "        [    39,    785,   3523,  ...,    180,   1465,   1466],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1643,     66,      6,  ...,      0,      0,      0],\n",
      "        [     6,    195,   1194,  ...,     17,    219,   1050],\n",
      "        [    80, 158104,   2711,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7274, 0.2726],\n",
      "        [0.8272, 0.1728],\n",
      "        [0.7756, 0.2244],\n",
      "        [0.7648, 0.2352],\n",
      "        [0.7688, 0.2312],\n",
      "        [0.8273, 0.1727],\n",
      "        [0.7690, 0.2310],\n",
      "        [0.8273, 0.1727],\n",
      "        [0.7859, 0.2141],\n",
      "        [0.7445, 0.2555]], grad_fn=<SoftmaxBackward>) yhat\n",
      "900\n",
      "tensor([[   959,     40,   2357,  ...,  53361,   2220,    463],\n",
      "        [269228,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4367,   5063,    195,  ...,  52431,     83,   4376],\n",
      "        ...,\n",
      "        [268289,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   591,   8657,     87,  ...,     23,    195,   4854],\n",
      "        [   424,   1746,     23,  ...,    557,  49541,   2213]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8301, 0.1699],\n",
      "        [0.7636, 0.2364],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.7829, 0.2171],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.7231, 0.2769],\n",
      "        [0.7322, 0.2678],\n",
      "        [0.8301, 0.1699]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1000\n",
      "tensor([[ 35615,  36144,  86707,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  22406,  10751,     17],\n",
      "        [  4041,   2834,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ...,  34418,     23,   9866],\n",
      "        [244117, 244117,     23,  ...,    991,     33,  10841],\n",
      "        [226803,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7462, 0.2538],\n",
      "        [0.8313, 0.1687],\n",
      "        [0.7765, 0.2235],\n",
      "        [0.7362, 0.2638],\n",
      "        [0.8313, 0.1687],\n",
      "        [0.7948, 0.2052],\n",
      "        [0.7657, 0.2343],\n",
      "        [0.7243, 0.2757],\n",
      "        [0.7238, 0.2762],\n",
      "        [0.8128, 0.1872]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1100\n",
      "tensor([[ 35615,  36144,  86707,  ...,    293, 207525, 207422],\n",
      "        [   158,      8,      9,  ...,      0,      0,      0],\n",
      "        [ 14238,     87,    945,  ...,   5901,     23,   2672],\n",
      "        ...,\n",
      "        [  3032,    546,     13,  ...,    557,   1505,  31885],\n",
      "        [  2336,     33,   8769,  ...,     23,     30,  92610],\n",
      "        [   959,     74,    286,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8324, 0.1676],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.7713, 0.2287],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.7871, 0.2129],\n",
      "        [0.7477, 0.2523],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.8324, 0.1676]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1011,   1375,    289,  ...,      0,      0,      0],\n",
      "        [ 39680,   7544,    195,  ...,    554,  55586,  27599],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 35615,  36144,  86707,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8335, 0.1665],\n",
      "        [0.8335, 0.1665],\n",
      "        [0.7350, 0.2650],\n",
      "        [0.7867, 0.2133],\n",
      "        [0.7812, 0.2188],\n",
      "        [0.8335, 0.1665],\n",
      "        [0.7415, 0.2585],\n",
      "        [0.7872, 0.2128],\n",
      "        [0.8335, 0.1665],\n",
      "        [0.7680, 0.2320]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1300\n",
      "tensor([[268266,     23,   1266,  ...,      0,      0,      0],\n",
      "        [196347,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 21789,  24496, 169696,  ...,     57,   2848,     39],\n",
      "        ...,\n",
      "        [  7113,   7347,   2380,  ...,  15040,   9423,     23],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   979,    326,   2852,  ...,    752,    971,     24]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8351, 0.1649],\n",
      "        [0.7345, 0.2655],\n",
      "        [0.8352, 0.1648],\n",
      "        [0.7576, 0.2424],\n",
      "        [0.8352, 0.1648],\n",
      "        [0.7796, 0.2204],\n",
      "        [0.8352, 0.1648],\n",
      "        [0.7574, 0.2426],\n",
      "        [0.7931, 0.2069],\n",
      "        [0.8352, 0.1648]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   959,   1222,     23,  ...,  11427,  11437,      6],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   828,      5,      6,  ...,   1226,    215,   1240],\n",
      "        [174987,     80,   9040,  ...,    737,   2698,   1159],\n",
      "        [ 63392,   1292,    272,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7650, 0.2350],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.7117, 0.2883],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.8363, 0.1637]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1500\n",
      "tensor([[ 80831, 220285,     56,  ...,     25,   7745,     33],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [    39,   1666,    195,  ...,      0,      0,      0],\n",
      "        [184045,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8378, 0.1622],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.7817, 0.2183],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.7588, 0.2412],\n",
      "        [0.8378, 0.1622]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1600\n",
      "tensor([[11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        [   39,  1666,  6988,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5619, 31158,    80,  ...,     0,     0,     0],\n",
      "        [   39,  1292,   272,  ...,    23,   184,  4756],\n",
      "        [  306, 23305,   526,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7561, 0.2439],\n",
      "        [0.7723, 0.2277],\n",
      "        [0.7386, 0.2614],\n",
      "        [0.8393, 0.1607],\n",
      "        [0.8393, 0.1607],\n",
      "        [0.7758, 0.2242],\n",
      "        [0.7868, 0.2132],\n",
      "        [0.8393, 0.1607],\n",
      "        [0.7687, 0.2313],\n",
      "        [0.8393, 0.1607]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1700\n",
      "tensor([[283716,     80,   9040,  ...,   1216,  19279,    182],\n",
      "        [ 35615,  36144,  86707,  ...,   2472,    317,   2855],\n",
      "        [223509,  31158,     80,  ...,   2912,    537,     30],\n",
      "        ...,\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   1580,  ...,    393,    737,     75],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7977, 0.2023],\n",
      "        [0.7806, 0.2194],\n",
      "        [0.8420, 0.1580],\n",
      "        [0.8420, 0.1580],\n",
      "        [0.7957, 0.2043],\n",
      "        [0.7969, 0.2031],\n",
      "        [0.8420, 0.1580],\n",
      "        [0.7700, 0.2300],\n",
      "        [0.8420, 0.1580],\n",
      "        [0.7840, 0.2160]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1800\n",
      "tensor([[  1643,     33,   4898,  ...,  43057,   1075,     75],\n",
      "        [228532,     80,   9040,  ...,     33,  40968,    151],\n",
      "        [  2916,   8069,    173,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [281751,     23,  99282,  ...,  20664,     17,   3400],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    380,     24,    134]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8429, 0.1571],\n",
      "        [0.8429, 0.1571],\n",
      "        [0.8429, 0.1571],\n",
      "        [0.8429, 0.1571],\n",
      "        [0.7850, 0.2150],\n",
      "        [0.8059, 0.1941],\n",
      "        [0.7926, 0.2074],\n",
      "        [0.8429, 0.1571],\n",
      "        [0.7737, 0.2263],\n",
      "        [0.7749, 0.2251]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1900\n",
      "tensor([[  2627,      6,    779,  ...,      0,      0,      0],\n",
      "        [271627,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,  28118,  84157,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    168,  17415,    354],\n",
      "        [    39,   3804,  45995,  ...,    291,    202,  54317]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8421, 0.1579],\n",
      "        [0.7785, 0.2215],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.8036, 0.1964],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.8428, 0.1572],\n",
      "        [0.7658, 0.2342]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2000\n",
      "tensor([[     5,      6,    202,  ...,      0,      0,      0],\n",
      "        [   691,  22860, 145703,  ...,     39,   5728,     87],\n",
      "        [224795,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    39,   8315,    340,  ...,      0,      0,      0],\n",
      "        [224807,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    48,    550,     12,  ...,   2213,  21504,   5125]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8449, 0.1551],\n",
      "        [0.8449, 0.1551],\n",
      "        [0.8449, 0.1551],\n",
      "        [0.8103, 0.1897],\n",
      "        [0.8109, 0.1891],\n",
      "        [0.8449, 0.1551],\n",
      "        [0.8449, 0.1551],\n",
      "        [0.7694, 0.2306],\n",
      "        [0.8047, 0.1953],\n",
      "        [0.8449, 0.1551]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2100\n",
      "tensor([[  1920,  35060,   3146,  ...,      0,      0,      0],\n",
      "        [155017,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6931,    184,    134,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229682,     23, 146543,  ...,     57,   2004,  78263],\n",
      "        [  2336,     33,    408,  ...,     13,   1284,   4579],\n",
      "        [  1344,   4118,    215,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8460, 0.1540],\n",
      "        [0.8460, 0.1540],\n",
      "        [0.8460, 0.1540],\n",
      "        [0.8460, 0.1540],\n",
      "        [0.7986, 0.2014],\n",
      "        [0.8460, 0.1540],\n",
      "        [0.7994, 0.2006],\n",
      "        [0.8145, 0.1855],\n",
      "        [0.8460, 0.1540],\n",
      "        [0.8460, 0.1540]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [274054,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6450,     19,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  3386,   1140,     25,  ...,    855,    539, 101895],\n",
      "        [253492,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 26069,     45,   4465,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539],\n",
      "        [0.8461, 0.1539]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2300\n",
      "tensor([[  6901,    682,    168,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [151345,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        [    39,   4578,   3264,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8029, 0.1971],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532],\n",
      "        [0.8468, 0.1532]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5753,    673,   1285,  ...,      0,      0,      0],\n",
      "        [226523,     80,   9040,  ...,     23,    299,     26],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7873, 0.2127],\n",
      "        [0.8479, 0.1521],\n",
      "        [0.8032, 0.1968],\n",
      "        [0.8479, 0.1521],\n",
      "        [0.7557, 0.2443],\n",
      "        [0.7864, 0.2136],\n",
      "        [0.8036, 0.1964],\n",
      "        [0.8479, 0.1521],\n",
      "        [0.8472, 0.1528],\n",
      "        [0.7700, 0.2300]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2500\n",
      "tensor([[ 11456,     80,   9040,  ...,     33,  24230,   1754],\n",
      "        [   318,     87,    278,  ...,      0,      0,      0],\n",
      "        [    39,  97080,     20,  ...,    497,    191,    195],\n",
      "        ...,\n",
      "        [156986,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   4053,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    434,   5200,   7419]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8159, 0.1841],\n",
      "        [0.8498, 0.1502],\n",
      "        [0.8498, 0.1502],\n",
      "        [0.8498, 0.1502],\n",
      "        [0.7893, 0.2107],\n",
      "        [0.8498, 0.1502],\n",
      "        [0.7892, 0.2108],\n",
      "        [0.8498, 0.1502],\n",
      "        [0.7583, 0.2417],\n",
      "        [0.8498, 0.1502]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2600\n",
      "tensor([[ 82374,  71026,     17,  ...,   1337,     13,   4418],\n",
      "        [  1004,    153,     19,  ...,      0,      0,      0],\n",
      "        [ 14962,     33,    374,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1176,     63,   2413,  ...,      0,      0,      0],\n",
      "        [ 27819,  12766,     23,  ...,   1337,     33,   2052],\n",
      "        [    39, 194425,   1155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8536, 0.1464],\n",
      "        [0.7760, 0.2240],\n",
      "        [0.8536, 0.1464],\n",
      "        [0.8094, 0.1906],\n",
      "        [0.7949, 0.2051],\n",
      "        [0.8536, 0.1464],\n",
      "        [0.7533, 0.2467],\n",
      "        [0.7779, 0.2221],\n",
      "        [0.8536, 0.1464],\n",
      "        [0.8054, 0.1946]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2700\n",
      "tensor([[184034,     23,   9845,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,   1497, 165486, 207960],\n",
      "        [228532,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [239851,     80,   9040,  ...,   1281,  12563,     26],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,    123,  39608,  ...,    271,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7885, 0.2115],\n",
      "        [0.8534, 0.1466],\n",
      "        [0.8534, 0.1466],\n",
      "        [0.8126, 0.1874],\n",
      "        [0.7952, 0.2048],\n",
      "        [0.8056, 0.1944],\n",
      "        [0.8534, 0.1466],\n",
      "        [0.8012, 0.1988],\n",
      "        [0.8081, 0.1919],\n",
      "        [0.7781, 0.2219]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2800\n",
      "tensor([[238655,     80,   9040,  ...,   5045,     23,     33],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [     5,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   959,    195,   4418,  ...,    321,      9,  10966],\n",
      "        [ 11456,     80,   9040,  ..., 221684,    415,    380],\n",
      "        [    39,  11142,  56734,  ...,     33, 154417,   2366]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8543, 0.1457],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.7909, 0.2091],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.8542, 0.1458],\n",
      "        [0.8543, 0.1457],\n",
      "        [0.7702, 0.2298]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2900\n",
      "tensor([[270542,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  2101,     26,   1213,  ...,    460,     57,  35040],\n",
      "        ...,\n",
      "        [237044,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,   3590,  24018,    317]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8106, 0.1894],\n",
      "        [0.8552, 0.1448],\n",
      "        [0.8552, 0.1448],\n",
      "        [0.8131, 0.1869],\n",
      "        [0.8450, 0.1550],\n",
      "        [0.8552, 0.1448],\n",
      "        [0.8552, 0.1448],\n",
      "        [0.8552, 0.1448],\n",
      "        [0.8247, 0.1753],\n",
      "        [0.8552, 0.1448]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3000\n",
      "tensor([[  348,   195, 49873,  ...,  2285,   184,  7332],\n",
      "        [   80,  9040,    83,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   39, 48223,   215,  ...,     0,     0,     0],\n",
      "        [ 4308,    87,   919,  ...,    23,   136,   550],\n",
      "        [ 2108,    87,    81,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7911, 0.2089],\n",
      "        [0.8566, 0.1434],\n",
      "        [0.7977, 0.2023],\n",
      "        [0.8520, 0.1480],\n",
      "        [0.8128, 0.1872],\n",
      "        [0.7934, 0.2066],\n",
      "        [0.7985, 0.2015],\n",
      "        [0.8566, 0.1434],\n",
      "        [0.8192, 0.1808],\n",
      "        [0.8058, 0.1942]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3100\n",
      "tensor([[ 14795,     87,     33,  ...,    190,   1028,     24],\n",
      "        [    39,      5,      6,  ...,      0,      0,      0],\n",
      "        [156247,     80,   9040,  ...,    286,     33,    820],\n",
      "        ...,\n",
      "        [  2424,  13494,  15193,  ...,      0,      0,      0],\n",
      "        [    48,    550,    102,  ...,   2443,     33, 179310],\n",
      "        [156986,     80,   9040,  ...,    182,    745,    180]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7818, 0.2182],\n",
      "        [0.7891, 0.2109],\n",
      "        [0.8232, 0.1768],\n",
      "        [0.8573, 0.1427],\n",
      "        [0.8573, 0.1427],\n",
      "        [0.8573, 0.1427],\n",
      "        [0.7881, 0.2119],\n",
      "        [0.8054, 0.1946],\n",
      "        [0.8573, 0.1427],\n",
      "        [0.7994, 0.2006]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3200\n",
      "tensor([[   55,   189,     9,  ...,    55,   321,     9],\n",
      "        [  828,   587,    25,  ..., 39620,  1180,  1642],\n",
      "        [11456,    80,  9040,  ...,   494,    26,    33],\n",
      "        ...,\n",
      "        [20278, 42710,   153,  ...,  3351,    57,    33],\n",
      "        [ 3155,   154,    57,  ...,     0,     0,     0],\n",
      "        [12353,    13, 22134,  ...,    26, 40852,   352]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8593, 0.1407],\n",
      "        [0.8179, 0.1821],\n",
      "        [0.8593, 0.1407],\n",
      "        [0.8063, 0.1937],\n",
      "        [0.8593, 0.1407],\n",
      "        [0.8593, 0.1407],\n",
      "        [0.8593, 0.1407],\n",
      "        [0.8007, 0.1993],\n",
      "        [0.8076, 0.1924],\n",
      "        [0.8026, 0.1974]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3300\n",
      "tensor([[  2849,   1931,   2887,  ...,      0,      0,      0],\n",
      "        [  1011,   6781,    737,  ...,   4540,   2117,     75],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ..., 227336,  16898,     17],\n",
      "        [    55,    286,    228,  ...,  80382,   9969,   2332],\n",
      "        [  6953,     23,   2180,  ...,     23,  91635,  16592]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7732, 0.2268],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8034, 0.1966],\n",
      "        [0.8596, 0.1404],\n",
      "        [0.7874, 0.2126],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8047, 0.1953]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3400\n",
      "tensor([[ 11456,     80,   9040,  ...,   4917, 232330,   3635],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [  2954,   3496,   6107,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   348,     87,   3095,  ...,      0,      0,      0],\n",
      "        [  2849,     17,   1140,  ...,      0,      0,      0],\n",
      "        [   348,   1102,   2971,  ...,     23,    518,   3540]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8098, 0.1902],\n",
      "        [0.8092, 0.1908],\n",
      "        [0.7934, 0.2066],\n",
      "        [0.8178, 0.1822],\n",
      "        [0.8620, 0.1380],\n",
      "        [0.8620, 0.1380],\n",
      "        [0.8620, 0.1380],\n",
      "        [0.8620, 0.1380],\n",
      "        [0.7574, 0.2426],\n",
      "        [0.8284, 0.1716]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3500\n",
      "tensor([[226803,     80,   9040,  ...,   1465,    134,  24526],\n",
      "        [148423,     23, 146543,  ...,    151,    199, 258704],\n",
      "        [287560,     80,   9040,  ...,    380,    153,    434],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [226143,     80,   9040,  ...,    168,   1785,   3191],\n",
      "        [  1140,   2619,   1324,  ...,   7892,   6766,   2324]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7865, 0.2135],\n",
      "        [0.8137, 0.1863],\n",
      "        [0.8629, 0.1371],\n",
      "        [0.8629, 0.1371],\n",
      "        [0.8403, 0.1597],\n",
      "        [0.8585, 0.1415],\n",
      "        [0.8615, 0.1385],\n",
      "        [0.8629, 0.1371],\n",
      "        [0.8629, 0.1371],\n",
      "        [0.8629, 0.1371]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3600\n",
      "tensor([[196264,     80,   9040,  ..., 110862,     40,   6262],\n",
      "        [ 18735,     77,  71050,  ...,    469,   3551,     26],\n",
      "        [   556,    557,  59223,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,  12454,   3771,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8640, 0.1360],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.7827, 0.2173],\n",
      "        [0.7661, 0.2339],\n",
      "        [0.8100, 0.1900],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.8640, 0.1360]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3700\n",
      "tensor([[    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [186960,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    66,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1746,   1222,     23,  ...,      0,      0,      0],\n",
      "        [   959,      7,   3064,  ...,      0,      0,      0],\n",
      "        [267953,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7812, 0.2188],\n",
      "        [0.8644, 0.1356],\n",
      "        [0.8644, 0.1356],\n",
      "        [0.8644, 0.1356],\n",
      "        [0.8640, 0.1360],\n",
      "        [0.8644, 0.1356],\n",
      "        [0.8427, 0.1573],\n",
      "        [0.8109, 0.1891],\n",
      "        [0.8644, 0.1356],\n",
      "        [0.8644, 0.1356]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3800\n",
      "tensor([[187396,     80,   9040,  ...,   5241,    195,  90777],\n",
      "        [   565,    184,  59549,  ...,      0,      0,      0],\n",
      "        [ 57654,  14867,     23,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  7116,  11820,    195,  ...,    708,     33,   8198],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   348,     87,    449,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8650, 0.1350],\n",
      "        [0.8650, 0.1350],\n",
      "        [0.7671, 0.2329],\n",
      "        [0.8192, 0.1808],\n",
      "        [0.8072, 0.1928],\n",
      "        [0.8650, 0.1350],\n",
      "        [0.8099, 0.1901],\n",
      "        [0.8056, 0.1944],\n",
      "        [0.8109, 0.1891],\n",
      "        [0.8141, 0.1859]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3900\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 43634,   8524, 147656,  ...,      0,      0,      0],\n",
      "        [    80, 222369,     13,  ...,   6618,  27482,    198],\n",
      "        ...,\n",
      "        [  4813,      9,   3030,  ...,   8945,    971,  13995],\n",
      "        [  1043,   1006,     94,  ...,   3773,    268,     33],\n",
      "        [ 16500,     17,   9610,  ...,    550,     94,   1287]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "0.5236249999999968 train_accuracy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) yval\n",
      "0 lineno1\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0]) y_val_updated\n",
      "0 lineno2\n",
      "0.518609406952965 valid_accuracy\n",
      "978 len(val_losses)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8388, 0.1612],\n",
      "        [0.7620, 0.2380],\n",
      "        [0.8662, 0.1338],\n",
      "        [0.8662, 0.1338],\n",
      "        [0.8662, 0.1338],\n",
      "        [0.8662, 0.1338],\n",
      "        [0.7889, 0.2111],\n",
      "        [0.8662, 0.1338],\n",
      "        [0.8649, 0.1351],\n",
      "        [0.8014, 0.1986]], grad_fn=<SoftmaxBackward>) yhat\n",
      "0\n",
      "tensor([[  1344,    374,    375,  ...,      0,      0,      0],\n",
      "        [  1920,    311,    219,  ...,  18521,     20, 191717],\n",
      "        [ 47166, 160856,   5047,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,   2405,   9431,  ...,      0,      0,      0],\n",
      "        [232939,     80,   9040,  ...,   1609,    237,   3529]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8674, 0.1326],\n",
      "        [0.8675, 0.1325],\n",
      "        [0.8675, 0.1325],\n",
      "        [0.8315, 0.1685],\n",
      "        [0.7936, 0.2064],\n",
      "        [0.8675, 0.1325],\n",
      "        [0.8358, 0.1642],\n",
      "        [0.8125, 0.1875],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.8675, 0.1325]], grad_fn=<SoftmaxBackward>) yhat\n",
      "100\n",
      "tensor([[180734,     23,  20169,  ...,      0,      0,      0],\n",
      "        [257231,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  2108,     87,     33,  ...,     23, 188845,     26],\n",
      "        [  7000,     94,     23,  ...,   1894,   1110,   1111],\n",
      "        [  1043,     33,   1448,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8686, 0.1314],\n",
      "        [0.8128, 0.1872],\n",
      "        [0.8161, 0.1839],\n",
      "        [0.8142, 0.1858],\n",
      "        [0.7858, 0.2142],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.8139, 0.1861],\n",
      "        [0.8281, 0.1719],\n",
      "        [0.8686, 0.1314],\n",
      "        [0.8686, 0.1314]], grad_fn=<SoftmaxBackward>) yhat\n",
      "200\n",
      "tensor([[ 6450,    19,    26,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,   268,    33,   860],\n",
      "        [ 3818,   662,    12,  ...,  1480,   168,    76],\n",
      "        ...,\n",
      "        [ 4813,     9,   891,  ...,  1324, 11294,  2394],\n",
      "        [    5,     6,   195,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8692, 0.1308],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.8264, 0.1736],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.7360, 0.2640],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.8428, 0.1572]], grad_fn=<SoftmaxBackward>) yhat\n",
      "300\n",
      "tensor([[281536,     80,   9040,  ...,      0,      0,      0],\n",
      "        [236697,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3019,    279,     75,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [245918,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [140748,     23, 140018,  ...,      7,    191,    102]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8700, 0.1300],\n",
      "        [0.7993, 0.2007],\n",
      "        [0.8700, 0.1300],\n",
      "        [0.8700, 0.1300],\n",
      "        [0.7843, 0.2157],\n",
      "        [0.8610, 0.1390],\n",
      "        [0.8413, 0.1587],\n",
      "        [0.8700, 0.1300],\n",
      "        [0.8700, 0.1300],\n",
      "        [0.8700, 0.1300]], grad_fn=<SoftmaxBackward>) yhat\n",
      "400\n",
      "tensor([[229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  3155,   1324,    497,  ...,    123,  10686,    168],\n",
      "        [  2336,     19,    674,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [268251,     23,    391,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8712, 0.1288],\n",
      "        [0.8712, 0.1288],\n",
      "        [0.8050, 0.1950],\n",
      "        [0.8712, 0.1288],\n",
      "        [0.7759, 0.2241],\n",
      "        [0.8075, 0.1925],\n",
      "        [0.8373, 0.1627],\n",
      "        [0.8712, 0.1288],\n",
      "        [0.8028, 0.1972],\n",
      "        [0.8326, 0.1674]], grad_fn=<SoftmaxBackward>) yhat\n",
      "500\n",
      "tensor([[251749,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4991, 139735, 153298,  ...,   2382,  51678,    348],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  65865,     17,     33],\n",
      "        [  6953,     23,     33,  ...,     39,    724,     87]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7790, 0.2210],\n",
      "        [0.8722, 0.1278],\n",
      "        [0.8015, 0.1985],\n",
      "        [0.8446, 0.1554],\n",
      "        [0.8722, 0.1278],\n",
      "        [0.8256, 0.1744],\n",
      "        [0.8190, 0.1810],\n",
      "        [0.8722, 0.1278],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.8722, 0.1278]], grad_fn=<SoftmaxBackward>) yhat\n",
      "600\n",
      "tensor([[226803,     80,   9040,  ...,   3675,  10010,     17],\n",
      "        [ 11477,  62205,     80,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    684,     20,   1672],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ..., 221684,     24,    380],\n",
      "        [  6879,    352,    155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8215, 0.1785],\n",
      "        [0.8241, 0.1759],\n",
      "        [0.8374, 0.1626],\n",
      "        [0.8044, 0.1956],\n",
      "        [0.8727, 0.1273],\n",
      "        [0.8728, 0.1272],\n",
      "        [0.8335, 0.1665],\n",
      "        [0.8447, 0.1553],\n",
      "        [0.8728, 0.1272],\n",
      "        [0.8404, 0.1596]], grad_fn=<SoftmaxBackward>) yhat\n",
      "700\n",
      "tensor([[239157,     80,   9040,  ...,   1324,  20646,     23],\n",
      "        [ 11456,     80,   9040,  ...,   3214,   3540,    873],\n",
      "        [229431,     80,   9040,  ...,    380,     23,     26],\n",
      "        ...,\n",
      "        [    39,    985,   1445,  ...,     26,     72,   1373],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   424,    870,     23,  ...,    119,   5865,     26]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8409, 0.1591],\n",
      "        [0.8426, 0.1574],\n",
      "        [0.8737, 0.1263],\n",
      "        [0.8309, 0.1691],\n",
      "        [0.8737, 0.1263],\n",
      "        [0.8270, 0.1730],\n",
      "        [0.8737, 0.1263],\n",
      "        [0.8737, 0.1263],\n",
      "        [0.8401, 0.1599],\n",
      "        [0.8737, 0.1263]], grad_fn=<SoftmaxBackward>) yhat\n",
      "800\n",
      "tensor([[     5,      6,     87,  ...,    173,   1709,  15605],\n",
      "        [    39,    785,   3523,  ...,    180,   1465,   1466],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1643,     66,      6,  ...,      0,      0,      0],\n",
      "        [     6,    195,   1194,  ...,     17,    219,   1050],\n",
      "        [    80, 158104,   2711,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7753, 0.2247],\n",
      "        [0.8746, 0.1254],\n",
      "        [0.8395, 0.1605],\n",
      "        [0.8105, 0.1894],\n",
      "        [0.8172, 0.1828],\n",
      "        [0.8746, 0.1254],\n",
      "        [0.8213, 0.1787],\n",
      "        [0.8746, 0.1254],\n",
      "        [0.8391, 0.1609],\n",
      "        [0.8070, 0.1930]], grad_fn=<SoftmaxBackward>) yhat\n",
      "900\n",
      "tensor([[   959,     40,   2357,  ...,  53361,   2220,    463],\n",
      "        [269228,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  4367,   5063,    195,  ...,  52431,     83,   4376],\n",
      "        ...,\n",
      "        [268289,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   591,   8657,     87,  ...,     23,    195,   4854],\n",
      "        [   424,   1746,     23,  ...,    557,  49541,   2213]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8765, 0.1235],\n",
      "        [0.8071, 0.1929],\n",
      "        [0.8765, 0.1235],\n",
      "        [0.8765, 0.1235],\n",
      "        [0.8765, 0.1235],\n",
      "        [0.8392, 0.1608],\n",
      "        [0.8765, 0.1235],\n",
      "        [0.7689, 0.2311],\n",
      "        [0.7792, 0.2208],\n",
      "        [0.8765, 0.1235]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1000\n",
      "tensor([[ 35615,  36144,  86707,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,  22406,  10751,     17],\n",
      "        [  4041,   2834,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ...,  34418,     23,   9866],\n",
      "        [244117, 244117,     23,  ...,    991,     33,  10841],\n",
      "        [226803,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7969, 0.2031],\n",
      "        [0.8774, 0.1226],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.7832, 0.2168],\n",
      "        [0.8774, 0.1226],\n",
      "        [0.8456, 0.1544],\n",
      "        [0.8173, 0.1827],\n",
      "        [0.7733, 0.2267],\n",
      "        [0.7707, 0.2293],\n",
      "        [0.8619, 0.1381]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1100\n",
      "tensor([[ 35615,  36144,  86707,  ...,    293, 207525, 207422],\n",
      "        [   158,      8,      9,  ...,      0,      0,      0],\n",
      "        [ 14238,     87,    945,  ...,   5901,     23,   2672],\n",
      "        ...,\n",
      "        [  3032,    546,     13,  ...,    557,   1505,  31885],\n",
      "        [  2336,     33,   8769,  ...,     23,     30,  92610],\n",
      "        [   959,     74,    286,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8782, 0.1218],\n",
      "        [0.8782, 0.1218],\n",
      "        [0.8230, 0.1770],\n",
      "        [0.8782, 0.1218],\n",
      "        [0.8379, 0.1621],\n",
      "        [0.8001, 0.1999],\n",
      "        [0.8782, 0.1218],\n",
      "        [0.8782, 0.1218],\n",
      "        [0.8782, 0.1218],\n",
      "        [0.8782, 0.1218]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1011,   1375,    289,  ...,      0,      0,      0],\n",
      "        [ 39680,   7544,    195,  ...,    554,  55586,  27599],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 35615,  36144,  86707,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8790, 0.1210],\n",
      "        [0.8790, 0.1210],\n",
      "        [0.7725, 0.2275],\n",
      "        [0.8349, 0.1651],\n",
      "        [0.8322, 0.1678],\n",
      "        [0.8790, 0.1210],\n",
      "        [0.7737, 0.2263],\n",
      "        [0.8346, 0.1654],\n",
      "        [0.8790, 0.1210],\n",
      "        [0.8116, 0.1884]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1300\n",
      "tensor([[268266,     23,   1266,  ...,      0,      0,      0],\n",
      "        [196347,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 21789,  24496, 169696,  ...,     57,   2848,     39],\n",
      "        ...,\n",
      "        [  7113,   7347,   2380,  ...,  15040,   9423,     23],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   979,    326,   2852,  ...,    752,    971,     24]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8801, 0.1199],\n",
      "        [0.7900, 0.2100],\n",
      "        [0.8802, 0.1198],\n",
      "        [0.7983, 0.2017],\n",
      "        [0.8802, 0.1198],\n",
      "        [0.8327, 0.1673],\n",
      "        [0.8802, 0.1198],\n",
      "        [0.8040, 0.1960],\n",
      "        [0.8361, 0.1639],\n",
      "        [0.8802, 0.1198]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   959,   1222,     23,  ...,  11427,  11437,      6],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   828,      5,      6,  ...,   1226,    215,   1240],\n",
      "        [174987,     80,   9040,  ...,    737,   2698,   1159],\n",
      "        [ 63392,   1292,    272,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8117, 0.1883],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.7750, 0.2250],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.8809, 0.1191]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1500\n",
      "tensor([[ 80831, 220285,     56,  ...,     25,   7745,     33],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [    39,   1666,    195,  ...,      0,      0,      0],\n",
      "        [184045,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8818, 0.1182],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8254, 0.1746],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8818, 0.1182],\n",
      "        [0.8087, 0.1913],\n",
      "        [0.8818, 0.1182]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1600\n",
      "tensor([[11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        [   39,  1666,  6988,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5619, 31158,    80,  ...,     0,     0,     0],\n",
      "        [   39,  1292,   272,  ...,    23,   184,  4756],\n",
      "        [  306, 23305,   526,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7992, 0.2008],\n",
      "        [0.8250, 0.1750],\n",
      "        [0.7754, 0.2246],\n",
      "        [0.8827, 0.1173],\n",
      "        [0.8827, 0.1173],\n",
      "        [0.8276, 0.1724],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.8827, 0.1173],\n",
      "        [0.8167, 0.1833],\n",
      "        [0.8827, 0.1173]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1700\n",
      "tensor([[283716,     80,   9040,  ...,   1216,  19279,    182],\n",
      "        [ 35615,  36144,  86707,  ...,   2472,    317,   2855],\n",
      "        [223509,  31158,     80,  ...,   2912,    537,     30],\n",
      "        ...,\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   1580,  ...,    393,    737,     75],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8454, 0.1546],\n",
      "        [0.8188, 0.1812],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.8438, 0.1562],\n",
      "        [0.8403, 0.1597],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.8063, 0.1937],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.8195, 0.1805]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1800\n",
      "tensor([[  1643,     33,   4898,  ...,  43057,   1075,     75],\n",
      "        [228532,     80,   9040,  ...,     33,  40968,    151],\n",
      "        [  2916,   8069,    173,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [281751,     23,  99282,  ...,  20664,     17,   3400],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    380,     24,    134]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8849, 0.1151],\n",
      "        [0.8849, 0.1151],\n",
      "        [0.8849, 0.1151],\n",
      "        [0.8849, 0.1151],\n",
      "        [0.8260, 0.1740],\n",
      "        [0.8515, 0.1485],\n",
      "        [0.8495, 0.1505],\n",
      "        [0.8849, 0.1151],\n",
      "        [0.8100, 0.1900],\n",
      "        [0.8205, 0.1795]], grad_fn=<SoftmaxBackward>) yhat\n",
      "1900\n",
      "tensor([[  2627,      6,    779,  ...,      0,      0,      0],\n",
      "        [271627,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,  28118,  84157,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    168,  17415,    354],\n",
      "        [    39,   3804,  45995,  ...,    291,    202,  54317]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8840, 0.1160],\n",
      "        [0.8220, 0.1780],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8496, 0.1504],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8848, 0.1152],\n",
      "        [0.8102, 0.1898]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2000\n",
      "tensor([[     5,      6,    202,  ...,      0,      0,      0],\n",
      "        [   691,  22860, 145703,  ...,     39,   5728,     87],\n",
      "        [224795,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [    39,   8315,    340,  ...,      0,      0,      0],\n",
      "        [224807,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    48,    550,     12,  ...,   2213,  21504,   5125]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8862, 0.1138],\n",
      "        [0.8862, 0.1138],\n",
      "        [0.8862, 0.1138],\n",
      "        [0.8571, 0.1429],\n",
      "        [0.8558, 0.1442],\n",
      "        [0.8862, 0.1138],\n",
      "        [0.8862, 0.1138],\n",
      "        [0.7978, 0.2022],\n",
      "        [0.8469, 0.1531],\n",
      "        [0.8862, 0.1138]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2100\n",
      "tensor([[  1920,  35060,   3146,  ...,      0,      0,      0],\n",
      "        [155017,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6931,    184,    134,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229682,     23, 146543,  ...,     57,   2004,  78263],\n",
      "        [  2336,     33,    408,  ...,     13,   1284,   4579],\n",
      "        [  1344,   4118,    215,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8869, 0.1131],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8317, 0.1683],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8439, 0.1561],\n",
      "        [0.8593, 0.1407],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8869, 0.1131]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2200\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [274054,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  6450,     19,     26,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  3386,   1140,     25,  ...,    855,    539, 101895],\n",
      "        [253492,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 26069,     45,   4465,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8868, 0.1132],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.8869, 0.1131],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.8870, 0.1130]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2300\n",
      "tensor([[  6901,    682,    168,  ...,      0,      0,      0],\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [229920,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [151345,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5619,  31158,     80,  ...,      0,      0,      0],\n",
      "        [    39,   4578,   3264,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8403, 0.1597],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.8874, 0.1126]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2400\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  5753,    673,   1285,  ...,      0,      0,      0],\n",
      "        [226523,     80,   9040,  ...,     23,    299,     26],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8227, 0.1773],\n",
      "        [0.8881, 0.1119],\n",
      "        [0.8470, 0.1530],\n",
      "        [0.8881, 0.1119],\n",
      "        [0.7846, 0.2154],\n",
      "        [0.8146, 0.1854],\n",
      "        [0.8400, 0.1600],\n",
      "        [0.8881, 0.1119],\n",
      "        [0.8873, 0.1127],\n",
      "        [0.7887, 0.2113]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2500\n",
      "tensor([[ 11456,     80,   9040,  ...,     33,  24230,   1754],\n",
      "        [   318,     87,    278,  ...,      0,      0,      0],\n",
      "        [    39,  97080,     20,  ...,    497,    191,    195],\n",
      "        ...,\n",
      "        [156986,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1643,     13,   4053,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,    434,   5200,   7419]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8562, 0.1438],\n",
      "        [0.8894, 0.1106],\n",
      "        [0.8894, 0.1106],\n",
      "        [0.8894, 0.1106],\n",
      "        [0.8210, 0.1790],\n",
      "        [0.8894, 0.1106],\n",
      "        [0.8264, 0.1736],\n",
      "        [0.8894, 0.1106],\n",
      "        [0.7953, 0.2047],\n",
      "        [0.8894, 0.1106]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2600\n",
      "tensor([[ 82374,  71026,     17,  ...,   1337,     13,   4418],\n",
      "        [  1004,    153,     19,  ...,      0,      0,      0],\n",
      "        [ 14962,     33,    374,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1176,     63,   2413,  ...,      0,      0,      0],\n",
      "        [ 27819,  12766,     23,  ...,   1337,     33,   2052],\n",
      "        [    39, 194425,   1155,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8917, 0.1083],\n",
      "        [0.7888, 0.2112],\n",
      "        [0.8917, 0.1083],\n",
      "        [0.8507, 0.1493],\n",
      "        [0.8420, 0.1580],\n",
      "        [0.8917, 0.1083],\n",
      "        [0.7785, 0.2215],\n",
      "        [0.8070, 0.1930],\n",
      "        [0.8917, 0.1083],\n",
      "        [0.8448, 0.1552]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2700\n",
      "tensor([[184034,     23,   9845,  ...,      0,      0,      0],\n",
      "        [164254,     80,   9040,  ...,   1497, 165486, 207960],\n",
      "        [228532,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [239851,     80,   9040,  ...,   1281,  12563,     26],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  1505,    123,  39608,  ...,    271,      0,      0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8190, 0.1810],\n",
      "        [0.8915, 0.1085],\n",
      "        [0.8915, 0.1085],\n",
      "        [0.8432, 0.1568],\n",
      "        [0.8264, 0.1736],\n",
      "        [0.8378, 0.1622],\n",
      "        [0.8915, 0.1085],\n",
      "        [0.8443, 0.1557],\n",
      "        [0.8399, 0.1601],\n",
      "        [0.8056, 0.1944]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2800\n",
      "tensor([[238655,     80,   9040,  ...,   5045,     23,     33],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [     5,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   959,    195,   4418,  ...,    321,      9,  10966],\n",
      "        [ 11456,     80,   9040,  ..., 221684,    415,    380],\n",
      "        [    39,  11142,  56734,  ...,     33, 154417,   2366]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8921, 0.1079],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.8137, 0.1863],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.8920, 0.1080],\n",
      "        [0.8921, 0.1079],\n",
      "        [0.7936, 0.2064]], grad_fn=<SoftmaxBackward>) yhat\n",
      "2900\n",
      "tensor([[270542,     80,   9040,  ...,      0,      0,      0],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [  2101,     26,   1213,  ...,    460,     57,  35040],\n",
      "        ...,\n",
      "        [237044,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,   3590,  24018,    317]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8480, 0.1520],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.8537, 0.1463],\n",
      "        [0.8838, 0.1162],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.8633, 0.1367],\n",
      "        [0.8927, 0.1073]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3000\n",
      "tensor([[  348,   195, 49873,  ...,  2285,   184,  7332],\n",
      "        [   80,  9040,    83,  ...,     0,     0,     0],\n",
      "        [11456,    80,  9040,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   39, 48223,   215,  ...,     0,     0,     0],\n",
      "        [ 4308,    87,   919,  ...,    23,   136,   550],\n",
      "        [ 2108,    87,    81,  ...,     0,     0,     0]]) x_batch\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8318, 0.1682],\n",
      "        [0.8935, 0.1065],\n",
      "        [0.8351, 0.1649],\n",
      "        [0.8893, 0.1107],\n",
      "        [0.8476, 0.1524],\n",
      "        [0.8382, 0.1618],\n",
      "        [0.8321, 0.1679],\n",
      "        [0.8935, 0.1065],\n",
      "        [0.8578, 0.1422],\n",
      "        [0.8270, 0.1730]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3100\n",
      "tensor([[ 14795,     87,     33,  ...,    190,   1028,     24],\n",
      "        [    39,      5,      6,  ...,      0,      0,      0],\n",
      "        [156247,     80,   9040,  ...,    286,     33,    820],\n",
      "        ...,\n",
      "        [  2424,  13494,  15193,  ...,      0,      0,      0],\n",
      "        [    48,    550,    102,  ...,   2443,     33, 179310],\n",
      "        [156986,     80,   9040,  ...,    182,    745,    180]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8266, 0.1734],\n",
      "        [0.8199, 0.1801],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.8939, 0.1061],\n",
      "        [0.8939, 0.1061],\n",
      "        [0.8939, 0.1061],\n",
      "        [0.7986, 0.2014],\n",
      "        [0.8353, 0.1647],\n",
      "        [0.8939, 0.1061],\n",
      "        [0.8235, 0.1765]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3200\n",
      "tensor([[   55,   189,     9,  ...,    55,   321,     9],\n",
      "        [  828,   587,    25,  ..., 39620,  1180,  1642],\n",
      "        [11456,    80,  9040,  ...,   494,    26,    33],\n",
      "        ...,\n",
      "        [20278, 42710,   153,  ...,  3351,    57,    33],\n",
      "        [ 3155,   154,    57,  ...,     0,     0,     0],\n",
      "        [12353,    13, 22134,  ...,    26, 40852,   352]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8953, 0.1047],\n",
      "        [0.8535, 0.1465],\n",
      "        [0.8953, 0.1047],\n",
      "        [0.8419, 0.1581],\n",
      "        [0.8953, 0.1047],\n",
      "        [0.8953, 0.1047],\n",
      "        [0.8953, 0.1047],\n",
      "        [0.8238, 0.1762],\n",
      "        [0.8324, 0.1676],\n",
      "        [0.8422, 0.1578]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3300\n",
      "tensor([[  2849,   1931,   2887,  ...,      0,      0,      0],\n",
      "        [  1011,   6781,    737,  ...,   4540,   2117,     75],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  5619,  31158,     80,  ..., 227336,  16898,     17],\n",
      "        [    55,    286,    228,  ...,  80382,   9969,   2332],\n",
      "        [  6953,     23,   2180,  ...,     23,  91635,  16592]]) x_batch\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7780, 0.2220],\n",
      "        [0.8956, 0.1044],\n",
      "        [0.8956, 0.1044],\n",
      "        [0.8315, 0.1685],\n",
      "        [0.8954, 0.1046],\n",
      "        [0.8092, 0.1908],\n",
      "        [0.8956, 0.1044],\n",
      "        [0.8956, 0.1044],\n",
      "        [0.8956, 0.1044],\n",
      "        [0.8323, 0.1677]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3400\n",
      "tensor([[ 11456,     80,   9040,  ...,   4917, 232330,   3635],\n",
      "        [     0,      0,      0,  ...,      0,      0,      0],\n",
      "        [  2954,   3496,   6107,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   348,     87,   3095,  ...,      0,      0,      0],\n",
      "        [  2849,     17,   1140,  ...,      0,      0,      0],\n",
      "        [   348,   1102,   2971,  ...,     23,    518,   3540]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8324, 0.1676],\n",
      "        [0.8357, 0.1643],\n",
      "        [0.8060, 0.1940],\n",
      "        [0.8414, 0.1586],\n",
      "        [0.8971, 0.1029],\n",
      "        [0.8971, 0.1029],\n",
      "        [0.8971, 0.1029],\n",
      "        [0.8971, 0.1029],\n",
      "        [0.7457, 0.2543],\n",
      "        [0.8641, 0.1359]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3500\n",
      "tensor([[226803,     80,   9040,  ...,   1465,    134,  24526],\n",
      "        [148423,     23, 146543,  ...,    151,    199, 258704],\n",
      "        [287560,     80,   9040,  ...,    380,    153,    434],\n",
      "        ...,\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [226143,     80,   9040,  ...,    168,   1785,   3191],\n",
      "        [  1140,   2619,   1324,  ...,   7892,   6766,   2324]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.7994, 0.2006],\n",
      "        [0.8582, 0.1418],\n",
      "        [0.8977, 0.1023],\n",
      "        [0.8977, 0.1023],\n",
      "        [0.8762, 0.1238],\n",
      "        [0.8923, 0.1077],\n",
      "        [0.8961, 0.1039],\n",
      "        [0.8977, 0.1023],\n",
      "        [0.8977, 0.1023],\n",
      "        [0.8977, 0.1023]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3600\n",
      "tensor([[196264,     80,   9040,  ..., 110862,     40,   6262],\n",
      "        [ 18735,     77,  71050,  ...,    469,   3551,     26],\n",
      "        [   556,    557,  59223,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [229424,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    39,  12454,   3771,  ...,      0,      0,      0],\n",
      "        [ 11456,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.8984, 0.1016],\n",
      "        [0.8984, 0.1016],\n",
      "        [0.8984, 0.1016],\n",
      "        [0.8984, 0.1016],\n",
      "        [0.8158, 0.1842],\n",
      "        [0.7554, 0.2446],\n",
      "        [0.8361, 0.1639],\n",
      "        [0.8984, 0.1016],\n",
      "        [0.8984, 0.1016],\n",
      "        [0.8984, 0.1016]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3700\n",
      "tensor([[    80,   9040,     83,  ...,      0,      0,      0],\n",
      "        [186960,     80,   9040,  ...,      0,      0,      0],\n",
      "        [    66,      6,    202,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  1746,   1222,     23,  ...,      0,      0,      0],\n",
      "        [   959,      7,   3064,  ...,      0,      0,      0],\n",
      "        [267953,     80,   9040,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) y_batch\n",
      "tensor([[0.7937, 0.2063],\n",
      "        [0.8986, 0.1014],\n",
      "        [0.8986, 0.1014],\n",
      "        [0.8986, 0.1014],\n",
      "        [0.8981, 0.1019],\n",
      "        [0.8986, 0.1014],\n",
      "        [0.8797, 0.1203],\n",
      "        [0.8415, 0.1585],\n",
      "        [0.8986, 0.1014],\n",
      "        [0.8986, 0.1014]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3800\n",
      "tensor([[187396,     80,   9040,  ...,   5241,    195,  90777],\n",
      "        [   565,    184,  59549,  ...,      0,      0,      0],\n",
      "        [ 57654,  14867,     23,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  7116,  11820,    195,  ...,    708,     33,   8198],\n",
      "        [196264,     80,   9040,  ...,      0,      0,      0],\n",
      "        [   348,     87,    449,  ...,      0,      0,      0]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]]) y_batch\n",
      "tensor([[0.8990, 0.1010],\n",
      "        [0.8990, 0.1010],\n",
      "        [0.7592, 0.2408],\n",
      "        [0.8389, 0.1611],\n",
      "        [0.8202, 0.1798],\n",
      "        [0.8990, 0.1010],\n",
      "        [0.8463, 0.1537],\n",
      "        [0.8429, 0.1571],\n",
      "        [0.8293, 0.1707],\n",
      "        [0.8492, 0.1508]], grad_fn=<SoftmaxBackward>) yhat\n",
      "3900\n",
      "tensor([[ 11456,     80,   9040,  ...,      0,      0,      0],\n",
      "        [ 43634,   8524, 147656,  ...,      0,      0,      0],\n",
      "        [    80, 222369,     13,  ...,   6618,  27482,    198],\n",
      "        ...,\n",
      "        [  4813,      9,   3030,  ...,   8945,    971,  13995],\n",
      "        [  1043,   1006,     94,  ...,   3773,    268,     33],\n",
      "        [ 16500,     17,   9610,  ...,    550,     94,   1287]]) x_batch\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) y_batch\n",
      "0.5236250000000013 train_accuracy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) yval\n",
      "0 lineno1\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0]) y_val_updated\n",
      "0 lineno2\n",
      "0.5186094069529659 valid_accuracy\n",
      "1467 len(val_losses)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embedding.weight', tensor([[-0.1369, -0.2651,  0.4201,  ...,  0.3693,  0.9930, -0.4033],\n",
      "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556],\n",
      "        [ 0.6056, -1.0701,  1.1141,  ..., -0.0452,  0.4221,  0.1775],\n",
      "        ...,\n",
      "        [ 0.3230,  0.8263, -1.0287,  ..., -1.3893,  0.1735, -0.1911],\n",
      "        [ 0.0469, -0.0180, -0.1875,  ..., -0.4879,  0.6600,  0.4085],\n",
      "        [-0.2246, -0.0722,  0.0473,  ...,  0.0635,  0.2814,  0.2100]])), ('gru.weight_ih_l0', tensor([[-0.1178,  0.0750, -0.1621,  ..., -0.2519,  0.0876, -0.1885],\n",
      "        [ 0.0444, -0.1321, -0.0226,  ..., -0.1250, -0.0472, -0.1916],\n",
      "        [-0.0707,  0.0703, -0.2266,  ...,  0.1639,  0.0501, -0.0111],\n",
      "        ...,\n",
      "        [ 0.1547,  0.0371,  0.2391,  ..., -0.1766,  0.0677,  0.1759],\n",
      "        [ 0.0692, -0.2492,  0.1493,  ...,  0.1858, -0.2025, -0.1705],\n",
      "        [-0.0766,  0.2009,  0.1715,  ..., -0.0649, -0.1920,  0.0299]])), ('gru.weight_hh_l0', tensor([[-0.0844, -0.1254,  0.2352,  ...,  0.1981, -0.2047, -0.0193],\n",
      "        [-0.0305,  0.1894, -0.0279,  ..., -0.1078, -0.0934, -0.2022],\n",
      "        [ 0.1590, -0.0466, -0.2331,  ..., -0.1004,  0.2041, -0.1434],\n",
      "        ...,\n",
      "        [-0.1800,  0.0826,  0.2252,  ..., -0.0585, -0.0096,  0.1316],\n",
      "        [ 0.0283, -0.0527, -0.1056,  ..., -0.1356, -0.0132,  0.1426],\n",
      "        [ 0.1403, -0.0122,  0.1105,  ..., -0.0389,  0.1003,  0.1999]])), ('gru.bias_ih_l0', tensor([ 0.1970, -0.1041,  0.0751, -0.1508,  0.0334,  0.1037, -0.1210,  0.0684,\n",
      "        -0.2132,  0.2326, -0.1534, -0.0614,  0.2268,  0.1569,  0.1891, -0.1312,\n",
      "        -0.0733,  0.1710, -0.0890,  0.1115, -0.1025,  0.0353,  0.0381,  0.0859,\n",
      "        -0.1097,  0.0429, -0.2015, -0.0567, -0.1659,  0.1638, -0.0783, -0.0795,\n",
      "         0.0031, -0.0717,  0.0346,  0.0691,  0.1436, -0.1623,  0.1850,  0.1367,\n",
      "        -0.0124,  0.2124,  0.1847, -0.0460,  0.1171,  0.1897, -0.0037,  0.1446,\n",
      "         0.1825,  0.0566, -0.2189, -0.1938, -0.1436,  0.1619,  0.1931,  0.1065,\n",
      "        -0.0041, -0.0064,  0.0542,  0.1484])), ('gru.bias_hh_l0', tensor([ 0.1756,  0.0458,  0.0022,  0.1202,  0.0062, -0.1956,  0.2486,  0.2290,\n",
      "        -0.1753,  0.0859,  0.0776,  0.1563, -0.1747,  0.1034,  0.1725, -0.0312,\n",
      "        -0.0077,  0.2496, -0.1541, -0.0279,  0.1309,  0.1091, -0.1655, -0.1975,\n",
      "        -0.0750, -0.0799,  0.1568,  0.0018, -0.0252, -0.0156, -0.1118, -0.0034,\n",
      "         0.0120,  0.1259,  0.2214, -0.0984, -0.2030,  0.2155,  0.1952,  0.1023,\n",
      "        -0.2535, -0.1112,  0.1821, -0.0365, -0.2194, -0.2003,  0.1648,  0.2090,\n",
      "         0.0062,  0.0552, -0.0267, -0.0271,  0.1554, -0.1297,  0.0556,  0.1276,\n",
      "        -0.0600,  0.1786, -0.0740, -0.0620])), ('gru.weight_ih_l1', tensor([[ 1.8190e-01,  1.7541e-01,  1.9051e-01,  ...,  1.2930e-01,\n",
      "         -2.7184e-02,  1.3961e-02],\n",
      "        [ 1.7016e-01, -8.0629e-02,  1.1202e-01,  ...,  4.8427e-05,\n",
      "         -1.6398e-01,  4.4707e-02],\n",
      "        [-1.6479e-01, -1.5681e-02, -7.6956e-02,  ...,  3.7133e-02,\n",
      "         -6.7493e-02, -1.5172e-01],\n",
      "        ...,\n",
      "        [ 1.3726e-02,  2.1113e-01, -2.1862e-02,  ...,  2.3427e-01,\n",
      "         -2.0750e-01,  2.0416e-02],\n",
      "        [-1.7103e-02,  1.2002e-01,  1.0206e-01,  ..., -2.0870e-01,\n",
      "          2.2741e-01,  1.2598e-01],\n",
      "        [ 8.1398e-02, -1.8885e-01,  6.3216e-02,  ...,  4.7143e-02,\n",
      "         -6.2908e-02,  1.2849e-01]])), ('gru.weight_hh_l1', tensor([[ 0.0343, -0.0909, -0.0159,  ..., -0.1107, -0.0138,  0.1920],\n",
      "        [ 0.1004,  0.0426, -0.1156,  ...,  0.0515,  0.0613,  0.0966],\n",
      "        [-0.1306, -0.0905,  0.0005,  ...,  0.1201, -0.0894,  0.0765],\n",
      "        ...,\n",
      "        [-0.1401,  0.0552,  0.1138,  ...,  0.1261,  0.1382, -0.0913],\n",
      "        [-0.2563, -0.0678,  0.0525,  ...,  0.1525,  0.0212, -0.0290],\n",
      "        [-0.0872, -0.1865, -0.1481,  ...,  0.0892, -0.1623, -0.1975]])), ('gru.bias_ih_l1', tensor([-0.1444,  0.1156, -0.0042,  0.1551,  0.0538, -0.1516,  0.0678,  0.1621,\n",
      "        -0.0794, -0.1785,  0.0227,  0.0763,  0.2249,  0.2342,  0.0488, -0.0256,\n",
      "        -0.0802,  0.0158, -0.1359,  0.1271,  0.2271, -0.0163,  0.1643,  0.0366,\n",
      "         0.0237,  0.0654,  0.1579, -0.1113,  0.0065,  0.2134, -0.1566, -0.0691,\n",
      "         0.1977, -0.1560,  0.0149, -0.1070, -0.0980, -0.1233, -0.2008,  0.2152,\n",
      "         0.1505, -0.0863, -0.2437, -0.0431,  0.1574, -0.1071, -0.0432,  0.0207,\n",
      "        -0.2136, -0.1446,  0.0436, -0.1870,  0.0537, -0.0690, -0.0697, -0.2199,\n",
      "         0.1432,  0.0675, -0.0520,  0.0812])), ('gru.bias_hh_l1', tensor([-0.0053,  0.0215, -0.1545,  0.1439,  0.1990,  0.0023,  0.0569,  0.1191,\n",
      "         0.0181, -0.1494,  0.0140,  0.0187, -0.0259,  0.2102,  0.1396, -0.1734,\n",
      "        -0.1746,  0.1173,  0.0144, -0.1712, -0.0206, -0.1159,  0.1047,  0.1945,\n",
      "        -0.0316, -0.2037,  0.0027, -0.0867,  0.0888,  0.0392,  0.1787, -0.1861,\n",
      "        -0.0402, -0.1715,  0.0716, -0.0997, -0.0036,  0.2072,  0.1322,  0.1403,\n",
      "         0.0619,  0.2455,  0.0963,  0.1927, -0.2274, -0.0075,  0.1890, -0.1483,\n",
      "        -0.2236,  0.1606,  0.1218, -0.0926, -0.0576,  0.0651, -0.0455, -0.0814,\n",
      "        -0.1037,  0.1113, -0.1724,  0.0538])), ('gru.weight_ih_l2', tensor([[ 0.1165,  0.2256,  0.0779,  ...,  0.0174, -0.1474, -0.1426],\n",
      "        [-0.0953,  0.2134, -0.2459,  ..., -0.0618, -0.2008,  0.1916],\n",
      "        [-0.1996, -0.0747, -0.0921,  ...,  0.1949,  0.0111,  0.1073],\n",
      "        ...,\n",
      "        [-0.2617,  0.0150,  0.0438,  ..., -0.1792,  0.0706, -0.1851],\n",
      "        [-0.1776,  0.0886, -0.0360,  ...,  0.1199, -0.0304, -0.0957],\n",
      "        [-0.1669, -0.2024, -0.1773,  ..., -0.2030,  0.2288, -0.1274]])), ('gru.weight_hh_l2', tensor([[-0.1141, -0.2049, -0.1494,  ...,  0.0104, -0.0858,  0.0280],\n",
      "        [ 0.1208,  0.0033, -0.0195,  ...,  0.1006, -0.1876,  0.1417],\n",
      "        [ 0.0558, -0.1361, -0.1539,  ..., -0.0425,  0.1301, -0.1598],\n",
      "        ...,\n",
      "        [-0.2289,  0.0389,  0.0071,  ...,  0.1456, -0.0648, -0.1358],\n",
      "        [ 0.2340,  0.0507, -0.2061,  ..., -0.0118, -0.0562,  0.1086],\n",
      "        [-0.0185, -0.0911, -0.1733,  ..., -0.1942, -0.1384,  0.1139]])), ('gru.bias_ih_l2', tensor([ 0.0602,  0.0575, -0.1925,  0.1040, -0.0252,  0.1798,  0.0355,  0.0104,\n",
      "         0.1863,  0.0689, -0.1903,  0.0682,  0.0370,  0.1954,  0.0462,  0.1679,\n",
      "         0.1935, -0.0474,  0.0790, -0.0337, -0.0898,  0.1588, -0.0232,  0.1535,\n",
      "         0.0173,  0.1765,  0.1921,  0.1478, -0.0770, -0.1308, -0.1005, -0.0357,\n",
      "        -0.2040,  0.0973, -0.1127, -0.2068,  0.1013,  0.1246, -0.1775,  0.1699,\n",
      "        -0.1013, -0.0423, -0.1154, -0.1654, -0.0607, -0.0790,  0.1271, -0.1502,\n",
      "        -0.1358, -0.1066,  0.1545, -0.1472,  0.0986,  0.1879,  0.1636, -0.1195,\n",
      "         0.0555,  0.1229,  0.1997,  0.1753])), ('gru.bias_hh_l2', tensor([ 0.2005, -0.1215,  0.2284, -0.1762,  0.1538,  0.1781,  0.0620,  0.1289,\n",
      "        -0.0764, -0.1830,  0.0527,  0.1105,  0.0124,  0.0890, -0.1510,  0.1085,\n",
      "        -0.0411,  0.0588, -0.0237, -0.1367,  0.1939,  0.1978, -0.1398, -0.1326,\n",
      "        -0.0432, -0.0619, -0.1933, -0.0117, -0.1206,  0.1065,  0.1395, -0.1555,\n",
      "        -0.1530,  0.1459,  0.1441, -0.0015,  0.1884,  0.2046,  0.0752,  0.0025,\n",
      "        -0.1007, -0.2501, -0.1738,  0.1613,  0.2414,  0.1526, -0.2420, -0.1043,\n",
      "         0.1953, -0.1648,  0.1396,  0.0933,  0.1113,  0.1202, -0.0928, -0.1085,\n",
      "         0.0022, -0.0581,  0.1049,  0.0091])), ('lin.weight', tensor([[ 0.0549, -0.2098,  0.2066, -0.1117,  0.2257,  0.2524, -0.1506, -0.1022,\n",
      "          0.0857, -0.1229,  0.0660,  0.2320,  0.1896, -0.1479,  0.0409,  0.0850,\n",
      "          0.0158, -0.2004,  0.1896, -0.0599],\n",
      "        [-0.1551,  0.2091,  0.1310,  0.1702, -0.0356, -0.2084, -0.1304, -0.1074,\n",
      "          0.1103, -0.0626,  0.1799, -0.0857,  0.0258, -0.0023,  0.0924, -0.1961,\n",
      "          0.0862, -0.0984, -0.0307,  0.2051]])), ('lin.bias', tensor([0.2153, 0.1569]))])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(toynet, loss_fn, optimizer)\n",
    "losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    lineno = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "#         x_batch = x_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "        \n",
    "\n",
    "#         loss, batch_accuracy = train_step(x_batch, y_batch)\n",
    "        loss, accuracy = train_step(x_batch, y_batch, lineno)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        if lineno % 100 == 0:\n",
    "            print(lineno)\n",
    "            print(x_batch, \"x_batch\")\n",
    "            print(y_batch, \"y_batch\")\n",
    "        lineno += 1\n",
    "        \n",
    "        # for some reason this is necessary to keep the epoch loop running\n",
    "        # if lineno == batch_size, then break the train loop\n",
    "#         if lineno == 5000:\n",
    "#             break\n",
    "\n",
    "    train_accuracy = sum(accuracies)/len(accuracies)\n",
    "    print(train_accuracy, \"train_accuracy\")\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"losses\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "    with torch.no_grad():\n",
    "        lineno1 = 0\n",
    "        lineno2 = 0\n",
    "        \n",
    "        for x_val, y_val in val_loader:\n",
    "#             x_val = x_val.to(device)\n",
    "#             y_val = y_val.to(device)\n",
    "            \n",
    "           \n",
    "            # initialize accuracy counter (averaging these accuracies is accurate, as all batches are size 10)\n",
    "            correct = 0\n",
    "            total = len(x_val)\n",
    "        \n",
    "    \n",
    "            if lineno1 % 1000 == 0:\n",
    "                print(y_val, \"yval\")\n",
    "                print(lineno1, \"lineno1\")\n",
    "                \n",
    "            lineno1 += 1\n",
    "    \n",
    "            toynet.eval()\n",
    "        \n",
    "            yhat = toynet(x_val, torch.zeros([20]))\n",
    "            \n",
    "            # convert yhat (tensor of lists) to single tensor\n",
    "            init_tensor = y_val\n",
    "#             print(init_tensor, \"init_tensor\")\n",
    "            cat_list = []\n",
    "            for val in enumerate(init_tensor):\n",
    "#             print(val, \"val\")\n",
    "                cat_list.append(val[1][0])\n",
    "                if val[1][0] == torch.argmax(yhat[val[0]]):\n",
    "#                 print(val[1][0], \"target val\")\n",
    "#                 print(yhat[val[0]], \"predicted val\")\n",
    "                    correct += 1\n",
    "            y_val_updated = torch.LongTensor(cat_list)\n",
    "        \n",
    "            if lineno2 % 1000 == 0:\n",
    "                print(y_val_updated, \"y_val_updated\")\n",
    "                print(lineno2, \"lineno2\")\n",
    "                \n",
    "            lineno2 += 1\n",
    "            \n",
    "            # calculate accuracy\n",
    "            val_accuracy = correct/total\n",
    "            \n",
    "            \n",
    "            val_loss = loss_fn(yhat, y_val_updated)\n",
    "            val_losses.append(val_loss.item())\n",
    "           \n",
    "            val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        \n",
    "        valid_accuracy = sum(val_accuracies)/len(val_accuracies)\n",
    "        print(valid_accuracy, \"valid_accuracy\")\n",
    "        \n",
    "        print(len(val_losses), \"len(val_losses)\")\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle(\"val_losses\")\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    \n",
    "# Checks model's parameters\n",
    "print(toynet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
