{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "fake_news_frame = pd.read_csv('fake_and_real_news/Fake.csv')\n",
    "true_news_frame = pd.read_csv('fake_and_real_news/True.csv')\n",
    "\n",
    "# add authenticity label\n",
    "\n",
    "\n",
    "fake_column = [\"Fake\"] * len(fake_news_frame)\n",
    "#print(len(fake_column))\n",
    "\n",
    "fake_news_frame.insert(4, 'authenticity', fake_column)\n",
    "#fake_news_frame.to_csv(path_or_buf='fake_and_real_news/NewFake.csv')\n",
    "\n",
    "\n",
    "true_column = [\"True\"] * len(true_news_frame)\n",
    "#print(len(true_column))\n",
    "\n",
    "true_news_frame.insert(4, 'authenticity', true_column)\n",
    "#true_news_frame.to_csv(path_or_buf='fake_and_real_news/NewTrue.csv')\n",
    "\n",
    "\n",
    "# combine datasets\n",
    "\n",
    "frames = [fake_news_frame, true_news_frame]\n",
    "combined_news_frame = pd.concat(frames)\n",
    "\n",
    "combined_news_frame.to_csv(path_or_buf='fake_and_real_news/Combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Drunk Bragging Trump Staffer Started Russian Collusion Investigation\n",
      "Text: House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.\n",
      "Subject: News\n",
      "Date: December 31, 2017\n",
      "Authenticity: Fake\n"
     ]
    }
   ],
   "source": [
    "news_frame = pd.read_csv('fake_and_real_news/Combined.csv')\n",
    "\n",
    "n = 1\n",
    "title = news_frame.iloc[n, 1]\n",
    "text = news_frame.iloc[n, 2]\n",
    "subject = news_frame.iloc[n, 3]\n",
    "date = news_frame.iloc[n, 4]\n",
    "authenticity = news_frame.iloc[n, 5]\n",
    "\n",
    "print('Title: {}'.format(title))\n",
    "print('Text: {}'.format(text))\n",
    "print('Subject: {}'.format(subject))\n",
    "print('Date: {}'.format(date))\n",
    "print('Authenticity: {}'.format(authenticity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\" News dataset. \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"Args:\n",
    "            csv_file (string): Path to the news csv file.\n",
    "            root_dir (string): Path to the root directory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.news_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.news_frame)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        news = self.news_frame.iloc[idx, 1:]\n",
    "        \n",
    "        # we are going to have to make some way to parse text as words, and to feed it to the NN\n",
    "        \n",
    "        return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset('fake_and_real_news/Combined.csv', 'fake_and_real_news/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: populate text dataset with news article texts\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "text_dataset = []\n",
    "\n",
    "for idx in range(0, len(news_dataset)):\n",
    "    text_dataset.append(news_dataset[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/paul/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/paul/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/paul/.local/lib/python3.6/site-packages (from nltk) (4.48.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/paul/.local/lib/python3.6/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/paul/.local/lib/python3.6/site-packages (from nltk) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing: encode one news article using nltk encoder\n",
    "import nltk\n",
    "\n",
    "from torchnlp.encoders.text import TreebankEncoder\n",
    "\n",
    "\n",
    "encoder = TreebankEncoder(text_dataset)\n",
    "encoder.encode(\"Test input.\")\n",
    "print(len(encoder.vocab))\n",
    "\n",
    "encoder.encode(text_dataset[10923])\n",
    "\n",
    "#lineno = 0\n",
    "\n",
    "#for text in text_dataset.values():\n",
    "#    encoder.encode(text)\n",
    "#    if lineno % 5000 == 0:\n",
    "#        print(lineno)\n",
    "#    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(text_dataset[10923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineno = 0\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for text in text_dataset:\n",
    "    lengths.append(len(text))\n",
    "#     lineno += 1\n",
    "#     if lineno % 5000 == 0:\n",
    "#         print(lineno)\n",
    "\n",
    "MAX_TEXT_LENGTH = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51794"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: pad texts to have matching length\n",
    "\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import pad_tensor\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "padded_texts = []\n",
    "\n",
    "for text in text_dataset:\n",
    "    padded_texts.append(pad_tensor(encoder.encode(text).long(), MAX_TEXT_LENGTH))\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: convert longs to floats (for NN)\n",
    "\n",
    "# lineno = 0\n",
    "\n",
    "# for text in padded_texts:\n",
    "#     for long in text:\n",
    "#         long = float(long)\n",
    "#     if lineno % 5000 == 0:\n",
    "#         print(lineno)\n",
    "#     lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([272, 273, 274,  ...,   0,   0,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: split news article texts into strings, store in dictionary\n",
    "\n",
    "# dictionary = []\n",
    "# lengths = []\n",
    "\n",
    "# split_texts = []\n",
    "# lineno = 0\n",
    "\n",
    "# for text in text_dataset.values():\n",
    "#     text_words = text.split()\n",
    "#     split_texts.append(text_words)\n",
    "#     #print(len(text), lineno)\n",
    "#     lengths.append(len(text))\n",
    "#     lineno += 1\n",
    "#     for word in text_words:\n",
    "#         if word not in dictionary:\n",
    "#             dictionary.append(word)\n",
    "\n",
    "            \n",
    "# MAX_TEXT_LENGTH = max(lengths)\n",
    "# print(MAX_TEXT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: pad data to provide uniform input to NN\n",
    "\n",
    "# dictionary = {'fake': 0, 'true': 1}\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# random train data to test NN\n",
    "\n",
    "# split_texts = []\n",
    "\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "# split_texts.append(['true', 'true', 'true', 'true', 'true', 'true', 'true'])\n",
    "\n",
    "# MAX_TEXT_LENGTH = 7\n",
    "\n",
    "###\n",
    "\n",
    "# for text in split_texts:\n",
    "#     while len(text) < MAX_TEXT_LENGTH:\n",
    "#         text.append('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: convert split texts (lists of strings) into lists of ints, using dictionary\n",
    "\n",
    "# converted_split_texts = []\n",
    "\n",
    "# for text in split_texts:\n",
    "#     converted_text = []\n",
    "#     for word in text:\n",
    "#         converted_text.append(dictionary[word])\n",
    "#     converted_split_texts.append(converted_text)\n",
    "    \n",
    "# print(converted_split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: collect vector of true/fake booleans to train dataset\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "true_fake_dataset = []\n",
    "\n",
    "for idx in np.nditer(rows):\n",
    "    if news_dataset[int(idx)][4] == 'Fake':\n",
    "        true_fake_dataset.append(0)\n",
    "    if news_dataset[int(idx)][4] == 'True':\n",
    "        true_fake_dataset.append(1)\n",
    "        \n",
    "###\n",
    "\n",
    "# random train data to test NN\n",
    "\n",
    "# true_fake_dataset = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Net class\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(MAX_TEXT_LENGTH, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=51794, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "8\n",
      "torch.Size([64, 51794])\n"
     ]
    }
   ],
   "source": [
    "# initialize net and print parameters\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize net with backprop; 3 epochs\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# EPOCHS = 3\n",
    "\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for i, padded_text in enumerate(padded_texts, start=0):\n",
    "#         # data is a batch of featuresets and labels\n",
    "#         #print(true_fake_dataset[i])\n",
    "#         X = padded_text\n",
    "#         #print(X)\n",
    "#         y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "#         if i % 500 == 0:\n",
    "#             print(i)\n",
    "#         #print(y)\n",
    "#         net.zero_grad()\n",
    "#         X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float, NN must read in float\n",
    "# #         print(X_float)\n",
    "#         output = net(X_float.view(-1, MAX_TEXT_LENGTH))\n",
    "#         loss = F.nll_loss(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and print accuracy\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, padded_text in enumerate(padded_texts, start=0):\n",
    "#         X = padded_text\n",
    "#         y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "#         X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float\n",
    "#         output = net(X_float.view(-1, MAX_TEXT_LENGTH))\n",
    "# #         print(torch.argmax(output))\n",
    "#         for idx, i in enumerate(output):\n",
    "#             if torch.argmax(i) == y[idx]:\n",
    "#                 correct += 1\n",
    "#             total += 1\n",
    "\n",
    "# print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glove relations\n",
    "\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'6B.50.dat', mode='w')\n",
    "\n",
    "\n",
    "with open(f'glove.6B.50d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "#         print(vectors[idx])\n",
    "    \n",
    "# print(vectors[20000000:20000050])\n",
    "    \n",
    "# save outputs to disk    \n",
    "\n",
    "vectors = bcolz.carray(vectors[1:].reshape((400001, 50)), rootdir=f'6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create glove dictionary\n",
    "\n",
    "vectors = bcolz.open(f'6B.50.dat')[:]\n",
    "words = pickle.load(open(f'6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.072617, -0.51393 ,  0.4728  , -0.52202 , -0.35534 ,  0.34629 ,\n",
       "        0.23211 ,  0.23096 ,  0.26694 ,  0.41028 ,  0.28031 ,  0.14107 ,\n",
       "       -0.30212 , -0.21095 , -0.10875 , -0.33659 , -0.46313 , -0.40999 ,\n",
       "        0.32764 ,  0.47401 , -0.43449 ,  0.19959 , -0.55808 , -0.34077 ,\n",
       "        0.078477,  0.62823 ,  0.17161 , -0.34454 , -0.2066  ,  0.1323  ,\n",
       "       -1.8076  , -0.38851 ,  0.37654 , -0.50422 , -0.012446,  0.046182,\n",
       "        0.70028 , -0.010573, -0.83629 , -0.24698 ,  0.6888  , -0.17986 ,\n",
       "       -0.066569, -0.48044 , -0.55946 , -0.27594 ,  0.056072, -0.18907 ,\n",
       "       -0.59021 ,  0.55559 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate weights matrix for entire vocab of encoder\n",
    "\n",
    "matrix_len = len(encoder.vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(encoder.vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296562, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = torch.Tensor(weights_matrix).size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.Tensor(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.lin = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        output, h_n = self.gru(self.embedding(inp), self.init_hidden(10))\n",
    "        print(output, \"GRU output (output)\")\n",
    "#         print(x.output.view(seq_len, batch, num_directions, hidden_size), \"GRU output, (output) unpacked\")\n",
    "#         print(x.h_n, \"GRU output (h_n)\")\n",
    "#         print(x.h_n.view(num_layers, num_directions, batch, hidden_size), \"GRU output (h_n) unpacked\")\n",
    "        x = F.relu(self.lin(output[:,-1,:]))\n",
    "        print(x, \"Relu output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "toynet = ToyNN(weights_matrix, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyNN(\n",
      "  (embedding): Embedding(296562, 50)\n",
      "  (gru): GRU(50, 20, num_layers=3, batch_first=True)\n",
      "  (lin): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n",
      "15\n",
      "torch.Size([296562, 50])\n"
     ]
    }
   ],
   "source": [
    "print(toynet)\n",
    "\n",
    "params = list(toynet.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 51794, 50])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(296562, 50)\n",
    "\n",
    "output = embedding(torch.stack(padded_texts[0:10]))\n",
    "# plus_batch = torch.Tensor([3, output])\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = [torch.Tensor([1, 2, 3, 4]), torch.Tensor([5, 6, 7, 8])]\n",
    "tens = torch.stack(tens)\n",
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_padded_texts = torch.stack(padded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44898, 51794])\n",
      "44898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(stacked_padded_texts.size())\n",
    "print(len(padded_texts))\n",
    "padded_texts[10923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0802, -0.0089, -0.0682,  ..., -0.0275, -0.0292, -0.0552],\n",
      "         [ 0.1526, -0.0160, -0.1275,  ..., -0.0467, -0.0839, -0.0824],\n",
      "         [ 0.1915,  0.0049, -0.1622,  ..., -0.0642, -0.0947, -0.1064],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]],\n",
      "\n",
      "        [[ 0.1070, -0.0142, -0.0691,  ...,  0.0047, -0.0547, -0.0737],\n",
      "         [ 0.1517,  0.0010, -0.1778,  ..., -0.0149, -0.0944, -0.1389],\n",
      "         [ 0.1791,  0.0353, -0.2583,  ..., -0.0364, -0.1395, -0.1748],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]],\n",
      "\n",
      "        [[ 0.0557, -0.0187, -0.0648,  ..., -0.0122, -0.0182, -0.0220],\n",
      "         [ 0.1206, -0.0307, -0.0808,  ..., -0.0098, -0.0322, -0.0438],\n",
      "         [ 0.1633, -0.0058, -0.0903,  ..., -0.0190, -0.0405, -0.0685],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0878,  0.0003, -0.0755,  ..., -0.0164, -0.0360, -0.0459],\n",
      "         [ 0.1354,  0.0386, -0.1247,  ..., -0.0485, -0.0500, -0.0795],\n",
      "         [ 0.1700,  0.0676, -0.1504,  ..., -0.0825, -0.0523, -0.1103],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]],\n",
      "\n",
      "        [[ 0.1036, -0.0120, -0.0593,  ..., -0.0147, -0.0392, -0.0552],\n",
      "         [ 0.1631,  0.0202, -0.1106,  ..., -0.0427, -0.0777, -0.1037],\n",
      "         [ 0.1981,  0.0717, -0.1435,  ..., -0.0644, -0.1073, -0.1445],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]],\n",
      "\n",
      "        [[ 0.0699, -0.0025, -0.0973,  ..., -0.0054, -0.0353, -0.0465],\n",
      "         [ 0.1056,  0.0137, -0.1754,  ..., -0.0243, -0.0614, -0.0819],\n",
      "         [ 0.1190,  0.0506, -0.2381,  ..., -0.0424, -0.0796, -0.1086],\n",
      "         ...,\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395],\n",
      "         [ 0.1878,  0.0209, -0.2578,  ..., -0.2110, -0.2441, -0.2395]]],\n",
      "       grad_fn=<TransposeBackward1>) GRU output (output)\n",
      "tensor([[0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000],\n",
      "        [0.0683, 0.0000]], grad_fn=<ReluBackward0>) Relu output\n"
     ]
    }
   ],
   "source": [
    "output = toynet(stacked_padded_texts[0:10], torch.zeros([20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0683, 0.0000], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
