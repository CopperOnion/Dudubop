{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "fake_news_frame = pd.read_csv('fake_and_real_news/Fake.csv')\n",
    "true_news_frame = pd.read_csv('fake_and_real_news/True.csv')\n",
    "\n",
    "# add authenticity label\n",
    "\n",
    "\n",
    "fake_column = [\"Fake\"] * len(fake_news_frame)\n",
    "#print(len(fake_column))\n",
    "\n",
    "fake_news_frame.insert(4, 'authenticity', fake_column)\n",
    "#fake_news_frame.to_csv(path_or_buf='fake_and_real_news/NewFake.csv')\n",
    "\n",
    "\n",
    "true_column = [\"True\"] * len(true_news_frame)\n",
    "#print(len(true_column))\n",
    "\n",
    "true_news_frame.insert(4, 'authenticity', true_column)\n",
    "#true_news_frame.to_csv(path_or_buf='fake_and_real_news/NewTrue.csv')\n",
    "\n",
    "\n",
    "# combine datasets\n",
    "\n",
    "frames = [fake_news_frame, true_news_frame]\n",
    "combined_news_frame = pd.concat(frames)\n",
    "\n",
    "combined_news_frame.to_csv(path_or_buf='fake_and_real_news/Combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Drunk Bragging Trump Staffer Started Russian Collusion Investigation\n",
      "Text: House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.\n",
      "Subject: News\n",
      "Date: December 31, 2017\n",
      "Authenticity: Fake\n"
     ]
    }
   ],
   "source": [
    "news_frame = pd.read_csv('fake_and_real_news/Combined.csv')\n",
    "\n",
    "n = 1\n",
    "title = news_frame.iloc[n, 1]\n",
    "text = news_frame.iloc[n, 2]\n",
    "subject = news_frame.iloc[n, 3]\n",
    "date = news_frame.iloc[n, 4]\n",
    "authenticity = news_frame.iloc[n, 5]\n",
    "\n",
    "print('Title: {}'.format(title))\n",
    "print('Text: {}'.format(text))\n",
    "print('Subject: {}'.format(subject))\n",
    "print('Date: {}'.format(date))\n",
    "print('Authenticity: {}'.format(authenticity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\" News dataset. \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"Args:\n",
    "            csv_file (string): Path to the news csv file.\n",
    "            root_dir (string): Path to the root directory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.news_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.news_frame)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        news = self.news_frame.iloc[idx, 1:]\n",
    "        \n",
    "        # we are going to have to make some way to parse text as words, and to feed it to the NN\n",
    "        \n",
    "        return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset('fake_and_real_news/Combined.csv', 'fake_and_real_news/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing: populate text dataset with news article texts\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "text_dataset = []\n",
    "\n",
    "for idx in range(0, len(news_dataset)):\n",
    "    text_dataset.append(news_dataset[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/paul/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/paul/.local/lib/python3.6/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/paul/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/paul/.local/lib/python3.6/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/paul/.local/lib/python3.6/site-packages (from nltk) (4.48.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/paul/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing: encode one news article using nltk encoder\n",
    "import nltk\n",
    "\n",
    "from torchnlp.encoders.text import TreebankEncoder\n",
    "\n",
    "\n",
    "encoder = TreebankEncoder(text_dataset)\n",
    "encoder.encode(\"Test input.\")\n",
    "print(len(encoder.vocab))\n",
    "\n",
    "encoder.encode(text_dataset[10923])\n",
    "\n",
    "#lineno = 0\n",
    "\n",
    "#for text in text_dataset.values():\n",
    "#    encoder.encode(text)\n",
    "#    if lineno % 5000 == 0:\n",
    "#        print(lineno)\n",
    "#    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(text_dataset[10923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 lineno\n",
      "398 len\n",
      "As Donald Trump s campaign continues to sink deeper into its self-sabotaging downward spiral, it s becoming clear that even Trump s campaign surrogates and former staffers are having trouble trying to stay positive about the outcome of this election. In just the past few days, we ve seen them completely deny that Trump s campaign made some massive changes in desperation, ignore polls that The Donald is losing and now thanks to former Trump campaign manager Corey Lewandowski, they re holding onto false hope that Trump can still somehow win this.Earlier today on CNN, Lewandowski   who is forbidden by contract to say anything negative about Trump   tried to convince everyone that Trump was still on track to win this election because his biggest opponent, Democratic nominee Hillary Clinton, was losing voters to Green Party candidate Jill Stein and Libertarian Party candidate Gary Johnson. Lewandowski said: This is not a two-person race. Gary Johnson and Jill Stein are in this race. I would say Jill Stein should be in the debates. She s a person who is going to take away votes from Hillary Clinton. You just had her here on the show  At the end of the day, Donald Trump is going to win this election, he s going to win in states because Gary Johnson and Jill Stein are going to take votes away from Hillary Clinton. While Lewandowski clung to his delusions, CNN panelist Bill Kristol was sitting right beside him, shaking his head while smiling and muffling his laughter. When Kristol could finally get enough control over himself to speak, he dropped a major truth bomb that Lewandowski and Trump weren t going to like: That s the last recourse of a campaign that s going to lose. It s hoping third-and-fourth-party candidates are going to magically change the equation. At the end of the day, 38% of the American people think Donald Trump should be president, it s never gotten much above that number. You can watch Kristol giggle at Lewandoski s silly hopes below:.@CLewandowski_: Donald Trump is going to win bc Gary Johnson + Jill Stein will take votes away fr Hillary Clinton https://t.co/jvwPm42LGm  New Day (@NewDay) August 18, 2016Featured image is a screenshot\n",
      "10000 lineno\n",
      "196 len\n",
      "Unbelievable! Women s March Activist went off on a crazy rant claiming that somehow President Trump s tax returns are linked to missing black girls in the U.S.:Transcript: to lock arms with one another. We demand that we see Trump s taxes while we also demand that all of our rights are protected. We demand that we see Trump s taxes while we also demand that women s rights be treated as human rights. We must speak with one voice and one accord and we must draw a direct line between Trump s taxes and missing girls, missing black girls all across this country.We must say that while we resist one, we resist all or else our fight is inauthentic. We must say Trump s taxes and equal education. We must say Trump s taxes no more police brutality. We must say that Trump s taxes is where we start today but we will not end there. That all our rights must be protected   and who the hell do you think you are to hide anything and treat us as though we are less than human?Via: GP\n",
      "15000 lineno\n",
      "669 len\n",
      "By looking at the two Democrat presidential candidates left standing, most Americans would think their platform is primarily about the destruction of capitalism and gun control. The real truth however, is that the Democrat party knows without their unyielding support for the culture of death, they would cease to exist. The sickening truth is, they either support the killing of the most vulnerable or it s curtains for them. What does that say about how far our society has fallen when leftist TV hosts celebrate China s brutal  One Child  policy on Twitter?Quintanilla may not have been endorsing the one-child policy, but he didn t offer any overt or even implied criticism of it either. In fact, saying  it worked  sounds a lot like he is suggesting it was a success.The problem, of course, is that the one-child policy, which China converted to a two-child policy this week, was a brutal, harrowing invasion of the human rights of millions of families under the guise of national policy. Since 1979, an estimated 13 million women a year underwent abortions, many of them forced to do so by government officials. Another 200 million women were sterilized under the same policy.https://twitter.com/carlquintanilla/status/659734982915657728These practices mostly went unseen and unnoticed in the era before social media and digital cameras. In 2012, the world got to see the result of this policy on one young woman when her family members posted an image of her lying next to her dead baby after she d been forced to have an abortion.The young woman, Feng Jianmei, already had one child; so after she became pregnant again, family planning officials called her house to persuade her to have an abortion. When that failed, they showed up at her home and spent hours pressuring her for consent. Jianmei slipped out of the house but was followed by a group of 15 officials to her aunt s house. After briefly escaping and hiding under the bed of a relative, Jianmei was found by the government officials and was reportedly carried out by four men.Here is the horrible story about forced abortions in China and their victims:Meanwhile, family planning officials were negotiating with Jianmei s husband. At first they demanded $15,000, but then dropped the birth planning fee to around $5,000. In any case, it was more than he could raise. Finally, Jianmei was forced to give consent by having her thumbprint placed on a form and was injected with a drug that killed her baby. Her family posted this image, which went viral, of Jianmei with her dead daughter. For criticizing the government, government officials led a march denouncing her family as  traitors,  and her husband was beaten.And that nightmare is not the only kind of suffering caused by the policy. Those who dared to violate the policy had to keep their secret children out of sight of government officials. As the Washington Post reported last month, the consequences of getting caught were severe: It was terrifying if you had an over-quota child,  my father says now.  If the government knew, you would be in trouble. People would come to your house, remove all your grains and do anything they could to you. And sometimes, they d destroy your house. My mother recalls:  Even for a new house, they d get on the roof, rip it apart and bulldoze the entire house. We had to keep moving and hiding. It was really painful for us. We knew it wasn t a long-term solution. The one-child policy was a decades long campaign of forced abortion, extortion, sterilization, and terror that traumatized hundreds of millions of people. Saying  it worked  seems like an odd way to sum up such a policy. Via: Breitbart News\n",
      "20000 lineno\n",
      "298 len\n",
      "The way Trump is packing in thousands of supporters at every rally in every state he visits, it s pretty clear his popularity way overshadows that of #CrookedHillary. She s barely getting enough people to fill a phone booth at her  rallies!   This event, at the Baptist National Convention in Kansas City turned out to be downright embarrassing for Hillary and her campaign. In a venue meant to hold 5,000 people, Hillary only drew about 1,000. Perhaps there just isn t a huge amount of interest with Baptist church members in Kansas City when it comes to spending time listening to a sociological liar Workers cutting the room down by a third with lower than expected turnout for #clintonkc speech. pic.twitter.com/L60YFfBpO1  Frank Morris (@FrankNewsman) September 8, 2016Check out the number of empty seats!Convention hall turned out lights in un-occupied part of hall for @HillaryClinton speech, now partitioning it off pic.twitter.com/O5Az8EfiRD  Brian Abel (@BrianAbelTV) September 8, 2016Here are the requested \"during speech\" pics of KC convention hall pic.twitter.com/ZRGOm77xnO  Brian Abel (@BrianAbelTV) September 8, 2016And finally, only 1,000 people filled a venue meant for 5,000. Who needs polls that are rigged by leftist media or polling firms with an agenda. From now on, we re going to continue to use the number of supporters who actually take time out of their busy day to see the candidates as a true indicator of who s leading in the race Front of press constituted 10 rows. 14 seats per section, 6 sections. Less than 1,000 of the >5k setup https://t.co/C6BcyCIl9F  Brian Abel (@BrianAbelTV) September 8, 2016h/t Gateway Pundit\n",
      "25000 lineno\n",
      "430 len\n",
      "WASHINGTON (Reuters) - U.S. President Donald Trump on Wednesday criticized Facebook Inc (FB.O) as “anti-Trump” and questioned its role during the 2016 presidential campaign, amid probes into alleged Russian interference in the election and possible collusion by Trump’s associates. His salvo came as the social media giant prepares to hand over 3,000 political ads to congressional investigators that it has said were likely purchased by Russian entities during and after last year’s presidential contest. Trump appeared to embrace the focus on the social media network in his comments on Wednesday, which also took aim at more traditional medial outlets, long targeted by the president as “fake news.” “Facebook was always anti-Trump. The networks were always anti-Trump,” Trump said on Twitter, levelling the same charge against the New York Times and the Washington Post. “Collusion?” Representatives for Facebook and the newspapers did not immediately respond to a request for comment on Trump’s tweet. U.S. Representative Adam Schiff, the top Democrat on the House Intelligence Committee, which is among those investigating Russia’s role, said he expected to have the ads by next week and that they should be made public. “You really need to see them ... to recognise how cynical an effort this was by the Kremlin, how they sought to just accentuate those divisions ... and drive American against American,” Schiff told MSNBC, adding that Facebook and Twitter Inc (TWTR.N) executives should testify publicly about the issue. “I have concerns about how long it took Facebook to realise the Russians were advertising on their network,” Schiff told MSNBC, adding that he has spoken several times with the company’s chief executive, Mark Zuckerberg.  Facebook and other technology companies are coming under increased scrutiny amid the Russia investigations. The probes, being conducted by several congressional committees along with the Department of Justice, have clouded Trump’s tenure since taking office in January and threatened his agenda, which has yet to secure a major legislative victory.  Moscow has denied any collusion. Trump himself has previously praised Facebook and credited it with helping him win the November election. His campaign has said it spent some $70 million on Facebook ads, and it also ran  a live Facebook show.  His latest comments did not appear to affect shares of the company, which were up 1.4 percent at $166.50 a share in late morning trading after analysts raised their price target for the stock. \n",
      "30000 lineno\n",
      "482 len\n",
      "WASHINGTON (Reuters) - President-elect Donald Trump voiced new doubts on Wednesday that Russian hackers attempted to influence the U.S. election on his behalf, saying WikiLeaks had denied Moscow was behind documents it made public during the campaign. Trump, writing on Twitter, continued to raise questions about the findings by U.S. intelligence agencies that Russia was behind a series of leaks that embarrassed Democratic candidate Hillary Clinton’s campaign before the Nov. 8 vote. The tweets prompted White House spokesman Josh Earnest to ask, “Who are you going to believe?” Documents stolen from the Democratic National Committee and John Podesta, Clinton’s campaign manager, were leaked to the media in advance of the election. One email showed the Clinton campaign received a question in advance of a town hall forum. Trump resumed sending notes on Twitter about the hacking issue on Wednesday, saying, “(WikiLeaks founder) Julian Assange said ‘a 14 year old could have hacked Podesta’ - why was DNC so careless? Also said the Russians did not give him the info!” Trump also quoted Assange as telling Fox News that U.S. media coverage of the matter was “very dishonest.” Asked about the tweets, Earnest said the president-elect’s public comments have pitted the Russians and Assange against 17 U.S. government intelligence agencies, outside cyber experts and lawmakers from both parties. “There’s a pretty stark line that’s been drawn, and the president-elect will have to determine who he’s going to believe,” he said at a daily news briefing. Vice President-elect Mike Pence defended Trump as simply voicing a “very sincere and healthy American skepticism about intelligence conclusions” he has been hearing. “Given some of the intelligence failures of recent years, the president-elect has made it clear to the American people that he’s skeptical about conclusions from the bureaucracy,” Pence told reporters at the U.S. Capitol.  But the top ranking elected Republican, House of Representatives Speaker Paul Ryan, had harsh words for Assange, whose group released the hacked emails. “I think the guy’s a sycophant for Russia, he leaks, he steals data and compromises national security,” Ryan told radio host Hugh Hewitt.  President Barack Obama last month ordered an investigation into malicious cyber activity and foreign intervention in the 2016 presidential election before he leaves office on Jan. 20. Earnest said the intelligence community will meet the deadline with ample time to spare. Separately, five Democratic U.S. senators introduced legislation urging the creation of an independent, nonpartisan commission to investigate any Russian interference in the election. Several lawmakers, including a few of Trump’s fellow Republicans, have backed calls for an investigation. Trump and Pence were scheduled to receive a briefing from intelligence officials on the hacking issue on Friday. \n",
      "35000 lineno\n",
      "420 len\n",
      "GENEVA (Reuters) - The U.N refugee agency on Friday accused Australia of abandoning hundreds of refugees and asylum seekers on Papua New Guinea s Manus Island and said it must take responsibility for the mess it created with its  offshore processing  system. About 800 refugees are still in a precarious situation on Manus Island, having been forcibly removed from a holding camp last month when Australia decided to close it, UNHCR spokeswoman Cecile Pouilly told a regular U.N. briefing in Geneva.   We are talking here about people who have suffered tremendously, extreme trauma, and are now feeling so insecure in the places where they are staying. There are many victims of torture, people who have been deeply traumatised, having no idea what is going to happen next to them,  she said.  In light of the continued perilous situation on Papua New Guinea s Manus Island for refugees and asylum seekers abandoned by Australia, UNHCR has called again this week on the Australian government to live up to its responsibility and urgently find humane and appropriate solutions.       Conditions in the camp, and another on the tiny Pacific island of Nauru, have been widely criticised by the United Nations and human rights groups. The two camps have been cornerstones of Australia s contentious immigration policy under which it refuses to allow asylum-seekers arriving by boat to reach its shores. The policy, aimed at deterring people from making a perilous sea voyage to Australia, has bipartisan political support. The closure of the Manus Island camp, criticised by the United Nations as  shocking , caused chaos, with the men refusing to leave the compound for fear of being attacked by Manus island residents. [nL3N1NT437] Pouilly said that in the past four weeks, there had been at least five security incidents, including an attempt by three people armed with machetes and an axe to force their way into a site where 150 refugees and asylum seekers have been accommodated since the Australian facility closed. Pouilly said that although Papua New Guinea now had to deal with the situation, the buck should stop with Australia.  What we clearly are saying is that it s Australia s responsibility in the first place,  she said.  Australia is the country that created the situation by putting in place this offshore processing facility. So what we are asking is for Australia to find solutions for these people.      \n",
      "40000 lineno\n",
      "535 len\n",
      "STRASBOURG (Reuters) - The European Parliament proposed on Wednesday to reduce EU funds to Turkey that are linked to its stalled bid to join the bloc, a call EU leaders are expected to back given a deteriorating relations with Ankara. Of the 217 million euros set to go to Turkey for reforms, infrastructure and agriculture in 2018, EU lawmakers agreed to cut up to 80 million euros. Of that, 50 million euros should be cut at first, with a further 30-million-euro reduction if Turkey does not improve its human rights record.  Turkey is not respecting freedom of speech, freedom of expression, human rights and is drifting further away from European democratic standards,  said centre-right lawmaker Siegfried Muresan, who led the budget discussions.   We cannot pretend we don t see that,  he told Reuters, emphasizing that the cuts would affect only the money earmarked for political reforms, not for infrastructure and farming. EU leaders must still sign off on the cuts but are expected to do so after an agreement at a summit last week to reduce the so-called pre-accession aid that is meant to help EU candidate countries prepare for membership. German Chancellor Angela Merkel, who pressed for action on Turkey during her re-election campaign, described Turkish behavior on human rights as  unacceptable  in Brussels last Thursday. The European Parliament s decision came on the first day of the trial in Istanbul of 11 human rights activists in Turkey, including a German and a Swedish national. At the EU summit, Merkel said that the rule of law in Turkey was  moving in the wrong direction , in a reference to the large-scale purge that President Tayyip Erdogan has carried out following a failed coup attempt in July 2016. While the EU condemned the coup attempt, the scope of Erdogan s response, his detention of U.S. and European citizens including dual nationals, and his jibes at Germany  for what he has called  Nazi-like  behavior have soured EU-Turkey ties. Erdogan says the purges across society are necessary to maintain stability in a NATO country bordering Iraq and Syria. Launched in 2005 after decades of seeking the formal start of an EU membership bid, Ankara s membership negotiations were always sensitive for France and Germany because of Turkey s status as a large, mainly Muslim country. They are not officially frozen, despite calls from Austria to formally scrap Turkey s EU membership program. That is in part because the EU relies on Ankara to take in Syrian refugees in return for billions of euros of aid. But a majority of EU countries, led by Germany and the Netherlands, say it no longer makes sense to fund political reforms in Turkey when formal EU membership talks have not taken place since last year. Aside from money that the EU gives Turkey as part of its  2016 migration deal, Ankara was set to receive 4.4 billion euros from the EU between 2014 and 2020. Some EU governments want that money to go to non-governmental groups in Turkey, not to Ankara. \n",
      "435.4645641231235\n"
     ]
    }
   ],
   "source": [
    "lineno = 0\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for text in text_dataset:\n",
    "    lengths.append(len(encoder.encode(text)))\n",
    "    lineno += 1\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno, \"lineno\")\n",
    "        print(len(encoder.encode(text)), \"len\")\n",
    "        print(text)\n",
    "\n",
    "# MAX_TEXT_LENGTH = max(lengths)\n",
    "\n",
    "average_len = sum(lengths)/len(lengths)\n",
    "print(average_len)\n",
    "\n",
    "TEXT_LENGTH = round(average_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  39,  696,   57,  997,   57,  998,  999,   17, 1000,  155,   57, 1001,\n",
      "        1002,   26,  463,  449, 1003, 1004,   23,  673,  151, 1005, 1006,  153,\n",
      "        1007,   26,   72, 1008, 1009,   17, 1010, 1011, 1012, 1013,  268,   33,\n",
      "         843,   57, 1014, 1015, 1008,   13, 1016, 1017,   57, 1018, 1019,   23,\n",
      "         408,  168,   13, 1020, 1021,  354,   13, 1022,   17, 1023, 1024,   23,\n",
      "         317, 1025,  348, 1007,   26,   72, 1008, 1009,   17, 1026,   23,  542,\n",
      "         102,  195, 1027,  195,  385, 1028, 1015, 1019,  286,  168,  448, 1029,\n",
      "         574,  191, 1030,   57, 1031, 1032,  168,   30, 1033,   23,  136,  311,\n",
      "          94,  463,   33, 1017,   94,   71, 1034,  153,  490, 1035, 1036, 1037,\n",
      "        1038,  215,  217, 1039,   26, 1040,   17, 1041,  183,  168,  398, 1042,\n",
      "        1043,   33, 1044,   94,  215,   91,   26,  463,   23, 1045, 1015, 1046,\n",
      "        1018,  562,  153,   33,  998,   25,  398, 1047, 1048,  317,  844,  159,\n",
      "         262, 1049, 1018, 1019,   23,  136,   55,  602,  673,   11,  190,  153,\n",
      "         439,   71,   72, 1050,   97,   57, 1051, 1052,   26, 1053,  809,   33,\n",
      "         110,   57, 1054,   57, 1055,  574, 1047, 1056, 1057,   57,  191,  286,\n",
      "         216,   13, 1058, 1059, 1060,   23,   19, 1061,   61,   72, 1062,   92,\n",
      "        1063, 1064, 1065, 1066,  160,  153,  102,  963,  663,   71,  599,   72,\n",
      "          33, 1067, 1068, 1069, 1070,   23, 1071,  195,  191, 1072,   23, 1012,\n",
      "          87, 1073, 1074,  286, 1075, 1076,   57, 1014, 1015, 1019,  557, 1077,\n",
      "        1036,  191,   33, 1078, 1079,   57,  102,  963,  663, 1080,   72,   23,\n",
      "         102,  195, 1081, 1082,   39,  998,  184,  219,  102, 1083,   26, 1084,\n",
      "         168,  613,   75,   13,  118,   23,  118,  546,  136,  191,   94, 1085,\n",
      "        1086, 1087,  159,  494,   13,  769, 1088,  844,   33, 1089, 1090, 1091,\n",
      "          23,  230,   26,  151,   33, 1092, 1093,   26, 1094, 1095,  107,  959,\n",
      "          23, 1096,   23,  195, 1028,  673,  174, 1097, 1098, 1099, 1077, 1100,\n",
      "         168, 1101,  134,  102,  151, 1102,  550, 1014, 1015, 1019,   25,  199,\n",
      "        1103, 1104, 1105, 1009,   17, 1026,   23,   17,  956, 1106,   26, 1107,\n",
      "        1108,   75, 1069, 1109, 1110, 1111,  428, 1112,  271])\n",
      "The number of cases of cops brutalizing and killing people of color seems to see no end. Now, we have another case that needs to be shared far and wide. An Alabama woman by the name of Angela Williams shared a graphic photo of her son, lying in a hospital bed with a beaten and fractured face, on Facebook. It needs to be shared far and wide, because this is unacceptable.It is unclear why Williams  son was in police custody or what sort of altercation resulted in his arrest, but when you see the photo you will realize that these details matter not. Cops are not supposed to beat and brutalize those in their custody. In the post you are about to see, Ms. Williams expresses her hope that the cops had their body cameras on while they were beating her son, but I think we all know that there will be some kind of convenient  malfunction  to explain away the lack of existence of dash or body camera footage of what was clearly a brutal beating. Hell, it could even be described as attempted murder. Something tells me that this young man will never be the same. Without further ado, here is what Troy, Alabama s finest decided was appropriate treatment of Angela Williams  son:No matter what the perceived crime of this young man might be, this is completely unacceptable. The cops who did this need to rot in jail for a long, long time   but what you wanna bet they get a paid vacation while the force  investigates  itself, only to have the officers returned to duty posthaste?This, folks, is why we say BLACK LIVES MATTER. No way in hell would this have happened if Angela Williams  son had been white. Please share far and wide, and stay tuned to Addicting Info for further updates.Featured image via David McNew/Stringer/Getty Images\n",
      "333\n",
      "333\n",
      "333\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: pad texts to have matching length\n",
    "\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import pad_tensor\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "padded_texts = []\n",
    "\n",
    "print(encoder.encode(text_dataset[5])[0:TEXT_LENGTH])\n",
    "print(text_dataset[5])\n",
    "print(lengths[5])\n",
    "print(len(encoder.encode(text_dataset[5])[0:TEXT_LENGTH]))\n",
    "print(len(encoder.encode(text_dataset[5])))\n",
    "# print(encoder.encode(text_dataset[5])[TEXT_LENGTH])\n",
    "\n",
    "for text in text_dataset:\n",
    "    if len(encoder.encode(text)) < TEXT_LENGTH:\n",
    "        padded_texts.append(pad_tensor(encoder.encode(text).long(), TEXT_LENGTH))\n",
    "#         print(len(pad_tensor(encoder.encode(text).long(), TEXT_LENGTH)))\n",
    "    if len(encoder.encode(text)) > TEXT_LENGTH:\n",
    "        padded_texts.append(encoder.encode(text)[0:TEXT_LENGTH])\n",
    "    if len(encoder.encode(text)) == TEXT_LENGTH:\n",
    "        padded_texts.append(encoder.encode(text))\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: convert longs to floats (for NN)\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "for text in padded_texts:\n",
    "    for long in text:\n",
    "        long = float(long)\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: collect vector of true/fake booleans to train dataset\n",
    "\n",
    "rows = np.arange(len(news_dataset))\n",
    "\n",
    "true_fake_dataset = []\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "for idx in np.nditer(rows):\n",
    "    if news_dataset[int(idx)][4] == 'Fake':\n",
    "        true_fake_dataset.append(torch.Tensor([0]))\n",
    "    if news_dataset[int(idx)][4] == 'True':\n",
    "        true_fake_dataset.append(torch.Tensor([1]))\n",
    "    if lineno % 5000 == 0:\n",
    "        print(lineno)\n",
    "    lineno += 1\n",
    "        \n",
    "###\n",
    "\n",
    "# random train data to test NN\n",
    "\n",
    "# true_fake_dataset = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n",
      "44898\n",
      "44898\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# processing: add true/fake label to padded_texts\n",
    "\n",
    "print(len(padded_texts))\n",
    "print(len(true_fake_dataset))\n",
    "print(len(text_dataset))\n",
    "\n",
    "trainset = []\n",
    "\n",
    "lineno = 0\n",
    "\n",
    "for idx, text in enumerate(padded_texts):\n",
    "    if lineno % 5000 == 0:\n",
    "#         print(padded_texts[idx][0])\n",
    "#         print(len(padded_texts[idx]))\n",
    "        print(lineno)\n",
    "    lineno += 1\n",
    "    trainset.append((text, true_fake_dataset[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[30000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,\n",
       "           19,  20,  21,  22,  23,  24,  25,  26,  27,  13,  28,  29,  26,  30,\n",
       "           31,  23,  32,  17,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "           43,  25,   7,  44,  45,  26,  46,  17,  24,   8,   9,  46,  47,  48,\n",
       "           49,  50,  51,  52,  53,  17,  54,  23,  55,  56,  26,  10,  11,  57,\n",
       "           58,  59,  23,  60,  23,  31,  23,  32,  23,  17,  61,  33,  34,  35,\n",
       "           62,  63,  64,  23,  13,  14,  17,  65,  15,  16,  23,  66,  67,  68,\n",
       "           69,  70,  71,  72,  13,  73,  74,  75,  76,  77,  48,  49,  50,  51,\n",
       "           52,  53,  17,  54,  23,  55,  56,  26,  10,  11,  57,  58,  59,  23,\n",
       "           60,  23,  31,  23,  32,  23,  17,  61,  33,  34,  35,  62,  63,  64,\n",
       "           23,  13,  14,  17,  65,  15,  78,  70,  71,  72,  13,  73,  74,  75,\n",
       "           76,  77,   5,  79,   6,  80,  81,  82,  83,  84,  85,  23,  86,  87,\n",
       "           88,  89,  90,  91,  92,  93,  92,  94,  95,  96,  97,  57,  98,  99,\n",
       "           13,  15,  16,  87, 100, 101, 102, 103,  23, 104,  23, 105, 106, 107,\n",
       "          108,   6,  77, 109, 110,  57, 111, 112,   9,  61, 113, 114,  26, 115,\n",
       "          116,  33, 117, 118, 119,  26,  10,  33, 120, 121,  13, 122, 123,  74,\n",
       "           77, 124, 125, 126,  80,  81, 127,  83,  84,  85,  23, 128,  44, 129,\n",
       "           94, 130,  80,  81, 131,  83,  84,  85,  23, 132, 133, 134, 135,  70,\n",
       "           13,  73,  74,  75,  76,  23, 136,  55, 137, 138, 139, 140, 141,  57,\n",
       "          142, 143, 144,  80,  81, 145,  83,  84,  85,  23, 146,  94, 147, 148,\n",
       "          149, 107, 150,  94, 151,  26, 152, 153, 154, 155, 153, 156,  94,  94,\n",
       "          151,  26, 157, 107, 158,  46,  33, 159,  11, 156, 160, 107, 161, 162,\n",
       "           80,  81, 163,  83,  84,  85,  23, 164, 165,  33, 166, 167, 168,  13,\n",
       "           15, 169,  10, 107, 107, 170,  80,  81, 171,  83,  84,  85,  23, 172,\n",
       "          173,   9,   7, 174, 122, 123,  74, 107, 175, 176,  80,  81, 177,  83,\n",
       "           84,  85,  23, 178,  87,   6,  87,  15,  16,  87, 179,  88, 180, 181,\n",
       "           15,  16,  26,  11,  23, 182,  26,  58, 154,  31,  17, 183, 184, 151,\n",
       "          185, 160,  17, 186, 187, 188, 159,   7, 189,   9, 190, 191,  26, 192,\n",
       "          193,  77,   5,  79,   6,  80,  81,  82,  83,  84,  85,  23, 194, 195,\n",
       "          196, 123,  75, 197, 198,  87, 199, 200, 102,  75, 201, 202, 203, 204,\n",
       "           26,  30,  31,  17,  32,  75,  15,  16,  87,  23, 205,  23, 206,  23,\n",
       "           17]), tensor([0.])),\n",
       " (tensor([272, 273, 274, 275, 276, 277, 195, 278,  26, 151,  13, 279, 280, 198,\n",
       "           87, 199, 281,  33, 282,  23, 101, 154,  57, 283,  23, 153,  33, 284,\n",
       "          285, 286, 191, 287,  33, 288, 289, 187,  24,  87, 199, 290,  29,  20,\n",
       "           33, 291,  57, 292,  17,  33, 293, 168, 294,  26, 295, 197,  48,  19,\n",
       "          296,  23,  33, 297, 195, 217, 191, 298,  33, 289,  23, 299,  26, 300,\n",
       "          301, 268,  33,  15, 302, 303,   6, 304, 305, 306, 307, 286, 308, 168,\n",
       "           13, 309, 310, 311,  24, 312, 313,  57, 314, 315, 316, 317, 318, 319,\n",
       "          320,  57, 153,  23, 307, 321,   9,   7,  13, 322, 323,  75,   6,  23,\n",
       "           92,  30, 324, 202, 325, 198,  25,  13, 326, 327, 328,  23, 136, 329,\n",
       "          187, 330,  92, 331,  13, 332, 333, 168,  13, 309, 334, 335, 336, 189,\n",
       "            9, 337,  26, 338,  13,  15, 302, 339, 340,   6,  17,  66, 341, 342,\n",
       "          343,  57, 344, 345, 346, 222,  33, 347, 348, 286, 349, 222, 153,  33,\n",
       "           40, 350, 351, 352, 353, 354, 355, 356,  75,   6,  23, 136, 357,   6,\n",
       "          358, 354, 114, 331, 359,  13, 360, 361, 362, 363,  23, 307, 312,  26,\n",
       "          364, 365, 366, 367, 153, 314, 368, 262, 369, 370, 371, 372, 317, 373,\n",
       "          374, 375, 318, 376, 377, 378, 326, 379, 307, 380, 153, 381,  20,  33,\n",
       "          382, 383, 384, 354,  33, 364,  23, 366, 367,  23, 195, 385,  23,  33,\n",
       "          386, 387, 388, 345, 346, 389,  23, 311, 390, 391, 392, 393, 394, 395,\n",
       "           23, 364, 368, 396,  33, 397,  91, 379, 307,  26, 398, 120, 399,  23,\n",
       "          299,  26, 400, 401,  17,  40, 120,  17, 402, 368, 354, 403, 313,  57,\n",
       "           33, 404, 405, 307, 406, 407,  26, 408,  26,  33, 409,  17, 195, 266,\n",
       "           13, 410, 411, 354, 412, 413, 414, 415,  87, 416, 417,   9,  13, 418,\n",
       "          348,  87,  13, 188, 419,  41, 420, 421, 268, 422, 423, 271, 229,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0]), tensor([0.])),\n",
       " (tensor([424, 425,  23,  19, 286, 312, 153,  40, 426, 427, 428, 429,  23, 184,\n",
       "          286, 331, 430,  75, 431, 432, 433, 168,   5,   6,  87, 324,  23, 202,\n",
       "          434, 435, 436,  57,  30, 437, 438,  23, 439, 286,  13, 440, 441, 317,\n",
       "           13, 442, 340, 429,  17, 443, 444, 445, 446,  23, 184,  24, 389,  25,\n",
       "          447, 268,  33, 448,  75, 449, 450, 451,  23, 452, 153, 453,  30, 454,\n",
       "          262, 455, 429, 456,  33, 448,  26, 457, 446, 458,  24, 459,  23,  17,\n",
       "          266,  23,  13, 460, 461, 202, 199, 462, 268,  33, 293,  26, 463,  33,\n",
       "          464, 195, 465,  19,  36,  37,  61, 466, 467,  57,  33, 460, 461, 215,\n",
       "          317,  33, 468,  55, 469, 470, 268, 471, 472, 473,  26, 474,  17, 475,\n",
       "          160, 354, 398, 476, 477, 478, 479,  26, 480, 160,  23,  33,  40, 481,\n",
       "           69,  55,  71, 482,  26, 483, 484, 168,  33, 485, 354,  13, 486, 487,\n",
       "           17, 488, 489, 490, 491, 492, 493, 159, 494,  47,  55, 151, 199, 495,\n",
       "          268, 496, 155, 497, 484, 498, 499,  55, 469, 470, 268, 471, 472, 473,\n",
       "           26, 474,  17, 475, 160, 354, 398, 476, 477, 478, 479,  26, 480, 500,\n",
       "           55,  71, 482,  26, 483, 484, 168,  33, 485, 354,  13, 486, 487,  17,\n",
       "          488, 489, 490, 491, 492, 493, 159, 494,  47,  55, 151, 199, 495, 268,\n",
       "          496, 155, 497, 484, 498, 499, 501, 428, 502, 429,  23, 503,  80,  81,\n",
       "          504,  83,  84, 505,  23, 227, 506,   9, 457, 507, 477,  77, 150, 508,\n",
       "          509, 510, 511, 352, 476, 477,  26, 474, 160,  23,  33, 512, 195, 513,\n",
       "          514,  20, 515, 516, 484, 168,  33, 517, 518, 519, 520, 521, 522, 523,\n",
       "          524, 525, 526,  13, 527, 101, 508, 509, 510, 528, 529, 496, 497,  26,\n",
       "           27, 484,  13, 530,  57, 398, 531, 532, 498, 533, 534, 428, 502, 429,\n",
       "           23, 503,  80,  81, 504,  83,  84, 505,  23, 535, 536, 537, 114, 538,\n",
       "          195, 539, 540, 541,  17, 153, 460, 461, 417,   9,  36,  23,  17,   7,\n",
       "          542,  33, 543, 217,  26, 544, 545,  20,  33, 546, 547,   9, 548, 159,\n",
       "          112,   9,  77, 549, 550,  94, 482,  26, 551, 552, 458, 553, 217,  26,\n",
       "          554, 429,  23, 435, 460, 461, 555, 556, 557, 558, 559,  80,  81, 560,\n",
       "           83,  84, 505,  23, 561,   7, 562,  33, 563,  57,  33, 564, 565, 566,\n",
       "            9, 567, 568, 569,  80,  81, 570,  83,  84, 505,  23, 571,  23, 572,\n",
       "          573,  23, 574, 575, 576, 107, 577, 578, 579,  80,  81, 580,  83,  84,\n",
       "          505,  23, 581, 582, 583,  80,  81, 584,  83,  84, 505,  23, 585, 537,\n",
       "          539]), tensor([0.])),\n",
       " (tensor([424, 681, 682,  23,   5,   6, 683, 153,  24, 134,  72, 684,  26, 220,\n",
       "           33, 685, 682,  23, 136,  24, 195, 686,  75,  33, 687, 682, 168,  13,\n",
       "          688,  39,  40,  41,  42,  43, 689,  40,  66, 690, 691,  75, 692, 693,\n",
       "           17, 266,   6, 195, 317, 694,  26, 695,  33, 696,  57, 693, 697,  30,\n",
       "          698, 699,  58, 700,  57,   6,  87, 701,  20,   6, 702, 703,  57, 693,\n",
       "          182, 704, 705, 706, 102, 707,  23,  24, 137, 708, 691,  87, 709, 710,\n",
       "          268, 711, 712, 713, 714, 556, 557, 715, 716, 717, 718,  80,  81, 719,\n",
       "           83,  84, 720,  23, 721, 722, 511, 191,  13, 723, 724, 725, 726, 317,\n",
       "            6,  87, 727, 728, 729,  23, 136, 730,  91, 102, 324, 195, 731, 732,\n",
       "           39, 733, 734,  13, 735,  26, 691,  17, 693, 557, 736, 691,  23, 673,\n",
       "          215, 737,  26, 738,  33, 739,  17, 217, 317,  33, 693, 740, 741,  23,\n",
       "           33, 733, 321,   9, 742, 743, 727,  57,   5,   6,  23, 184, 202, 744,\n",
       "          745, 746, 168,  13, 747,  20,  33, 693, 748,  23, 195, 749,  26, 750,\n",
       "          352,  33, 685, 751, 168,  33, 752,  57, 434, 753, 754, 755, 557, 556,\n",
       "          557, 756, 757, 284, 758,  80,  81, 759,  83,  84, 760,  23, 761, 762,\n",
       "           57, 763, 764,  26,  72, 317,  11, 556, 557, 765, 766,  23, 597,  33,\n",
       "          767, 768, 195, 769,  75, 268,  33, 770, 107, 771, 284, 758,  80,  81,\n",
       "          759,  83,  84, 760,  23, 772,  87, 138,  11, 634, 556, 557, 773,  48,\n",
       "          774, 151, 775, 168, 102, 776,  23, 102, 195, 729, 763,  17,  19,  87,\n",
       "          217, 777,  19, 134, 778, 779, 780,  23, 136, 184, 781, 284, 758,  80,\n",
       "           81, 759,  83,  84, 760,  23, 782,  33, 733, 286, 537,  29,  23,  33,\n",
       "          735,  26, 691, 286, 783, 557,  39, 693, 755, 751, 202, 199, 784, 180,\n",
       "           33,   6,  17, 785, 786, 787, 138, 788,  33, 789, 790, 791, 792, 793,\n",
       "          794, 217, 777, 311, 490, 204, 134, 779, 780,  23, 795,  33, 796, 797,\n",
       "           80,  17, 798, 799,  83, 800, 801,  13, 802, 751, 803, 284, 758,  80,\n",
       "           81, 759,  83,  84, 720,  23, 761, 804, 587,  20, 805, 770, 574,  33,\n",
       "            6, 806, 195, 807, 119,  26,   6,  87, 693, 739,  26, 135, 102, 808,\n",
       "          513, 809, 810, 258, 155, 811, 812, 151, 449, 813, 378, 326,  55,  95,\n",
       "          814,  26, 463,  33, 435, 815, 153, 816, 283, 817, 284, 758,  80,  81,\n",
       "          759,  83,  84, 720,  23, 721,  39, 763, 286, 818, 819, 820, 821,  91,\n",
       "          102, 195, 153, 159, 215, 822,  33, 790,  80, 823,  83, 824, 597, 825,\n",
       "          153]), tensor([0.])),\n",
       " (tensor([849, 850, 851,  30, 852, 681, 853, 751,  26, 854,   5,   6, 855,  61,\n",
       "          856,  30, 857,  39, 849, 858,  30, 751,   7, 746, 458, 859,  57,  33,\n",
       "          860, 861, 862,   6,  87, 863,  26, 864, 865,  92,  33, 866,  57, 867,\n",
       "           39, 868, 869, 317, 870,  75,  33, 871, 872,  57, 345, 873, 874, 875,\n",
       "          876,  17, 877, 878, 879, 671, 463, 880, 168,  33, 881,  57,  33, 882,\n",
       "          883, 184, 482,  26, 884, 542,  57, 885, 886, 340, 887,  17, 888,  23,\n",
       "          850, 889, 424, 102, 890, 682,  23, 891, 283, 892,  33, 893,  75, 894,\n",
       "           75, 865,  17,  75,  11,  33, 895, 896, 897, 283, 898, 153,  33,  71,\n",
       "           26, 899, 900, 901, 902, 340,  33, 903,  17, 153,  13, 904, 905, 173,\n",
       "          906,  72, 907,  39, 849,  89, 317,  26, 908,  75, 909,  57, 910, 184,\n",
       "          151, 199, 911, 180, 398, 912,  23,  17, 153, 195, 434, 808,   6, 913,\n",
       "           26, 914, 915, 850, 851, 880,  75, 597, 439, 286, 449, 916, 168,  33,\n",
       "          917,  92, 434, 918, 919,  23,  92,  33, 920,  57, 921, 215, 922, 168,\n",
       "           49, 355,  17, 434, 923, 924,  57, 925, 913,  26, 926, 927,  23, 928,\n",
       "           17, 929, 930,  23, 681, 931, 283,  26, 932, 317,  33, 933,  57,  33,\n",
       "          934,  17,  26, 864, 114, 168,  33, 935,  57, 936, 881,  23, 937, 183,\n",
       "           75, 938,  23, 101, 880,  23, 439, 195, 449, 916, 168,  33, 917,  23,\n",
       "           24, 889, 880, 939, 940,  33, 941,  57, 217, 331, 942,  17, 378, 943,\n",
       "           19, 195, 217,  26, 151,  13, 916,  26, 944,  44,  87, 945,  23,  24,\n",
       "          946, 362,  49, 947, 217,  72, 948,  92, 159, 262, 168,  33, 912,  57,\n",
       "          949,  39, 849, 380, 153, 950,  17, 951, 262, 952, 184, 953,  26, 954,\n",
       "           13, 955, 916,  26, 956, 168, 949, 787,  25,  26,  18, 398, 155,  23,\n",
       "          398, 957,  23,  17, 398, 958,  23, 850, 889, 959, 286, 449, 960, 574,\n",
       "          961, 962,  75,  13, 963, 964,  91,  26, 151,  13, 965, 706, 966,  23,\n",
       "          159, 262, 967,  57, 562,  17, 968, 542,  57,  33, 969,  91,  26,  72,\n",
       "          970, 971, 972, 398, 973, 262, 974,  90, 268,  33, 975,  17, 976, 153,\n",
       "          977, 183, 184, 151,  26,  18, 398, 957, 978, 979, 154, 980, 981, 215,\n",
       "          982, 168,  33, 981,  57, 951,  17, 950,  23, 850, 380, 983, 671, 463,\n",
       "           33, 984,  57, 985, 986, 911,  26, 351,  29, 168,  49, 531, 280, 671,\n",
       "          463,  33, 984,  57, 987,  57, 988, 184,  46, 217, 989,  26, 513, 809,\n",
       "           23, 136, 990, 180, 398, 958,  23,  18, 991, 398, 992, 993, 994,  26,\n",
       "          995]), tensor([0.])),\n",
       " (tensor([  39,  696,   57,  997,   57,  998,  999,   17, 1000,  155,   57, 1001,\n",
       "          1002,   26,  463,  449, 1003, 1004,   23,  673,  151, 1005, 1006,  153,\n",
       "          1007,   26,   72, 1008, 1009,   17, 1010, 1011, 1012, 1013,  268,   33,\n",
       "           843,   57, 1014, 1015, 1008,   13, 1016, 1017,   57, 1018, 1019,   23,\n",
       "           408,  168,   13, 1020, 1021,  354,   13, 1022,   17, 1023, 1024,   23,\n",
       "           317, 1025,  348, 1007,   26,   72, 1008, 1009,   17, 1026,   23,  542,\n",
       "           102,  195, 1027,  195,  385, 1028, 1015, 1019,  286,  168,  448, 1029,\n",
       "           574,  191, 1030,   57, 1031, 1032,  168,   30, 1033,   23,  136,  311,\n",
       "            94,  463,   33, 1017,   94,   71, 1034,  153,  490, 1035, 1036, 1037,\n",
       "          1038,  215,  217, 1039,   26, 1040,   17, 1041,  183,  168,  398, 1042,\n",
       "          1043,   33, 1044,   94,  215,   91,   26,  463,   23, 1045, 1015, 1046,\n",
       "          1018,  562,  153,   33,  998,   25,  398, 1047, 1048,  317,  844,  159,\n",
       "           262, 1049, 1018, 1019,   23,  136,   55,  602,  673,   11,  190,  153,\n",
       "           439,   71,   72, 1050,   97,   57, 1051, 1052,   26, 1053,  809,   33,\n",
       "           110,   57, 1054,   57, 1055,  574, 1047, 1056, 1057,   57,  191,  286,\n",
       "           216,   13, 1058, 1059, 1060,   23,   19, 1061,   61,   72, 1062,   92,\n",
       "          1063, 1064, 1065, 1066,  160,  153,  102,  963,  663,   71,  599,   72,\n",
       "            33, 1067, 1068, 1069, 1070,   23, 1071,  195,  191, 1072,   23, 1012,\n",
       "            87, 1073, 1074,  286, 1075, 1076,   57, 1014, 1015, 1019,  557, 1077,\n",
       "          1036,  191,   33, 1078, 1079,   57,  102,  963,  663, 1080,   72,   23,\n",
       "           102,  195, 1081, 1082,   39,  998,  184,  219,  102, 1083,   26, 1084,\n",
       "           168,  613,   75,   13,  118,   23,  118,  546,  136,  191,   94, 1085,\n",
       "          1086, 1087,  159,  494,   13,  769, 1088,  844,   33, 1089, 1090, 1091,\n",
       "            23,  230,   26,  151,   33, 1092, 1093,   26, 1094, 1095,  107,  959,\n",
       "            23, 1096,   23,  195, 1028,  673,  174, 1097, 1098, 1099, 1077, 1100,\n",
       "           168, 1101,  134,  102,  151, 1102,  550, 1014, 1015, 1019,   25,  199,\n",
       "          1103, 1104, 1105, 1009,   17, 1026,   23,   17,  956, 1106,   26, 1107,\n",
       "          1108,   75, 1069, 1109, 1110, 1111,  428, 1112,  271,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([   5,    6,  744,   13, 1113, 1114,   57,   30,  682,   20,   30,  693,\n",
       "          1115,   23, 1116,   33, 1117,  682,   24,   87,  742,  187,  795, 1118,\n",
       "            33, 1119,   57, 1120,  348, 1121,  151,  199,   13,  279, 1122,  542,\n",
       "             7,  458,  153,   23,    6, 1123,   29,   20,  293, 1124, 1125,  269,\n",
       "          1126,  317, 1127,  685,   13,  386, 1128, 1126, 1129,   26, 1130,  168,\n",
       "            13,  233, 1131,   39,  386, 1132, 1126,   87, 1133,  168, 1134,   57,\n",
       "          1135, 1136,  102, 1137,   23,   92,  940,   92, 1138, 1139,  180, 1140,\n",
       "          1141,   33,  288, 1142,   23, 1143,   23,    6,  495, 1126,  354,   13,\n",
       "           551, 1144,  173,  293, 1124, 1125,  269, 1126,   23,   33,  663,  168,\n",
       "           554,   23, 1145,  354, 1146, 1147, 1148,   23,   57,   33, 1149,  318,\n",
       "          1150,  289,   80,  182, 1018, 1151, 1152, 1153,  392,   83,   72, 1154,\n",
       "          1155, 1156,   75, 1157,   87,  304,  268, 1150, 1158, 1159,  289,  107,\n",
       "             6, 1160,  173,  293, 1124, 1125,  269, 1126,   23,   33,  663,  168,\n",
       "           554,   23, 1145,  354, 1146, 1147, 1148,   23,   57,   33, 1149,  318,\n",
       "          1150,  289,   80,  182, 1018, 1151, 1152, 1153,  392,   83,   72, 1154,\n",
       "          1155, 1156,   75, 1157,   87,  304,  268, 1150, 1158, 1159,  289,  107,\n",
       "             5,   79,    6,   80,   81,   82,   83,   84, 1161,   23,  227,  506,\n",
       "             9,  457, 1162, 1124, 1125,  269, 1126,  195, 1163,   33, 1164,   26,\n",
       "          1130,  354,  967, 1165, 1166,  746,   26,  513,  107,   77,   77,   77,\n",
       "             5,   79,    6,   80,   81,   82,   83,   84, 1161,   23, 1167,   23,\n",
       "           293, 1168, 1147, 1169, 1170,   23,  299,   26,   81, 1171,    5,   79,\n",
       "             6,   80,   81,   82,   83,   84, 1161,   23, 1172,   11,   57,   33,\n",
       "          1173,   20,    6,   87, 1174,   23,   24,   87, 1175,   30,  397,  180,\n",
       "          1176, 1177, 1126,  744, 1178,   57,   30, 1179,  168,   33,  914, 1180,\n",
       "          1181,   17,  266,   24,   87,  331,  495,  268,   33, 1182, 1183,    6,\n",
       "           202,  199, 1184,  222,  317,   30, 1185,   57,   30, 1157, 1186, 1155,\n",
       "          1156,   75, 1018, 1187,  775,  168, 1188,  711,  153,    6,   87,   88,\n",
       "            91,  269, 1126,  195,   13, 1189, 1190,   57,   33, 1191, 1192,   33,\n",
       "          1193,  153, 1126,  259, 1150, 1194,   92,   13, 1195, 1196,  547,    9,\n",
       "           135,  326, 1197,  311,  673,  829,   20,   33, 1198,  109,  711,   88,\n",
       "           286, 1199, 1200, 1201,  388,    6, 1202,  490, 1203,  542,   24,  939,\n",
       "            30,   60,   71, 1204,  484,  855, 1205,   26, 1206,  348,   87,  640,\n",
       "            13, 1207,   23, 1208,  268, 1209, 1210, 1211,  271,  229,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([1043,   33, 1212,   57,  972, 1005, 1213,  553,  153, 1214,    5,    6,\n",
       "            87, 1215,   26,  310, 1216,  180, 1217,   33,  860, 1218,   23,   33,\n",
       "            15,  302, 1219, 1220,   13,  386,  317, 1221, 1222, 1223,   33,   98,\n",
       "            87, 1224,   20,  217, 1175,   30, 1100,   17,  378, 1009,  684,  153,\n",
       "          1224, 1225,   26,   33, 1226,   23,  684,  168, 1227,   23,    6, 1228,\n",
       "           260,   33, 1229, 1230,   23, 1231,   91,   33, 1232,   57,   33, 1233,\n",
       "          1234,   23,  597,   24,  672,  134,   72, 1235,   17, 1236,  168,  916,\n",
       "           268, 1237,   22,   23,   24, 1238,   23, 1239,   25, 1240,  199, 1241,\n",
       "            26,  952,   20, 1242,   13, 1243,  153,   30,   59,  262,  465,   26,\n",
       "           174,   24, 1244,  101,   13,  333,  458, 1245,   30, 1246, 1247,  286,\n",
       "          1248,  153,    6,  393, 1249,  180,   13, 1250,  153,   13,  320, 1251,\n",
       "            23,  775, 1252, 1253, 1254, 1255,   23,   25, 1256,  114,    7,  222,\n",
       "            33,  339,  354,   30, 1257,   39,  800, 1258,  378,  154, 1239,   25,\n",
       "           199, 1241,  102,   74,   23,   17, 1259, 1260,  180, 1261,   80,   13,\n",
       "          1262,  217,  317,   33, 1233, 1234,   83,   23, 1263,  180, 1264,   80,\n",
       "           138,  217, 1259,   83,   23,   17, 1265,  180, 1266,   80, 1267,   13,\n",
       "          1268,  972,  107,   83,   23,   17,    6, 1269,   30, 1270,   20, 1271,\n",
       "            26, 1272,  168,   33, 1273,  184, 1274,   26,   33, 1219,  317, 1275,\n",
       "            57, 1276,   23,   17,  184,  262, 1277, 1145,  354, 1278, 1279,  774,\n",
       "            75,   33, 1226,   23,    6,  537,   29, 1280, 1262,   75, 1281, 1282,\n",
       "            92,   24, 1283,  557, 1261,  286,   13, 1284, 1285,   23,   33,  155,\n",
       "            57, 1266,  134,  599,  513,  684,   26,  398, 1286,  258,  159, 1287,\n",
       "            33, 1288,   57,   76,   23,   17,  952,  180, 1264,   11,  151, 1289,\n",
       "          1290,   33, 1291,  316,  742,  268,   33,  541,   23,   33, 1292,  272,\n",
       "            57,  748, 1293,  153, 1294, 1242, 1295,  286, 1296, 1154,    6,   87,\n",
       "          1297, 1298,   17,   30, 1251, 1254, 1255,   87, 1299, 1252, 1300,   23,\n",
       "            19,  134,   72,  449, 1301,  550,   13, 1302,   57, 1303, 1304, 1305,\n",
       "          1306,  352,  153,   33, 1250,  168, 1307,   25,   33, 1308, 1309, 1310,\n",
       "            92, 1311,   75,   33,   98,   26, 1312,   30, 1313, 1314,  348,  286,\n",
       "          1255,   23,  458,   11,   23,  184,  286, 1315,   75,   33,  120, 1316,\n",
       "          1317,  153,    6,  858,   20,   30, 1318, 1297,  195,   13, 1319,   26,\n",
       "            76,   23,   17,   24,  547,    9, 1320, 1321,  153,  102, 1262, 1322,\n",
       "          1323,  897,   87,  562,  153, 1324, 1325,  180,  414,  415,  215,  317,\n",
       "           398, 1100,   92,  673, 1326, 1110, 1111,  591, 1327,  271,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.])),\n",
       " (tensor([1328,  155,  151, 1329,   33, 1330, 1141,   33, 1331,  153,    5,    6,\n",
       "           195, 1332, 1333,   26,  223,  434, 1334,   39, 1335,  195,   23, 1336,\n",
       "          1337, 1338,  514,  281,   33,  155,   87, 1339,   23,  542,  159,  173,\n",
       "          1340,  829,  101, 1336,  168,   33, 1341, 1342,  959,  286, 1343,  268,\n",
       "          1344,  428, 1345,    7,   13,  964,   57,  346,  260,    5,    6,   87,\n",
       "          1346,   23,  168,   13, 1347,  168,   39, 1348,  537, 1144,   26, 1349,\n",
       "           434, 1350, 1043, 1331,   23,  550,   94,  728,  829, 1351,   20,  191,\n",
       "           195, 1352,  514,  266,   33, 1353, 1354,   57, 1355, 1356, 1242,   92,\n",
       "            33, 1357, 1358,   17,   33, 1359, 1360,   57, 1361,   92,  940,   33,\n",
       "           403, 1362,   57,   33,  291,   57,  292,  168,  294,   26,  513,  458,\n",
       "             6,   87,   40, 1195, 1363,   23,  363,  391,  375,  318, 1150,   23,\n",
       "            17,   94,  151,   33, 1364,   57,  434, 1365,  671,  215, 1324,  497,\n",
       "           940,  317,   49, 1366, 1367,   23,   44, 1368,   57, 1369,   23,   33,\n",
       "           272,   57, 1370,   23, 1240,  202,   13, 1371, 1372,   57, 1140,  184,\n",
       "           215, 1373,   13, 1374,  289,   26,   33, 1375,  314, 1376,  289,   23,\n",
       "           354,   33, 1377, 1378,   57, 1379,   17, 1354,   33,  813,  153,    6,\n",
       "          1061,  151, 1380,  742, 1321,  263,  354,   33, 1381,  168,  294,   26,\n",
       "          1382,   33,  363, 1383,  168,   30, 1384,   57,  153,  195,    7,   75,\n",
       "          1385,   23, 1386, 1004,   23,  673,  151,    6, 1245,  860,  861, 1387,\n",
       "          1388, 1389,  527,   17, 1390,  980, 1391,  168,   33,  860,  861,  184,\n",
       "           254, 1180,    6,   87,  553,   26,  257, 1392, 1393,  311,   19, 1394,\n",
       "            26, 1395,   57,  865,   92,   33,  866,   57,   33, 1396, 1397, 1398,\n",
       "            23,   44, 1399,   23,  184,  195, 1400, 1401, 1402,   23,  202,   25,\n",
       "           119,   57,    6,   87, 1403, 1404,  557, 1405, 1406, 1125, 1407, 1408,\n",
       "          1409,   39, 1410, 1411, 1412,   26,    6,   87, 1413, 1414,   23, 1127,\n",
       "            23,   17,  689,   33,  553,  557,    6, 1415, 1416,   26, 1417, 1180,\n",
       "          1418,  153, 1419, 1420,  514,  168, 1421,   26, 1422, 1423, 1424,  317,\n",
       "           865,  195, 1425, 1426, 1427,   81,   82, 1428, 1429, 1430,   17, 1431,\n",
       "           180, 1432, 1433, 1400, 1434,  168, 1435,   23, 1436, 1437, 1407, 1408,\n",
       "          1438,   80,   81, 1439,   83,   84, 1440,   23, 1441, 1438,  195, 1442,\n",
       "            23,   57,  740,    6,  195, 1443,    7,  101,  434, 1444,   23,   17,\n",
       "           187,  154,  155,  168,   33, 1445,  215, 1446,  311,   19, 1394,   26,\n",
       "           102, 1447, 1448,   23,  168,  597,   33, 1449, 1416,   26, 1450,   17,\n",
       "            33,   34, 1451,   57,   33, 1452, 1091,  195,   33,  120, 1183, 1453,\n",
       "            12,   23,  673,  190,   33, 1454, 1369,   71,  217,   72,   33, 1455,\n",
       "           317,    6,  153,  159,  215, 1039,   26, 1456,  348,   87,  546,   26,\n",
       "           494,   29,   17]), tensor([0.])),\n",
       " (tensor([1471,  311,   94, 1080,  151,  672,  673,   95,  494,   13,  633,  180,\n",
       "          1472,  155, 1473,    5,    6,   87, 1474,   17, 1475,   30, 1476, 1477,\n",
       "          1478,   23,   13, 1479, 1480, 1481,  434, 1477,  153,   87,  196,  136,\n",
       "           155,  200,   61, 1324,   57,  183, 1482, 1483,   76, 1484, 1485,  195,\n",
       "           351,   26, 1486,  102, 1477,   23,  537, 1487,  812,   23,   66,    6,\n",
       "            23,  317,  681,  853,   17,   23,  940,   23,  673, 1488,  352,   13,\n",
       "           936,  168,   49, 1489,  638,   26, 1490, 1491,   23,   33, 1492,  195,\n",
       "           196,  136,  155, 1493,   11,  634,    6,   75,   11,   33, 1494,   24,\n",
       "          1495,    9,  779, 1496,   39, 1477, 1497,   13, 1498,  354,   13,  936,\n",
       "          1499, 1500,    6,   75, 1501,  684, 1502,  681,   23,  597,  599,   89,\n",
       "           809,   80,  439,  215,   61, 1503,   57,   66,  691, 1128, 1502,  681,\n",
       "          1504,   83,  229, 1505,  663, 1506,  114,   75, 1507,   30, 1508, 1192,\n",
       "            76, 1484,  768,  153, 1509,   12, 1510,  215, 1500,    6,   75,  331,\n",
       "          1242,   13,   73,   17, 1511, 1512,  820, 1513,   87,  778,  742,  191,\n",
       "            24,   87, 1496,  198,   87,  236,   11, 1514,   57, 1515, 1516, 1517,\n",
       "          1518,   23,   33,  166, 1519, 1394,   26, 1520,  311, 1472, 1521,  722,\n",
       "            87,  191,   19,  195, 1522, 1519, 1523,  587,  184, 1524,    9, 1083,\n",
       "           102,   97,   57, 1525, 1526, 1527,  102, 1477, 1528, 1529,  557,   39,\n",
       "          1100,   33, 1530,  215, 1531,   26, 1532, 1533,  195, 1534, 1535,   17,\n",
       "          1536,   39,  663,  202,  742,  196,   23,   17,   30, 1537,   71, 1538,\n",
       "            33,   34, 1539,   12,  184,  215, 1500, 1540, 1541,   23,   19,   71,\n",
       "          1542,  434, 1543, 1544,   57,  941,  222,  159,  137, 1545,  398, 1546,\n",
       "            17,  463,  159, 1547,  199, 1548,  268,   13, 1549,  663,  354,   13,\n",
       "           279, 1550,   30, 1551, 1083,   75,  102,   97,   57, 1552,  195,   23,\n",
       "            20,  820,   23, 1553,   57,   30, 1120,  959, 1477,  195, 1554, 1110,\n",
       "          1111, 1555, 1556,  271,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0]), tensor([0.]))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Net class\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(TEXT_LENGTH, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=435, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "8\n",
      "torch.Size([64, 435])\n"
     ]
    }
   ],
   "source": [
    "# initialize net and print parameters\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "tensor(0., grad_fn=<NllLossBackward>)\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "tensor(0., grad_fn=<NllLossBackward>)\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "tensor(0., grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimize net with backprop; 3 epochs\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, padded_text in enumerate(padded_texts, start=0):\n",
    "        # data is a batch of featuresets and labels\n",
    "        #print(true_fake_dataset[i])\n",
    "        X = padded_text\n",
    "        #print(X)\n",
    "        y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        #print(y)\n",
    "        net.zero_grad()\n",
    "        X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float, NN must read in float\n",
    "#         print(X_float)\n",
    "        output = net(X_float.view(-1, TEXT_LENGTH))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.477\n"
     ]
    }
   ],
   "source": [
    "# calculate and print accuracy\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, padded_text in enumerate(padded_texts, start=0):\n",
    "        X = padded_text\n",
    "        y = torch.tensor([true_fake_dataset[i]], dtype=torch.long)\n",
    "        X_float = X.new_tensor(X, dtype=torch.float) # convert tensor long to float\n",
    "        output = net(X_float.view(-1, TEXT_LENGTH))\n",
    "#         print(torch.argmax(output))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
